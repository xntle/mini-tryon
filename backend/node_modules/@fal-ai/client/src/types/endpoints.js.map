{"version":3,"file":"endpoints.js","sourceRoot":"","sources":["../../../../../libs/client/src/types/endpoints.ts"],"names":[],"mappings":"","sourcesContent":["export type Audio = {\n  /**\n   * Type of media (always 'audio') Default value: `\"audio\"`\n   */\n  media_type?: string;\n  /**\n   * URL where the media file can be accessed\n   */\n  url: string;\n  /**\n   * MIME type of the media file\n   */\n  content_type: string;\n  /**\n   * Original filename of the media\n   */\n  file_name: string;\n  /**\n   * Size of the file in bytes\n   */\n  file_size: number;\n  /**\n   * Duration of the media in seconds\n   */\n  duration: number;\n  /**\n   * Overall bitrate of the media in bits per second\n   */\n  bitrate: number;\n  /**\n   * Codec used to encode the media\n   */\n  codec: string;\n  /**\n   * Container format of the media file (e.g., 'mp4', 'mov')\n   */\n  container: string;\n  /**\n   * Number of audio channels\n   */\n  channels: number;\n  /**\n   * Audio sample rate in Hz\n   */\n  sample_rate: number;\n};\nexport type AudioFile = {\n  /**\n   * The URL where the file can be downloaded from.\n   */\n  url: string;\n  /**\n   * The mime type of the file.\n   */\n  content_type?: string;\n  /**\n   * The name of the file. It will be auto-generated if not provided.\n   */\n  file_name?: string;\n  /**\n   * The size of the file in bytes.\n   */\n  file_size?: number;\n  /**\n   * File data\n   */\n  file_data?: string;\n  /**\n   * The duration of the audio file in seconds.\n   */\n  duration: number;\n};\nexport type AudioSetting = {\n  /**\n   * Sample rate of generated audio Default value: `\"32000\"`\n   */\n  sample_rate?: \"8000\" | \"16000\" | \"22050\" | \"24000\" | \"32000\" | \"44100\";\n  /**\n   * Bitrate of generated audio Default value: `\"128000\"`\n   */\n  bitrate?: \"32000\" | \"64000\" | \"128000\" | \"256000\";\n  /**\n   * Audio format Default value: `\"mp3\"`\n   */\n  format?: \"mp3\" | \"pcm\" | \"flac\";\n  /**\n   * Number of audio channels (1=mono, 2=stereo) Default value: `\"1\"`\n   */\n  channel?: \"1\" | \"2\";\n};\nexport type AudioTrack = {\n  /**\n   * Audio codec used (e.g., 'aac', 'mp3')\n   */\n  codec: string;\n  /**\n   * Number of audio channels\n   */\n  channels: number;\n  /**\n   * Audio sample rate in Hz\n   */\n  sample_rate: number;\n  /**\n   * Audio bitrate in bits per second\n   */\n  bitrate: number;\n};\nexport type BBoxPromptBase = {\n  /**\n   * X Min Coordinate of the box (0-1)\n   */\n  x_min?: number;\n  /**\n   * Y Min Coordinate of the box (0-1)\n   */\n  y_min?: number;\n  /**\n   * X Max Coordinate of the prompt (0-1)\n   */\n  x_max?: number;\n  /**\n   * Y Max Coordinate of the prompt (0-1)\n   */\n  y_max?: number;\n};\nexport type BoxPrompt = {\n  /**\n   * X Min Coordinate of the box\n   */\n  x_min?: number;\n  /**\n   * Y Min Coordinate of the box\n   */\n  y_min?: number;\n  /**\n   * X Max Coordinate of the prompt\n   */\n  x_max?: number;\n  /**\n   * Y Max Coordinate of the prompt\n   */\n  y_max?: number;\n  /**\n   * The frame index to interact with.\n   */\n  frame_index?: number;\n};\nexport type BoxPromptBase = {\n  /**\n   * X Min Coordinate of the box\n   */\n  x_min?: number;\n  /**\n   * Y Min Coordinate of the box\n   */\n  y_min?: number;\n  /**\n   * X Max Coordinate of the prompt\n   */\n  x_max?: number;\n  /**\n   * Y Max Coordinate of the prompt\n   */\n  y_max?: number;\n};\nexport type CameraControl = {\n  /**\n   * The type of camera movement\n   */\n  movement_type: \"horizontal\" | \"vertical\" | \"pan\" | \"tilt\" | \"roll\" | \"zoom\";\n  /**\n   * The value of the camera movement\n   */\n  movement_value: number;\n};\nexport type ColorPalette = {\n  /**\n   * A list of color palette members that define the color palette\n   */\n  members?: Array<ColorPaletteMember>;\n  /**\n   * A color palette preset value\n   */\n  name?:\n    | \"EMBER\"\n    | \"FRESH\"\n    | \"JUNGLE\"\n    | \"MAGIC\"\n    | \"MELON\"\n    | \"MOSAIC\"\n    | \"PASTEL\"\n    | \"ULTRAMARINE\";\n};\nexport type ColorPaletteMember = {\n  /**\n   * RGB color value for the palette member\n   */\n  rgb: RGBColor;\n  /**\n   * The weight of the color in the color palette Default value: `0.5`\n   */\n  color_weight?: number;\n};\nexport type ControlLoraWeight = {\n  /**\n   * URL or the path to the LoRA weights.\n   */\n  path: string;\n  /**\n   * The scale of the LoRA weight. This is used to scale the LoRA weight\n   * before merging it with the base model. Providing a dictionary as {\"layer_name\":layer_scale} allows per-layer lora scale settings. Layers with no scale provided will have scale 1.0. Default value: `1`\n   */\n  scale?: any | number;\n  /**\n   * URL of the image to be used as the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * Type of preprocessing to apply to the input image. Default value: `\"None\"`\n   */\n  preprocess?: \"canny\" | \"depth\" | \"None\";\n};\nexport type ControlNet = {\n  /**\n   * URL or the path to the control net weights.\n   */\n  path: string;\n  /**\n   * optional URL to the controlnet config.json file.\n   */\n  config_url?: string | Blob | File;\n  /**\n   * The optional variant if a Hugging Face repo key is used.\n   */\n  variant?: string;\n  /**\n   * URL of the image to be used as the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * URL of the mask for the control image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Threshold for mask. Default value: `0.5`\n   */\n  mask_threshold?: number;\n  /**\n   * The scale of the control net weight. This is used to scale the control net weight\n   * before merging it with the base model. Default value: `1`\n   */\n  conditioning_scale?: number;\n  /**\n   * The percentage of the image to start applying the controlnet in terms of the total timesteps.\n   */\n  start_percentage?: number;\n  /**\n   * The percentage of the image to end applying the controlnet in terms of the total timesteps. Default value: `1`\n   */\n  end_percentage?: number;\n};\nexport type ControlNetUnion = {\n  /**\n   * URL or the path to the control net weights.\n   */\n  path: string;\n  /**\n   * optional URL to the controlnet config.json file.\n   */\n  config_url?: string | Blob | File;\n  /**\n   * The optional variant if a Hugging Face repo key is used.\n   */\n  variant?: string;\n  /**\n   * The control images and modes to use for the control net.\n   */\n  controls: Array<ControlNetUnionInput>;\n};\nexport type DynamicMask = {\n  /**\n   * URL of the image for Dynamic Brush Application Area (Mask image created by users using the motion brush)\n   */\n  mask_url: string | Blob | File;\n  /**\n   * List of trajectories\n   */\n  trajectories?: Array<Trajectory>;\n};\nexport type EasyControlWeight = {\n  /**\n   * URL to safetensor weights of control method to be applied. Can also be one of `canny`, `depth`, `hedsketch`, `inpainting`, `pose`, `seg`, `subject`, `ghibli`\n   */\n  control_method_url: string | Blob | File;\n  /**\n   * Scale for the control method. Default value: `1`\n   */\n  scale?: number;\n  /**\n   * URL of an image to use as a control\n   */\n  image_url: string | Blob | File;\n  /**\n   * Control type of the image. Must be one of `spatial` or `subject`.\n   */\n  image_control_type: \"subject\" | \"spatial\";\n};\nexport type fal__toolkit__image__image__Image = {\n  /**\n   * The URL where the file can be downloaded from.\n   */\n  url: string;\n  /**\n   * The mime type of the file.\n   */\n  content_type?: string;\n  /**\n   * The name of the file. It will be auto-generated if not provided.\n   */\n  file_name?: string;\n  /**\n   * The size of the file in bytes.\n   */\n  file_size?: number;\n  /**\n   * File data\n   */\n  file_data?: string;\n  /**\n   * The width of the image in pixels.\n   */\n  width?: number;\n  /**\n   * The height of the image in pixels.\n   */\n  height?: number;\n};\nexport type File = {\n  /**\n   * The URL where the file can be downloaded from.\n   */\n  url: string;\n  /**\n   * The mime type of the file.\n   */\n  content_type?: string;\n  /**\n   * The name of the file. It will be auto-generated if not provided.\n   */\n  file_name?: string;\n  /**\n   * The size of the file in bytes.\n   */\n  file_size?: number;\n};\nexport type Image = {\n  /**\n   *\n   */\n  url: string;\n  /**\n   *\n   */\n  width: number;\n  /**\n   *\n   */\n  height: number;\n  /**\n   *  Default value: `\"image/jpeg\"`\n   */\n  content_type?: string;\n};\nexport type ImageCondition = {\n  /**\n   * The URL of the image to use as input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The frame number to start the condition on.\n   */\n  start_frame_number?: number;\n  /**\n   * The strength of the condition. Default value: `1`\n   */\n  strength?: number;\n};\nexport type ImageSize = {\n  /**\n   * The width of the generated image. Default value: `512`\n   */\n  width?: number;\n  /**\n   * The height of the generated image. Default value: `512`\n   */\n  height?: number;\n};\nexport type IPAdapter = {\n  /**\n   * Hugging Face path to the IP-Adapter\n   */\n  path: string;\n  /**\n   * Subfolder in which the ip_adapter weights exist\n   */\n  subfolder?: string;\n  /**\n   * Name of the safetensors file containing the ip-adapter weights\n   */\n  weight_name?: string;\n  /**\n   * Path to the Image Encoder for the IP-Adapter, for example 'openai/clip-vit-large-patch14'\n   */\n  image_encoder_path: string;\n  /**\n   * Subfolder in which the image encoder weights exist.\n   */\n  image_encoder_subfolder?: string;\n  /**\n   * Name of the image encoder.\n   */\n  image_encoder_weight_name?: string;\n  /**\n   * URL of Image for IP-Adapter conditioning.\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of the mask for the control image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Threshold for mask. Default value: `0.5`\n   */\n  mask_threshold?: number;\n  /**\n   * Scale for ip adapter.\n   */\n  scale: number;\n};\nexport type Keyframe = {\n  /**\n   * The timestamp in milliseconds where this keyframe starts\n   */\n  timestamp: number;\n  /**\n   * The duration in milliseconds of this keyframe\n   */\n  duration: number;\n  /**\n   * The URL where this keyframe's media file can be accessed\n   */\n  url: string;\n};\nexport type LoraWeight = {\n  /**\n   * URL or the path to the LoRA weights.\n   */\n  path: string;\n  /**\n   * The scale of the LoRA weight. This is used to scale the LoRA weight\n   * before merging it with the base model. Default value: `1`\n   */\n  scale?: number;\n};\nexport type LoRAWeight = {\n  /**\n   * URL or path to the LoRA weights.\n   */\n  path: string;\n  /**\n   * Name of the LoRA weight. Only used if `path` is a HuggingFace repository, and is only required when the repository contains multiple LoRA weights.\n   */\n  weight_name?: string;\n  /**\n   * Scale of the LoRA weight. This is a multiplier applied to the LoRA weight when loading it. Default value: `1`\n   */\n  scale?: number;\n};\nexport type LoudnessNormalizationSetting = {\n  /**\n   * Enable loudness normalization for the audio Default value: `true`\n   */\n  enabled?: boolean;\n  /**\n   * Target loudness in LUFS (default -18.0) Default value: `-18`\n   */\n  target_loudness?: number;\n  /**\n   * Target loudness range in LU (default 8.0) Default value: `8`\n   */\n  target_range?: number;\n  /**\n   * Target peak level in dBTP (default -0.5). Default value: `-0.5`\n   */\n  target_peak?: number;\n};\nexport type LoudnormSummary = {\n  /**\n   * Input integrated loudness in LUFS\n   */\n  input_integrated?: number;\n  /**\n   * Input true peak in dBTP\n   */\n  input_true_peak?: number;\n  /**\n   * Input loudness range in LU\n   */\n  input_lra?: number;\n  /**\n   * Input threshold in LUFS\n   */\n  input_threshold?: number;\n  /**\n   * Output integrated loudness in LUFS\n   */\n  output_integrated?: number;\n  /**\n   * Output true peak in dBTP\n   */\n  output_true_peak?: number;\n  /**\n   * Output loudness range in LU\n   */\n  output_lra?: number;\n  /**\n   * Output threshold in LUFS\n   */\n  output_threshold?: number;\n  /**\n   * Type of normalization applied (Dynamic/Linear)\n   */\n  normalization_type?: string;\n  /**\n   * Target offset in LU\n   */\n  target_offset?: number;\n};\nexport type PikaImage = {\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n};\nexport type PointPrompt = {\n  /**\n   * X Coordinate of the prompt Default value: `305`\n   */\n  x?: number;\n  /**\n   * Y Coordinate of the prompt Default value: `350`\n   */\n  y?: number;\n  /**\n   * Label of the prompt. 1 for foreground, 0 for background Default value: `\"1\"`\n   */\n  label?: \"0\" | \"1\";\n  /**\n   * The frame index to interact with.\n   */\n  frame_index?: number;\n};\nexport type PronunciationDict = {\n  /**\n   * List of pronunciation replacements in format ['text/(pronunciation)', ...]. For Chinese, tones are 1-5. Example: ['燕少飞/(yan4)(shao3)(fei1)']\n   */\n  tone_list?: Array<string>;\n};\nexport type ReferenceImage = {\n  /**\n   * URL to the reference image file (PNG format recommended)\n   */\n  image_url: string | Blob | File;\n};\nexport type registry__image__fast_sdxl__models__Image = {\n  /**\n   *\n   */\n  url: string;\n  /**\n   *\n   */\n  width: number;\n  /**\n   *\n   */\n  height: number;\n  /**\n   *  Default value: `\"image/jpeg\"`\n   */\n  content_type?: string;\n};\nexport type Resolution = {\n  /**\n   * Display aspect ratio (e.g., '16:9')\n   */\n  aspect_ratio: string;\n  /**\n   * Width of the video in pixels\n   */\n  width: number;\n  /**\n   * Height of the video in pixels\n   */\n  height: number;\n};\nexport type RGBColor = {\n  /**\n   * Red color value\n   */\n  r?: number;\n  /**\n   * Green color value\n   */\n  g?: number;\n  /**\n   * Blue color value\n   */\n  b?: number;\n};\nexport type Speaker = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  speaker_id: number;\n  /**\n   *\n   */\n  audio_url: string | Blob | File;\n};\nexport type Track = {\n  /**\n   * Unique identifier for the track\n   */\n  id: string;\n  /**\n   * Type of track ('video' or 'audio')\n   */\n  type: string;\n  /**\n   * List of keyframes that make up this track\n   */\n  keyframes: Array<Keyframe>;\n};\nexport type Trajectory = {\n  /**\n   * X coordinate of the motion trajectory\n   */\n  x: number;\n  /**\n   * Y coordinate of the motion trajectory\n   */\n  y: number;\n};\nexport type TranscriptionWord = {\n  /**\n   * The transcribed word or audio event\n   */\n  text: string;\n  /**\n   * Start time in seconds\n   */\n  start: number;\n  /**\n   * End time in seconds\n   */\n  end: number;\n  /**\n   * Type of element (word, spacing, or audio_event)\n   */\n  type: string;\n  /**\n   * Speaker identifier if diarization was enabled\n   */\n  speaker_id?: string;\n};\nexport type Turn = {\n  /**\n   *\n   */\n  speaker_id: number;\n  /**\n   *\n   */\n  text: string;\n};\nexport type Validation = {\n  /**\n   * The prompt to use for validation.\n   */\n  prompt: string;\n  /**\n   * An image to use for image-to-video validation. If provided for one validation, _all_ validation inputs must have an image.\n   */\n  image_url?: string | Blob | File;\n};\nexport type Video = {\n  /**\n   * Type of media (always 'video') Default value: `\"video\"`\n   */\n  media_type?: string;\n  /**\n   * URL where the media file can be accessed\n   */\n  url: string;\n  /**\n   * MIME type of the media file\n   */\n  content_type: string;\n  /**\n   * Original filename of the media\n   */\n  file_name: string;\n  /**\n   * Size of the file in bytes\n   */\n  file_size: number;\n  /**\n   * Duration of the media in seconds\n   */\n  duration: number;\n  /**\n   * Overall bitrate of the media in bits per second\n   */\n  bitrate: number;\n  /**\n   * Codec used to encode the media\n   */\n  codec: string;\n  /**\n   * Container format of the media file (e.g., 'mp4', 'mov')\n   */\n  container: string;\n  /**\n   * Frames per second\n   */\n  fps: number;\n  /**\n   * Total number of frames in the video\n   */\n  frame_count: number;\n  /**\n   * Time base used for frame timestamps\n   */\n  timebase: string;\n  /**\n   * Video resolution information\n   */\n  resolution: Resolution;\n  /**\n   * Detailed video format information\n   */\n  format: VideoFormat;\n  /**\n   * Audio track information if video has audio\n   */\n  audio?: AudioTrack;\n  /**\n   * URL of the extracted first frame\n   */\n  start_frame_url?: string | Blob | File;\n  /**\n   * URL of the extracted last frame\n   */\n  end_frame_url?: string | Blob | File;\n};\nexport type VideoCondition = {\n  /**\n   * The URL of the video to use as input.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The frame number to start the condition on.\n   */\n  start_frame_number?: number;\n  /**\n   * The strength of the condition. Default value: `1`\n   */\n  strength?: number;\n};\nexport type VideoFormat = {\n  /**\n   * Container format of the video\n   */\n  container: string;\n  /**\n   * Video codec used (e.g., 'h264')\n   */\n  video_codec: string;\n  /**\n   * Codec profile (e.g., 'main', 'high')\n   */\n  profile: string;\n  /**\n   * Codec level (e.g., 4.1)\n   */\n  level: number;\n  /**\n   * Pixel format used (e.g., 'yuv420p')\n   */\n  pixel_format: string;\n  /**\n   * Video bitrate in bits per second\n   */\n  bitrate: number;\n};\nexport type VoiceSetting = {\n  /**\n   * Predefined voice ID to use for synthesis Default value: `\"Wise_Woman\"`\n   */\n  voice_id?: string;\n  /**\n   * Speech speed (0.5-2.0) Default value: `1`\n   */\n  speed?: number;\n  /**\n   * Volume (0-10) Default value: `1`\n   */\n  vol?: number;\n  /**\n   * Voice pitch (-12 to 12)\n   */\n  pitch?: number;\n  /**\n   * Emotion of the generated speech\n   */\n  emotion?:\n    | \"happy\"\n    | \"sad\"\n    | \"angry\"\n    | \"fearful\"\n    | \"disgusted\"\n    | \"surprised\"\n    | \"neutral\";\n  /**\n   * Enables English text normalization to improve number reading performance, with a slight increase in latency\n   */\n  english_normalization?: boolean;\n};\nexport type WordTime = {\n  /**\n   * The word to inpaint.\n   */\n  text: string;\n  /**\n   * The start and end timestamp of the word.\n   */\n  timestamp: Array<void>;\n};\nexport type AceStepAudioInpaintInput = {\n  /**\n   * URL of the audio file to be inpainted.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Whether the start time is relative to the start or end of the audio. Default value: `\"start\"`\n   */\n  start_time_relative_to?: \"start\" | \"end\";\n  /**\n   * start time in seconds for the inpainting process.\n   */\n  start_time?: number;\n  /**\n   * Whether the end time is relative to the start or end of the audio. Default value: `\"start\"`\n   */\n  end_time_relative_to?: \"start\" | \"end\";\n  /**\n   * end time in seconds for the inpainting process. Default value: `30`\n   */\n  end_time?: number;\n  /**\n   * Comma-separated list of genre tags to control the style of the generated audio.\n   */\n  tags: string;\n  /**\n   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `\"\"`\n   */\n  lyrics?: string;\n  /**\n   * Variance for the inpainting process. Higher values can lead to more diverse results. Default value: `0.5`\n   */\n  variance?: number;\n  /**\n   * Number of steps to generate the audio. Default value: `27`\n   */\n  number_of_steps?: number;\n  /**\n   * Random seed for reproducibility. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Scheduler to use for the generation process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"heun\";\n  /**\n   * Type of CFG to use for the generation process. Default value: `\"apg\"`\n   */\n  guidance_type?: \"cfg\" | \"apg\" | \"cfg_star\";\n  /**\n   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`\n   */\n  granularity_scale?: number;\n  /**\n   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`\n   */\n  guidance_interval?: number;\n  /**\n   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.\n   */\n  guidance_interval_decay?: number;\n  /**\n   * Guidance scale for the generation. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * Minimum guidance scale for the generation after the decay. Default value: `3`\n   */\n  minimum_guidance_scale?: number;\n  /**\n   * Tag guidance scale for the generation. Default value: `5`\n   */\n  tag_guidance_scale?: number;\n  /**\n   * Lyric guidance scale for the generation. Default value: `1.5`\n   */\n  lyric_guidance_scale?: number;\n};\nexport type AceStepAudioInpaintOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The random seed used for the generation process.\n   */\n  seed: number;\n  /**\n   * The genre tags used in the generation process.\n   */\n  tags: string;\n  /**\n   * The lyrics used in the generation process.\n   */\n  lyrics: string;\n};\nexport type AceStepAudioOutpaintInput = {\n  /**\n   * URL of the audio file to be outpainted.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Duration in seconds to extend the audio from the start.\n   */\n  extend_before_duration?: number;\n  /**\n   * Duration in seconds to extend the audio from the end. Default value: `30`\n   */\n  extend_after_duration?: number;\n  /**\n   * Comma-separated list of genre tags to control the style of the generated audio.\n   */\n  tags: string;\n  /**\n   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `\"\"`\n   */\n  lyrics?: string;\n  /**\n   * Number of steps to generate the audio. Default value: `27`\n   */\n  number_of_steps?: number;\n  /**\n   * Random seed for reproducibility. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Scheduler to use for the generation process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"heun\";\n  /**\n   * Type of CFG to use for the generation process. Default value: `\"apg\"`\n   */\n  guidance_type?: \"cfg\" | \"apg\" | \"cfg_star\";\n  /**\n   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`\n   */\n  granularity_scale?: number;\n  /**\n   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`\n   */\n  guidance_interval?: number;\n  /**\n   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.\n   */\n  guidance_interval_decay?: number;\n  /**\n   * Guidance scale for the generation. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * Minimum guidance scale for the generation after the decay. Default value: `3`\n   */\n  minimum_guidance_scale?: number;\n  /**\n   * Tag guidance scale for the generation. Default value: `5`\n   */\n  tag_guidance_scale?: number;\n  /**\n   * Lyric guidance scale for the generation. Default value: `1.5`\n   */\n  lyric_guidance_scale?: number;\n};\nexport type AceStepAudioOutpaintOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The random seed used for the generation process.\n   */\n  seed: number;\n  /**\n   * The genre tags used in the generation process.\n   */\n  tags: string;\n  /**\n   * The lyrics used in the generation process.\n   */\n  lyrics: string;\n};\nexport type AceStepAudioToAudioInput = {\n  /**\n   * URL of the audio file to be outpainted.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Whether to edit the lyrics only or remix the audio. Default value: `\"remix\"`\n   */\n  edit_mode?: \"lyrics\" | \"remix\";\n  /**\n   * Original tags of the audio file.\n   */\n  original_tags: string;\n  /**\n   * Original lyrics of the audio file. Default value: `\"\"`\n   */\n  original_lyrics?: string;\n  /**\n   * Comma-separated list of genre tags to control the style of the generated audio.\n   */\n  tags: string;\n  /**\n   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `\"\"`\n   */\n  lyrics?: string;\n  /**\n   * Number of steps to generate the audio. Default value: `27`\n   */\n  number_of_steps?: number;\n  /**\n   * Random seed for reproducibility. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Scheduler to use for the generation process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"heun\";\n  /**\n   * Type of CFG to use for the generation process. Default value: `\"apg\"`\n   */\n  guidance_type?: \"cfg\" | \"apg\" | \"cfg_star\";\n  /**\n   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`\n   */\n  granularity_scale?: number;\n  /**\n   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`\n   */\n  guidance_interval?: number;\n  /**\n   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.\n   */\n  guidance_interval_decay?: number;\n  /**\n   * Guidance scale for the generation. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * Minimum guidance scale for the generation after the decay. Default value: `3`\n   */\n  minimum_guidance_scale?: number;\n  /**\n   * Tag guidance scale for the generation. Default value: `5`\n   */\n  tag_guidance_scale?: number;\n  /**\n   * Lyric guidance scale for the generation. Default value: `1.5`\n   */\n  lyric_guidance_scale?: number;\n  /**\n   * Original seed of the audio file.\n   */\n  original_seed?: number;\n};\nexport type AceStepAudioToAudioOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The random seed used for the generation process.\n   */\n  seed: number;\n  /**\n   * The genre tags used in the generation process.\n   */\n  tags: string;\n  /**\n   * The lyrics used in the generation process.\n   */\n  lyrics: string;\n};\nexport type AceStepInput = {\n  /**\n   * Comma-separated list of genre tags to control the style of the generated audio.\n   */\n  tags: string;\n  /**\n   * Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song. Default value: `\"\"`\n   */\n  lyrics?: string;\n  /**\n   * The duration of the generated audio in seconds. Default value: `60`\n   */\n  duration?: number;\n  /**\n   * Number of steps to generate the audio. Default value: `27`\n   */\n  number_of_steps?: number;\n  /**\n   * Random seed for reproducibility. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Scheduler to use for the generation process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"heun\";\n  /**\n   * Type of CFG to use for the generation process. Default value: `\"apg\"`\n   */\n  guidance_type?: \"cfg\" | \"apg\" | \"cfg_star\";\n  /**\n   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`\n   */\n  granularity_scale?: number;\n  /**\n   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`\n   */\n  guidance_interval?: number;\n  /**\n   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.\n   */\n  guidance_interval_decay?: number;\n  /**\n   * Guidance scale for the generation. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * Minimum guidance scale for the generation after the decay. Default value: `3`\n   */\n  minimum_guidance_scale?: number;\n  /**\n   * Tag guidance scale for the generation. Default value: `5`\n   */\n  tag_guidance_scale?: number;\n  /**\n   * Lyric guidance scale for the generation. Default value: `1.5`\n   */\n  lyric_guidance_scale?: number;\n};\nexport type AceStepOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The random seed used for the generation process.\n   */\n  seed: number;\n  /**\n   * The genre tags used in the generation process.\n   */\n  tags: string;\n  /**\n   * The lyrics used in the generation process.\n   */\n  lyrics: string;\n};\nexport type AceStepPromptToAudioInput = {\n  /**\n   * Prompt to control the style of the generated audio. This will be used to generate tags and lyrics.\n   */\n  prompt: string;\n  /**\n   * Whether to generate an instrumental version of the audio.\n   */\n  instrumental?: boolean;\n  /**\n   * The duration of the generated audio in seconds. Default value: `60`\n   */\n  duration?: number;\n  /**\n   * Number of steps to generate the audio. Default value: `27`\n   */\n  number_of_steps?: number;\n  /**\n   * Random seed for reproducibility. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Scheduler to use for the generation process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"heun\";\n  /**\n   * Type of CFG to use for the generation process. Default value: `\"apg\"`\n   */\n  guidance_type?: \"cfg\" | \"apg\" | \"cfg_star\";\n  /**\n   * Granularity scale for the generation process. Higher values can reduce artifacts. Default value: `10`\n   */\n  granularity_scale?: number;\n  /**\n   * Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps) Default value: `0.5`\n   */\n  guidance_interval?: number;\n  /**\n   * Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.\n   */\n  guidance_interval_decay?: number;\n  /**\n   * Guidance scale for the generation. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * Minimum guidance scale for the generation after the decay. Default value: `3`\n   */\n  minimum_guidance_scale?: number;\n  /**\n   * Tag guidance scale for the generation. Default value: `5`\n   */\n  tag_guidance_scale?: number;\n  /**\n   * Lyric guidance scale for the generation. Default value: `1.5`\n   */\n  lyric_guidance_scale?: number;\n};\nexport type AceStepPromptToAudioOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The random seed used for the generation process.\n   */\n  seed: number;\n  /**\n   * The genre tags used in the generation process.\n   */\n  tags: string;\n  /**\n   * The lyrics used in the generation process.\n   */\n  lyrics: string;\n};\nexport type AdvancedFaceSwapInput = {\n  /**\n   * User's face image to face swap FROM\n   */\n  face_image_0: Image;\n  /**\n   * The gender of the person in the face image.\n   */\n  gender_0: \"male\" | \"female\" | \"non-binary\";\n  /**\n   * (Optional) The Second face image to face swap FROM\n   */\n  face_image_1?: Image;\n  /**\n   * The gender of the person in the second face image.\n   */\n  gender_1?: \"male\" | \"female\" | \"non-binary\";\n  /**\n   * The target image to face swap TO\n   */\n  target_image?: Image;\n  /**\n   * The type of face swap workflow. target_hair = preserve target's hair. user_hair = preserve user's hair.\n   */\n  workflow_type: \"user_hair\" | \"target_hair\";\n  /**\n   * Apply 2x upscale and boost quality. Upscaling will refine the image and make the subjects brighter. Default value: `true`\n   */\n  upscale?: boolean;\n  /**\n   * (Beta) Apply detailer to the image. Detailer will improve certain image details, but slightly increase generation time.\n   */\n  detailer?: boolean;\n};\nexport type AdvancedFaceSwapOutput = {\n  /**\n   * The mirrored image.\n   */\n  image: Image;\n};\nexport type AgeProgressionInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The age change to apply. Default value: `\"20 years older\"`\n   */\n  prompt?: string;\n};\nexport type AgeProgressionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type AiAvatarInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the audio file.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `145`\n   */\n  num_frames?: number;\n  /**\n   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `\"480p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * The acceleration level to use for generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type AiAvatarMultiInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the Person 1 audio file.\n   */\n  first_audio_url: string | Blob | File;\n  /**\n   * The URL of the Person 2 audio file.\n   */\n  second_audio_url?: string | Blob | File;\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `181`\n   */\n  num_frames?: number;\n  /**\n   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `\"480p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `81`\n   */\n  seed?: number;\n  /**\n   * Whether to use only the first audio file.\n   */\n  use_only_first_audio?: boolean;\n  /**\n   * The acceleration level to use for generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type AiAvatarMultiOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type AiAvatarMultiTextInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The text input to guide video generation.\n   */\n  first_text_input: string;\n  /**\n   * The text input to guide video generation.\n   */\n  second_text_input: string;\n  /**\n   * The first person's voice to use for speech generation Default value: `\"Sarah\"`\n   */\n  voice1?:\n    | \"Aria\"\n    | \"Roger\"\n    | \"Sarah\"\n    | \"Laura\"\n    | \"Charlie\"\n    | \"George\"\n    | \"Callum\"\n    | \"River\"\n    | \"Liam\"\n    | \"Charlotte\"\n    | \"Alice\"\n    | \"Matilda\"\n    | \"Will\"\n    | \"Jessica\"\n    | \"Eric\"\n    | \"Chris\"\n    | \"Brian\"\n    | \"Daniel\"\n    | \"Lily\"\n    | \"Bill\";\n  /**\n   * The second person's voice to use for speech generation Default value: `\"Roger\"`\n   */\n  voice2?:\n    | \"Aria\"\n    | \"Roger\"\n    | \"Sarah\"\n    | \"Laura\"\n    | \"Charlie\"\n    | \"George\"\n    | \"Callum\"\n    | \"River\"\n    | \"Liam\"\n    | \"Charlotte\"\n    | \"Alice\"\n    | \"Matilda\"\n    | \"Will\"\n    | \"Jessica\"\n    | \"Eric\"\n    | \"Chris\"\n    | \"Brian\"\n    | \"Daniel\"\n    | \"Lily\"\n    | \"Bill\";\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `191`\n   */\n  num_frames?: number;\n  /**\n   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `\"480p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `81`\n   */\n  seed?: number;\n  /**\n   * The acceleration level to use for generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type AiAvatarMultiTextOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type AiAvatarOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type AiAvatarSingleTextInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The text input to guide video generation.\n   */\n  text_input: string;\n  /**\n   * The voice to use for speech generation\n   */\n  voice:\n    | \"Aria\"\n    | \"Roger\"\n    | \"Sarah\"\n    | \"Laura\"\n    | \"Charlie\"\n    | \"George\"\n    | \"Callum\"\n    | \"River\"\n    | \"Liam\"\n    | \"Charlotte\"\n    | \"Alice\"\n    | \"Matilda\"\n    | \"Will\"\n    | \"Jessica\"\n    | \"Eric\"\n    | \"Chris\"\n    | \"Brian\"\n    | \"Daniel\"\n    | \"Lily\"\n    | \"Bill\";\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `136`\n   */\n  num_frames?: number;\n  /**\n   * Resolution of the video to generate. Must be either 480p or 720p. Default value: `\"480p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * The acceleration level to use for generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type AiAvatarSingleTextOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type AmEngOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type AnyLlmEnterpriseInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n  /**\n   * Name of the model to use. Premium models are charged at 10x the rate of standard models, they include: anthropic/claude-3.5-sonnet, meta-llama/llama-3.2-90b-vision-instruct, anthropic/claude-3.7-sonnet, openai/o3, openai/gpt-5-chat, anthropic/claude-3-5-haiku, google/gemini-pro-1.5, openai/gpt-4o. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.7-sonnet\"\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-5-haiku\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"google/gemini-2.0-flash-001\"\n    | \"meta-llama/llama-3.2-1b-instruct\"\n    | \"meta-llama/llama-3.2-3b-instruct\"\n    | \"meta-llama/llama-3.1-8b-instruct\"\n    | \"meta-llama/llama-3.1-70b-instruct\"\n    | \"openai/gpt-oss-120b\"\n    | \"openai/gpt-4o-mini\"\n    | \"openai/gpt-4o\"\n    | \"openai/o3\"\n    | \"openai/gpt-5-chat\"\n    | \"openai/gpt-5-mini\"\n    | \"openai/gpt-5-nano\"\n    | \"meta-llama/llama-4-maverick\"\n    | \"meta-llama/llama-4-scout\";\n};\nexport type AnyLlmEnterpriseOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Generated reasoning for the final answer\n   */\n  reasoning?: string;\n  /**\n   * Whether the output is partial\n   */\n  partial?: boolean;\n  /**\n   * Error message if an error occurred\n   */\n  error?: string;\n};\nexport type Audio2VideoInput = {\n  /**\n   * The avatar to use for the video\n   */\n  avatar_id:\n    | \"emily_vertical_primary\"\n    | \"emily_vertical_secondary\"\n    | \"marcus_vertical_primary\"\n    | \"marcus_vertical_secondary\"\n    | \"mira_vertical_primary\"\n    | \"mira_vertical_secondary\"\n    | \"jasmine_vertical_primary\"\n    | \"jasmine_vertical_secondary\"\n    | \"jasmine_vertical_walking\"\n    | \"aisha_vertical_walking\"\n    | \"elena_vertical_primary\"\n    | \"elena_vertical_secondary\"\n    | \"any_male_vertical_primary\"\n    | \"any_female_vertical_primary\"\n    | \"any_male_vertical_secondary\"\n    | \"any_female_vertical_secondary\"\n    | \"any_female_vertical_walking\"\n    | \"emily_primary\"\n    | \"emily_side\"\n    | \"marcus_primary\"\n    | \"marcus_side\"\n    | \"aisha_walking\"\n    | \"elena_primary\"\n    | \"elena_side\"\n    | \"any_male_primary\"\n    | \"any_female_primary\"\n    | \"any_male_side\"\n    | \"any_female_side\";\n  /**\n   *\n   */\n  audio_url: string | Blob | File;\n};\nexport type AudioInput = {\n  /**\n   * The URL of the audio file.\n   */\n  audio_url: string | Blob | File;\n};\nexport type AudioOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The prompt used to generate the audio.\n   */\n  prompt: string;\n};\nexport type AuraFlowInput = {\n  /**\n   * The prompt to generate images from\n   */\n  prompt: string;\n  /**\n   * The number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The seed to use for generating images\n   */\n  seed?: number;\n  /**\n   * Classifier free guidance scale Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to take Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to perform prompt expansion (recommended) Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type AuraFlowOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * The seed used to generate the images\n   */\n  seed: number;\n  /**\n   * The expanded prompt\n   */\n  prompt: string;\n};\nexport type AuraSrInput = {\n  /**\n   * URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Upscaling factor. More coming soon. Default value: `\"4\"`\n   */\n  upscaling_factor?: \"4\";\n  /**\n   * Whether to use overlapping tiles for upscaling. Setting this to true helps remove seams but doubles the inference time.\n   */\n  overlapping_tiles?: boolean;\n  /**\n   * Checkpoint to use for upscaling. More coming soon. Default value: `\"v1\"`\n   */\n  checkpoint?: \"v1\" | \"v2\";\n};\nexport type AuraSrOutput = {\n  /**\n   * Upscaled image\n   */\n  image: Image;\n  /**\n   * Timings for each step in the pipeline.\n   */\n  timings: any;\n};\nexport type AutoCaptionInput = {\n  /**\n   * URL to the .mp4 video with audio. Only videos of size <100MB are allowed.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation. Default value: `\"white\"`\n   */\n  txt_color?: string;\n  /**\n   * Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file Default value: `\"Standard\"`\n   */\n  txt_font?: string;\n  /**\n   * Size of text in generated captions. Default value: `24`\n   */\n  font_size?: number;\n  /**\n   * Width of the text strokes in pixels Default value: `1`\n   */\n  stroke_width?: number;\n  /**\n   * Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0) Default value: `center`\n   */\n  left_align?: string | number;\n  /**\n   * Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0) Default value: `center`\n   */\n  top_align?: string | number;\n  /**\n   * Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once. Default value: `1.5`\n   */\n  refresh_interval?: number;\n};\nexport type AutoCaptionOutput = {\n  /**\n   * URL to the caption .mp4 video.\n   */\n  video_url: string | Blob | File;\n};\nexport type AvatarsAudioToVideoInput = {\n  /**\n   * The avatar to use for the video\n   */\n  avatar_id:\n    | \"emily_vertical_primary\"\n    | \"emily_vertical_secondary\"\n    | \"marcus_vertical_primary\"\n    | \"marcus_vertical_secondary\"\n    | \"mira_vertical_primary\"\n    | \"mira_vertical_secondary\"\n    | \"jasmine_vertical_primary\"\n    | \"jasmine_vertical_secondary\"\n    | \"jasmine_vertical_walking\"\n    | \"aisha_vertical_walking\"\n    | \"elena_vertical_primary\"\n    | \"elena_vertical_secondary\"\n    | \"any_male_vertical_primary\"\n    | \"any_female_vertical_primary\"\n    | \"any_male_vertical_secondary\"\n    | \"any_female_vertical_secondary\"\n    | \"any_female_vertical_walking\"\n    | \"emily_primary\"\n    | \"emily_side\"\n    | \"marcus_primary\"\n    | \"marcus_side\"\n    | \"aisha_walking\"\n    | \"elena_primary\"\n    | \"elena_side\"\n    | \"any_male_primary\"\n    | \"any_female_primary\"\n    | \"any_male_side\"\n    | \"any_female_side\";\n  /**\n   *\n   */\n  audio_url: string | Blob | File;\n};\nexport type AvatarsAudioToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n};\nexport type AvatarsTextToVideoInput = {\n  /**\n   * The avatar to use for the video\n   */\n  avatar_id:\n    | \"emily_vertical_primary\"\n    | \"emily_vertical_secondary\"\n    | \"marcus_vertical_primary\"\n    | \"marcus_vertical_secondary\"\n    | \"mira_vertical_primary\"\n    | \"mira_vertical_secondary\"\n    | \"jasmine_vertical_primary\"\n    | \"jasmine_vertical_secondary\"\n    | \"jasmine_vertical_walking\"\n    | \"aisha_vertical_walking\"\n    | \"elena_vertical_primary\"\n    | \"elena_vertical_secondary\"\n    | \"any_male_vertical_primary\"\n    | \"any_female_vertical_primary\"\n    | \"any_male_vertical_secondary\"\n    | \"any_female_vertical_secondary\"\n    | \"any_female_vertical_walking\"\n    | \"emily_primary\"\n    | \"emily_side\"\n    | \"marcus_primary\"\n    | \"marcus_side\"\n    | \"aisha_walking\"\n    | \"elena_primary\"\n    | \"elena_side\"\n    | \"any_male_primary\"\n    | \"any_female_primary\"\n    | \"any_male_side\"\n    | \"any_female_side\";\n  /**\n   *\n   */\n  text: string;\n};\nexport type AvatarsTextToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n};\nexport type BabyVersionInput = {\n  /**\n   * URL of the image to transform into a baby version.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BabyVersionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type BackgroundChangeInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The desired background to apply. Default value: `\"beach sunset with palm trees\"`\n   */\n  prompt?: string;\n};\nexport type BackgroundChangeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type BagelEditInput = {\n  /**\n   * The prompt to edit the image with.\n   */\n  prompt: string;\n  /**\n   * The seed to use for the generation.\n   */\n  seed?: number;\n  /**\n   * Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%.\n   */\n  use_thought?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The image to edit.\n   */\n  image_url: string | Blob | File;\n};\nexport type BagelEditOutput = {\n  /**\n   * The edited images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type BagelInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The seed to use for the generation.\n   */\n  seed?: number;\n  /**\n   * Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%.\n   */\n  use_thought?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type BagelOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type BagelUnderstandInput = {\n  /**\n   * The image for the query.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to query the image with.\n   */\n  prompt: string;\n  /**\n   * The seed to use for the generation.\n   */\n  seed?: number;\n};\nexport type BagelUnderstandOutput = {\n  /**\n   * The answer to the query.\n   */\n  text: string;\n  /**\n   * The seed used for the generation.\n   */\n  seed: number;\n  /**\n   * The query used for the generation.\n   */\n  prompt: string;\n  /**\n   * The timings of the generation.\n   */\n  timings: any;\n};\nexport type BaseFlux1ImageToInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseFlux1Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseFlux1ReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseImageToInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKontextEditInput = {\n  /**\n   * The URL of the image to edit.\n   *\n   * Max width: 14142px, Max height: 14142px, Timeout: 20s\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to edit the image.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * Determines how the output resolution is set for image editing.\n   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.\n   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).\n   * Apart from these, a few aspect ratios are also supported. Default value: `\"match_input\"`\n   */\n  resolution_mode?:\n    | \"auto\"\n    | \"match_input\"\n    | \"1:1\"\n    | \"16:9\"\n    | \"21:9\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"9:16\"\n    | \"9:21\";\n};\nexport type BaseKontextImg2ImgInput = {\n  /**\n   * The prompt for the image to image task.\n   */\n  prompt: string;\n  /**\n   * The URL of the image for image-to-image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`\n   */\n  strength?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKontextInpaintInput = {\n  /**\n   * The URL of the image to be inpainted.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt for the image to image task.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * The URL of the reference image for inpainting.\n   */\n  reference_image_url: string | Blob | File;\n  /**\n   * The URL of the mask for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`\n   */\n  strength?: number;\n};\nexport type BaseKontextInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKreaFlux1ImageToInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKreaFlux1Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKreaFlux1ReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKreaImageToInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKreaInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseKreaReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type BaseTextToImageInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BatchMoonDreamOutput = {\n  /**\n   * URL to the generated captions JSON file containing filename-caption pairs.\n   */\n  captions_file: File;\n  /**\n   * List of generated captions\n   */\n  outputs: Array<string>;\n};\nexport type BatchQueryInput = {\n  /**\n   * List of image URLs to be processed (maximum 32 images)\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Single prompt to apply to all images\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type BboxInput = {\n  /**\n   * The URL of the image to remove objects from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * List of bounding box coordinates to erase (only one box prompt is supported)\n   */\n  box_prompts?: Array<BBoxPromptBase>;\n  /**\n   *  Default value: `\"best_quality\"`\n   */\n  model?: \"low_quality\" | \"medium_quality\" | \"high_quality\" | \"best_quality\";\n  /**\n   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`\n   */\n  mask_expansion?: number;\n};\nexport type BenV2ImageInput = {\n  /**\n   * URL of image to be used for background removal\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type BenV2ImageOutput = {\n  /**\n   * The output image after background removal.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type BenV2VideoInput = {\n  /**\n   * URL of video to be used for background removal.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]\n   */\n  background_color?: Array<void>;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type BenV2VideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type BGRemoveBatchedInput = {\n  /**\n   * List of image URLs to be processed (maximum 32 images)\n   */\n  images_data_url: string | Blob | File;\n};\nexport type BGRemoveBatchedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n};\nexport type BGRemoveInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BGRemoveOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type BGReplaceInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the reference image to be used for generating the new background. Use \"\" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt?: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to refine prompt Default value: `true`\n   */\n  refine_prompt?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BGReplaceOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BlurInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of blur to apply Default value: `\"gaussian\"`\n   */\n  blur_type?: \"gaussian\" | \"kuwahara\";\n  /**\n   * Blur radius Default value: `3`\n   */\n  blur_radius?: number;\n  /**\n   * Sigma for Gaussian blur Default value: `1`\n   */\n  blur_sigma?: number;\n};\nexport type BlurOutput = {\n  /**\n   * The processed images with blur effect\n   */\n  images: Array<Image>;\n};\nexport type BrEngOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type BriaBackgroundRemoveInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaBackgroundRemoveOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type BriaBackgroundReplaceInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the reference image to be used for generating the new background. Use \"\" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt?: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to refine prompt Default value: `true`\n   */\n  refine_prompt?: boolean;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaBackgroundReplaceOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaEraserInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'. Default value: `\"manual\"`\n   */\n  mask_type?: \"manual\" | \"automatic\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, attempts to preserve the alpha channel of the input image.\n   */\n  preserve_alpha?: boolean;\n};\nexport type BriaEraserOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type BriaExpandInput = {\n  /**\n   * The URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels.\n   */\n  canvas_size: Array<number>;\n  /**\n   * The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results.\n   */\n  original_image_size: Array<number>;\n  /**\n   * The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped).\n   */\n  original_image_location: Array<number>;\n  /**\n   * Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional.\n   */\n  seed?: number;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaExpandOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaGenfillInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaGenfillOutput = {\n  /**\n   * Generated Images\n   */\n  images: Array<Image>;\n};\nexport type BriaProductShotInput = {\n  /**\n   * The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters.\n   */\n  scene_description?: string;\n  /**\n   * The URL of the reference image to be used for generating the new scene or background for the product shot. Use \"\" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * Whether to optimize the scene description Default value: `true`\n   */\n  optimize_description?: boolean;\n  /**\n   * The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product. Default value: `\"manual_placement\"`\n   */\n  placement_type?:\n    | \"original\"\n    | \"automatic\"\n    | \"manual_placement\"\n    | \"manual_padding\";\n  /**\n   * This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio.\n   */\n  original_quality?: boolean;\n  /**\n   * The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement.\n   */\n  shot_size?: Array<number>;\n  /**\n   * If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request. Default value: `\"bottom_center\"`\n   */\n  manual_placement_selection?:\n    | \"upper_left\"\n    | \"upper_right\"\n    | \"bottom_left\"\n    | \"bottom_right\"\n    | \"right_center\"\n    | \"left_center\"\n    | \"upper_center\"\n    | \"bottom_center\"\n    | \"center_vertical\"\n    | \"center_horizontal\";\n  /**\n   * The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API.\n   */\n  padding_values?: Array<number>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaProductShotOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n};\nexport type BriaReimagineInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The URL of the structure reference image. Use \"\" to leave empty. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  structure_image_url?: string | Blob | File;\n  /**\n   * The influence of the structure reference on the generated image. Default value: `0.75`\n   */\n  structure_ref_influence?: number;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaReimagineOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaTextToImageBaseInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `4`\n   */\n  num_images?: number;\n  /**\n   * The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.\n   */\n  prompt_enhancement?: boolean;\n  /**\n   * Which medium should be included in your generated images. This parameter is optional.\n   */\n  medium?: \"photography\" | \"art\";\n  /**\n   * Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference.\n   */\n  guidance?: Array<GuidanceInput>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaTextToImageBaseOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaTextToImageFastInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `4`\n   */\n  num_images?: number;\n  /**\n   * The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `8`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.\n   */\n  prompt_enhancement?: boolean;\n  /**\n   * Which medium should be included in your generated images. This parameter is optional.\n   */\n  medium?: \"photography\" | \"art\";\n  /**\n   * Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference.\n   */\n  guidance?: Array<GuidanceInput>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaTextToImageFastOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BriaTextToImageHdInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `4`\n   */\n  num_images?: number;\n  /**\n   * The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images.\n   */\n  prompt_enhancement?: boolean;\n  /**\n   * Which medium should be included in your generated images. This parameter is optional.\n   */\n  medium?: \"photography\" | \"art\";\n  /**\n   * Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference.\n   */\n  guidance?: Array<GuidanceInput>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BriaTextToImageHdOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type BroccoliHaircutInput = {\n  /**\n   * URL of the image to apply broccoli haircut style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type BroccoliHaircutOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type BrPortugeseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type BytedanceDreaminaV31TextToImageInput = {\n  /**\n   * The text prompt used to generate the image\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Width and height must be between 512 and 2048.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Whether to use an LLM to enhance the prompt\n   */\n  enhance_prompt?: boolean;\n  /**\n   * Number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed to control the stochasticity of image generation.\n   */\n  seed?: number;\n};\nexport type BytedanceDreaminaV31TextToImageOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type BytedanceOmnihumanInput = {\n  /**\n   * The URL of the image used to generate the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the audio file to generate the video. Audio must be under 30s long.\n   */\n  audio_url: string | Blob | File;\n};\nexport type BytedanceOmnihumanOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Duration of audio input/video output as used for billing.\n   */\n  duration: number;\n};\nexport type BytedanceSeedanceV1LiteImageToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n  /**\n   * The URL of the image used to generate video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the image the video ends with. Defaults to None.\n   */\n  end_image_url?: string | Blob | File;\n};\nexport type BytedanceSeedanceV1LiteImageToVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type BytedanceSeedanceV1LiteTextToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\" | \"9:21\";\n  /**\n   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n};\nexport type BytedanceSeedanceV1LiteTextToVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type BytedanceSeedanceV1ProImageToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `\"1080p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n  /**\n   * The URL of the image used to generate video\n   */\n  image_url: string | Blob | File;\n};\nexport type BytedanceSeedanceV1ProImageToVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type BytedanceSeedanceV1ProTextToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"21:9\" | \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `\"1080p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n};\nexport type BytedanceSeedanceV1ProTextToVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type BytedanceSeededitV3EditImageInput = {\n  /**\n   * The text prompt used to edit the image\n   */\n  prompt: string;\n  /**\n   * URL of the image to be edited.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `0.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Random seed to control the stochasticity of image generation.\n   */\n  seed?: number;\n};\nexport type BytedanceSeededitV3EditImageOutput = {\n  /**\n   * Generated image\n   */\n  image: Image;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type BytedanceSeedreamV3TextToImageInput = {\n  /**\n   * The text prompt used to generate the image\n   */\n  prompt: string;\n  /**\n   * Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed to control the stochasticity of image generation.\n   */\n  seed?: number;\n};\nexport type BytedanceSeedreamV3TextToImageOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type CalligrapherInput = {\n  /**\n   * Base64-encoded source image with drawn mask layers\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * Base64-encoded mask image (optional if using auto_mask_generation)\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Optional base64 reference image for style\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Text prompt to inpaint or customize\n   */\n  prompt: string;\n  /**\n   * How many images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Target image size for generation\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Guidance or strength scale for the model Default value: `1`\n   */\n  cfg_scale?: number;\n  /**\n   * Number of inference steps (1-100) Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Random seed for reproducibility\n   */\n  seed?: number;\n  /**\n   * Whether to prepend context reference to the input Default value: `true`\n   */\n  use_context?: boolean;\n  /**\n   * Whether to automatically generate mask from detected text\n   */\n  auto_mask_generation?: boolean;\n  /**\n   * Source text to replace (if empty, masks all detected text) Default value: `\"\"`\n   */\n  source_text?: string;\n};\nexport type CalligrapherOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n};\nexport type CartoonifyInput = {\n  /**\n   * URL of the image to apply Pixar style to\n   */\n  image_url: string | Blob | File;\n  /**\n   * Scale factor for the Pixar effect Default value: `1`\n   */\n  scale?: number;\n  /**\n   * Guidance scale for the generation Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to use CFG zero\n   */\n  use_cfg_zero?: boolean;\n  /**\n   * The seed for image generation. Same seed with same parameters will generate same image.\n   */\n  seed?: number;\n};\nexport type CartoonifyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type CatVtonInput = {\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n  /**\n   * Type of the Cloth to be tried on.\n   *\n   * Options:\n   * upper: Upper body cloth\n   * lower: Lower body cloth\n   * overall: Full body cloth\n   * inner: Inner cloth, like T-shirt inside a jacket\n   * outer: Outer cloth, like a jacket over a T-shirt\n   */\n  cloth_type: \"upper\" | \"lower\" | \"overall\" | \"inner\" | \"outer\";\n  /**\n   * The size of the generated image. Default value: `portrait_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type CatVtonOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n};\nexport type ChainOfZoomInput = {\n  /**\n   * Input image to zoom into\n   */\n  image_url: string | Blob | File;\n  /**\n   * Zoom scale in powers of 2 Default value: `5`\n   */\n  scale?: number;\n  /**\n   * X coordinate of zoom center (0-1) Default value: `0.5`\n   */\n  center_x?: number;\n  /**\n   * Y coordinate of zoom center (0-1) Default value: `0.5`\n   */\n  center_y?: number;\n  /**\n   * Additional prompt text to guide the zoom enhancement Default value: `\"\"`\n   */\n  user_prompt?: string;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ChainOfZoomOutput = {\n  /**\n   * List of intermediate images\n   */\n  images: Array<Image>;\n  /**\n   * Actual linear zoom scale applied\n   */\n  scale: number;\n  /**\n   * Center coordinates used for zoom\n   */\n  zoom_center: Array<number>;\n};\nexport type ChatInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n  /**\n   * Name of the model to use. Premium models are charged at 10x the rate of standard models, they include: anthropic/claude-3.5-sonnet, google/gemini-pro-1.5, meta-llama/llama-3.2-90b-vision-instruct, deepseek/deepseek-r1, openai/o3, openai/gpt-5-chat, anthropic/claude-3-5-haiku, anthropic/claude-3.7-sonnet, openai/gpt-4o. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"deepseek/deepseek-r1\"\n    | \"anthropic/claude-3.7-sonnet\"\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-5-haiku\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"google/gemini-2.0-flash-001\"\n    | \"meta-llama/llama-3.2-1b-instruct\"\n    | \"meta-llama/llama-3.2-3b-instruct\"\n    | \"meta-llama/llama-3.1-8b-instruct\"\n    | \"meta-llama/llama-3.1-70b-instruct\"\n    | \"openai/gpt-oss-120b\"\n    | \"openai/gpt-4o-mini\"\n    | \"openai/gpt-4o\"\n    | \"openai/o3\"\n    | \"openai/gpt-5-chat\"\n    | \"openai/gpt-5-mini\"\n    | \"openai/gpt-5-nano\"\n    | \"meta-llama/llama-4-maverick\"\n    | \"meta-llama/llama-4-scout\";\n};\nexport type ChatterboxhdSpeechToSpeechInput = {\n  /**\n   * URL to the source audio file to be voice-converted.\n   */\n  source_audio_url: string | Blob | File;\n  /**\n   * The voice to use for the speech-to-speech request. If neither target_voice nor target_voice_audio_url are provided, a random target voice will be used.\n   */\n  target_voice?:\n    | \"Aurora\"\n    | \"Blade\"\n    | \"Britney\"\n    | \"Carl\"\n    | \"Cliff\"\n    | \"Richard\"\n    | \"Rico\"\n    | \"Siobhan\"\n    | \"Vicky\";\n  /**\n   * URL to the audio file which represents the voice of the output audio. If provided, this will override the target_voice setting. If neither target_voice nor target_voice_audio_url are provided, the default target voice will be used.\n   */\n  target_voice_audio_url?: string | Blob | File;\n  /**\n   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.\n   */\n  high_quality_audio?: boolean;\n};\nexport type ChatterboxhdSpeechToSpeechOutput = {\n  /**\n   * The generated voice-converted audio file.\n   */\n  audio: Audio;\n};\nexport type ChatterboxhdTextToSpeechInput = {\n  /**\n   * Text to synthesize into speech. Default value: `\"My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next.\"`\n   */\n  text?: string;\n  /**\n   * The voice to use for the TTS request. If neither voice nor audio are provided, a random voice will be used.\n   */\n  voice?:\n    | \"Aurora\"\n    | \"Blade\"\n    | \"Britney\"\n    | \"Carl\"\n    | \"Cliff\"\n    | \"Richard\"\n    | \"Rico\"\n    | \"Siobhan\"\n    | \"Vicky\";\n  /**\n   * URL to the audio sample to use as a voice prompt for zero-shot TTS voice cloning. Providing a audio sample will override the voice setting. If neither voice nor audio_url are provided, a random voice will be used.\n   */\n  audio_url?: string | Blob | File;\n  /**\n   * Controls emotion exaggeration. Range typically 0.25 to 2.0. Default value: `0.5`\n   */\n  exaggeration?: number;\n  /**\n   * Classifier-free guidance scale (CFG) controls the conditioning factor. Range typically 0.2 to 1.0. For expressive or dramatic speech, try lower cfg values (e.g. ~0.3) and increase exaggeration to around 0.7 or higher. If the reference speaker has a fast speaking style, lowering cfg to around 0.3 can improve pacing. Default value: `0.5`\n   */\n  cfg?: number;\n  /**\n   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.\n   */\n  high_quality_audio?: boolean;\n  /**\n   * Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed.\n   */\n  seed?: number;\n  /**\n   * Controls the randomness of generation. Range typically 0.05 to 5. Default value: `0.8`\n   */\n  temperature?: number;\n};\nexport type ChatterboxhdTextToSpeechOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: Audio;\n};\nexport type ChatterboxOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type ChatterboxSpeechToSpeechInput = {\n  /**\n   *\n   */\n  source_audio_url: string | Blob | File;\n  /**\n   * Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio.\n   */\n  target_voice_audio_url?: string | Blob | File;\n};\nexport type ChatterboxSpeechToSpeechOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type ChatterboxTextToSpeechInput = {\n  /**\n   * The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>\n   */\n  text: string;\n  /**\n   * Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio. Default value: `\"https://storage.googleapis.com/chatterbox-demo-samples/prompts/male_rickmorty.mp3\"`\n   */\n  audio_url?: string | Blob | File;\n  /**\n   * Exaggeration factor for the generated speech (0.0 = no exaggeration, 1.0 = maximum exaggeration). Default value: `0.25`\n   */\n  exaggeration?: number;\n  /**\n   * Temperature for generation (higher = more creative). Default value: `0.7`\n   */\n  temperature?: number;\n  /**\n   *  Default value: `0.5`\n   */\n  cfg?: number;\n  /**\n   * Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed..\n   */\n  seed?: number;\n};\nexport type ChromaticAberrationInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Red channel shift amount\n   */\n  red_shift?: number;\n  /**\n   * Red channel shift direction Default value: `\"horizontal\"`\n   */\n  red_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Green channel shift amount\n   */\n  green_shift?: number;\n  /**\n   * Green channel shift direction Default value: `\"horizontal\"`\n   */\n  green_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Blue channel shift amount\n   */\n  blue_shift?: number;\n  /**\n   * Blue channel shift direction Default value: `\"horizontal\"`\n   */\n  blue_direction?: \"horizontal\" | \"vertical\";\n};\nexport type ChromaticAberrationOutput = {\n  /**\n   * The processed images with chromatic aberration effect\n   */\n  images: Array<Image>;\n};\nexport type ClarityUpscalerInput = {\n  /**\n   * The URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for generating the image. Be as descriptive as possible for best results. Default value: `\"masterpiece, best quality, highres\"`\n   */\n  prompt?: string;\n  /**\n   * The upscale factor Default value: `2`\n   */\n  upscale_factor?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want in the image. Default value: `\"(worst quality, low quality, normal quality:2)\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The creativity of the model. The higher the creativity, the more the model will deviate from the prompt.\n   * Refers to the denoise strength of the sampling. Default value: `0.35`\n   */\n  creativity?: number;\n  /**\n   * The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image.\n   * Refers to the strength of the ControlNet. Default value: `0.6`\n   */\n  resemblance?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `18`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of Stable Diffusion\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to false, the safety checker will be disabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type ClarityUpscalerOutput = {\n  /**\n   * The URL of the generated image.\n   */\n  image: Image;\n  /**\n   * The seed used to generate the image.\n   */\n  seed: number;\n  /**\n   * The timings of the different steps in the workflow.\n   */\n  timings: any;\n};\nexport type CodeformerInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Weight of the fidelity factor. Default value: `0.5`\n   */\n  fidelity?: number;\n  /**\n   * Upscaling factor Default value: `2`\n   */\n  upscaling?: number;\n  /**\n   * Should faces etc should be aligned.\n   */\n  aligned?: boolean;\n  /**\n   * Should only center face be restored\n   */\n  only_center_face?: boolean;\n  /**\n   * Should faces be upscaled Default value: `true`\n   */\n  face_upscale?: boolean;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type CodeformerOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type Cogvideox5bInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video.\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n};\nexport type Cogvideox5bOutput = {\n  /**\n   * The URL to the generated video\n   */\n  video: File;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated video. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * The prompt used for generating the video.\n   */\n  prompt: string;\n};\nexport type Cogview4Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type Cogview4Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type CollectionToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ColorCorrectionInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Color temperature adjustment\n   */\n  temperature?: number;\n  /**\n   * Brightness adjustment\n   */\n  brightness?: number;\n  /**\n   * Contrast adjustment\n   */\n  contrast?: number;\n  /**\n   * Saturation adjustment\n   */\n  saturation?: number;\n  /**\n   * Gamma adjustment Default value: `1`\n   */\n  gamma?: number;\n};\nexport type ColorCorrectionOutput = {\n  /**\n   * The processed images with color correction\n   */\n  images: Array<Image>;\n};\nexport type ColorTintInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Tint strength Default value: `1`\n   */\n  tint_strength?: number;\n  /**\n   * Tint color mode Default value: `\"sepia\"`\n   */\n  tint_mode?:\n    | \"sepia\"\n    | \"red\"\n    | \"green\"\n    | \"blue\"\n    | \"cyan\"\n    | \"magenta\"\n    | \"yellow\"\n    | \"purple\"\n    | \"orange\"\n    | \"warm\"\n    | \"cool\"\n    | \"lime\"\n    | \"navy\"\n    | \"vintage\"\n    | \"rose\"\n    | \"teal\"\n    | \"maroon\"\n    | \"peach\"\n    | \"lavender\"\n    | \"olive\";\n};\nexport type ColorTintOutput = {\n  /**\n   * The processed images with color tint effect\n   */\n  images: Array<Image>;\n};\nexport type CombineInput = {\n  /**\n   * URL of the video file to use as the video track\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the audio file to use as the audio track\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Offset in seconds for when the audio should start relative to the video\n   */\n  start_offset?: number;\n};\nexport type CombineOutput = {\n  /**\n   * Output video with merged audio.\n   */\n  video: File;\n};\nexport type ComposeOutput = {\n  /**\n   * URL of the processed video file\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the video's thumbnail image\n   */\n  thumbnail_url: string | Blob | File;\n};\nexport type ControlNetUnionInput = {\n  /**\n   * URL of the image to be used as the control image.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * URL of the mask for the control image.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Control Mode for Flux Controlnet Union. Supported values are:\n   * - canny: Uses the edges for guided generation.\n   * - tile: Uses the tiles for guided generation.\n   * - depth: Utilizes a grayscale depth map for guided generation.\n   * - blur: Adds a blur to the image.\n   * - pose: Uses the pose of the image for guided generation.\n   * - gray: Converts the image to grayscale.\n   * - low-quality: Converts the image to a low-quality image.\n   */\n  control_mode:\n    | \"canny\"\n    | \"tile\"\n    | \"depth\"\n    | \"blur\"\n    | \"pose\"\n    | \"gray\"\n    | \"low-quality\";\n  /**\n   * The scale of the control net weight. This is used to scale the control net weight\n   * before merging it with the base model. Default value: `1`\n   */\n  conditioning_scale?: number;\n  /**\n   * Threshold for mask. Default value: `0.5`\n   */\n  mask_threshold?: number;\n  /**\n   * The percentage of the image to start applying the controlnet in terms of the total timesteps.\n   */\n  start_percentage?: number;\n  /**\n   * The percentage of the image to end applying the controlnet in terms of the total timesteps. Default value: `1`\n   */\n  end_percentage?: number;\n};\nexport type CreateVoiceInput = {\n  /**\n   * Voice name (required, max 255 characters).\n   */\n  name: string;\n  /**\n   * A list of audio URLs used for cloning (must be between 1 and 5 URLs).\n   */\n  samples: Array<AudioInput>;\n};\nexport type CreateVoiceOutput = {\n  /**\n   * The S3 URI of the cloned voice.\n   */\n  voice: string;\n};\nexport type Csm1bInput = {\n  /**\n   * The text to generate an audio from.\n   */\n  scene: Array<Turn>;\n  /**\n   * The context to generate an audio from.\n   */\n  context?: Array<Speaker>;\n};\nexport type Csm1bOutput = {\n  /**\n   * The generated audio.\n   */\n  audio: File | string;\n};\nexport type DdcolorInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type DdcolorOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DepthLoraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * If set to true, the input image will be preprocessed to extract depth information.\n   * This is useful for generating depth maps from images. Default value: `true`\n   */\n  preprocess_depth?: boolean;\n};\nexport type DesaturateInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Desaturation factor Default value: `1`\n   */\n  desaturate_factor?: number;\n  /**\n   * Desaturation method Default value: `\"luminance (Rec.709)\"`\n   */\n  desaturate_method?:\n    | \"luminance (Rec.709)\"\n    | \"luminance (Rec.601)\"\n    | \"average\"\n    | \"lightness\";\n};\nexport type DesaturateOutput = {\n  /**\n   * The processed images with desaturation effect\n   */\n  images: Array<Image>;\n};\nexport type DetectionInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of detection to perform\n   */\n  task_type: \"bbox_detection\" | \"point_detection\" | \"gaze_detection\";\n  /**\n   * Text description of what to detect\n   */\n  detection_prompt: string;\n  /**\n   * Whether to use ensemble for gaze detection\n   */\n  use_ensemble?: boolean;\n};\nexport type DetectionOutput = {\n  /**\n   * Output image with detection visualization\n   */\n  image: Image;\n  /**\n   * Detection results as text\n   */\n  text_output: string;\n};\nexport type DevImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DevReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DevTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DiaCloneOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type DiaOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type DiaTtsInput = {\n  /**\n   * The text to be converted to speech.\n   */\n  text: string;\n};\nexport type DiaTtsOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type DiaTtsVoiceCloneInput = {\n  /**\n   * The text to be converted to speech.\n   */\n  text: string;\n  /**\n   * The URL of the reference audio file.\n   */\n  ref_audio_url: string | Blob | File;\n  /**\n   * The reference text to be used for TTS.\n   */\n  ref_text: string;\n};\nexport type DiaTtsVoiceCloneOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type DifferentialDiffusionInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment.\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation.\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * EasyControl Inputs to use for image generation.\n   */\n  easycontrols?: Array<EasyControlWeight>;\n  /**\n   * Use an image input to influence the generation. Can be used to fill images in masked areas.\n   */\n  fill_image?: ImageFillInput;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.\n   */\n  use_cfg_zero?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * Sigmas schedule for the denoising process.\n   */\n  sigma_schedule?: \"sgm_uniform\";\n  /**\n   * Scheduler for the denoising process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"dpmpp_2m\";\n  /**\n   * Negative prompt to steer the image generation away from unwanted features.\n   * By default, we will be using NAG for processing the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The scale for NAG. Higher values will result in a image that is more distant\n   * to the negative prompt. Default value: `3`\n   */\n  nag_scale?: number;\n  /**\n   * The tau for NAG. Controls the normalization of the hidden state.\n   * Higher values will result in a less aggressive normalization,\n   * but may also lead to unexpected changes with respect to the original image.\n   * Not recommended to change this value. Default value: `2.5`\n   */\n  nag_tau?: number;\n  /**\n   * The alpha value for NAG. This value is used as a final weighting\n   * factor for steering the normalized guidance (positive and negative prompts)\n   * in the direction of the positive prompt. Higher values will result in less\n   * steering on the normalized guidance where lower values will result in\n   * considering the positive prompt guidance more. Default value: `0.25`\n   */\n  nag_alpha?: number;\n  /**\n   * The proportion of steps to apply NAG. After the specified proportion\n   * of steps has been iterated, the remaining steps will use original\n   * attention processors in FLUX. Default value: `0.25`\n   */\n  nag_end?: number;\n  /**\n   * URL of image to use as initial image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of change map.\n   */\n  change_map_image_url: string | Blob | File;\n  /**\n   * The strength to use for differential diffusion. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type DiffrhythmInput = {\n  /**\n   * The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse].\n   */\n  lyrics: string;\n  /**\n   * The URL of the reference audio to use for the music generation.\n   */\n  reference_audio_url?: string | Blob | File;\n  /**\n   * The style prompt to use for the music generation.\n   */\n  style_prompt?: string;\n  /**\n   * The duration of the music to generate. Default value: `\"95s\"`\n   */\n  music_duration?: \"95s\" | \"285s\";\n  /**\n   * The CFG strength to use for the music generation. Default value: `4`\n   */\n  cfg_strength?: number;\n  /**\n   * The scheduler to use for the music generation. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"midpoint\" | \"rk4\" | \"implicit_adams\";\n  /**\n   * The number of inference steps to use for the music generation. Default value: `32`\n   */\n  num_inference_steps?: number;\n};\nexport type DiffrhythmOutput = {\n  /**\n   * Generated music file.\n   */\n  audio: File;\n};\nexport type DissolveInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of second image for dissolve\n   */\n  dissolve_image_url: string | Blob | File;\n  /**\n   * Dissolve blend factor Default value: `0.5`\n   */\n  dissolve_factor?: number;\n};\nexport type DissolveOutput = {\n  /**\n   * The processed images with dissolve effect\n   */\n  images: Array<Image>;\n};\nexport type DistilledExtendVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type DistilledImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type DistilledMultiConditioningVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type DistilledTextToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9, 1:1 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DocresDewarpInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type DocresDewarpOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DocresInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Task to perform\n   */\n  task: \"deshadowing\" | \"appearance\" | \"deblurring\" | \"binarization\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type DocResInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Task to perform\n   */\n  task: \"deshadowing\" | \"appearance\" | \"deblurring\" | \"binarization\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n};\nexport type DocresOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type DodgeBurnInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Dodge and burn intensity Default value: `0.5`\n   */\n  dodge_burn_intensity?: number;\n  /**\n   * Dodge and burn mode Default value: `\"dodge\"`\n   */\n  dodge_burn_mode?:\n    | \"dodge\"\n    | \"burn\"\n    | \"dodge_and_burn\"\n    | \"burn_and_dodge\"\n    | \"color_dodge\"\n    | \"color_burn\"\n    | \"linear_dodge\"\n    | \"linear_burn\";\n};\nexport type DodgeBurnOutput = {\n  /**\n   * The processed images with dodge and burn effect\n   */\n  images: Array<Image>;\n};\nexport type DrctSuperResolutionInput = {\n  /**\n   * URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Upscaling factor. Default value: `\"4\"`\n   */\n  upscaling_factor?: \"4\";\n};\nexport type DrctSuperResolutionOutput = {\n  /**\n   * Upscaled image\n   */\n  image: Image;\n};\nexport type DreaminaInput = {\n  /**\n   * The text prompt used to generate the image\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Width and height must be between 512 and 2048.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Whether to use an LLM to enhance the prompt\n   */\n  enhance_prompt?: boolean;\n  /**\n   * Number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed to control the stochasticity of image generation.\n   */\n  seed?: number;\n};\nexport type DreaminaOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type DreamoInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * URL of first reference image to use for generation.\n   */\n  first_image_url?: string | Blob | File;\n  /**\n   * URL of second reference image to use for generation.\n   */\n  second_image_url?: string | Blob | File;\n  /**\n   * Task for first reference image (ip/id/style). Default value: `\"ip\"`\n   */\n  first_reference_task?: \"ip\" | \"id\" | \"style\";\n  /**\n   * Task for second reference image (ip/id/style). Default value: `\"ip\"`\n   */\n  second_reference_task?: \"ip\" | \"id\" | \"style\";\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `12`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Resolution for reference images. Default value: `512`\n   */\n  ref_resolution?: number;\n  /**\n   * The weight of the CFG loss. Default value: `1`\n   */\n  true_cfg?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type DreamoOutput = {\n  /**\n   * The URLs of the generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used to generate the image.\n   */\n  prompt: string;\n};\nexport type DubbingInput = {\n  /**\n   * Input video URL to be dubbed.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Target language to dub the video to Default value: `\"hindi\"`\n   */\n  target_language?: \"hindi\" | \"turkish\" | \"english\";\n  /**\n   * Whether to lip sync the audio to the video Default value: `true`\n   */\n  do_lipsync?: boolean;\n};\nexport type DubbingOutput = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type EaselAvatarInput = {\n  /**\n   * The face image with which the scene is generated.\n   */\n  face_image_0: Image;\n  /**\n   * The gender of the person in the face image\n   */\n  gender_0: \"male\" | \"female\" | \"non-binary\";\n  /**\n   * (Optional) The second face image used to generate a two-person scene.\n   */\n  face_image_1?: Image;\n  /**\n   * The gender of the person in the second face image Default value: `\"female\"`\n   */\n  gender_1?: \"male\" | \"female\" | \"non-binary\";\n  /**\n   * The prompt to generate the scene\n   */\n  prompt: string;\n  /**\n   * The style of the generated image. Hyperrealistic-likeness: preserves more likeness including hair styles; hyperrealistic: ideal for fun and creative scenes; Realistic: photorealistic with good text rendering; Stylistic: softer, more artistic Default value: `\"hyperrealistic-likeness\"`\n   */\n  style?:\n    | \"hyperrealistic-likeness\"\n    | \"hyperrealistic\"\n    | \"realistic\"\n    | \"stylistic\";\n};\nexport type EaselAvatarOutput = {\n  /**\n   * The generated avatar image\n   */\n  image: Image;\n};\nexport type EaselGifswapInput = {\n  /**\n   * The face image to swap onto the GIF\n   */\n  face_image: Image;\n  /**\n   * The GIF to apply face swap to. GIF size affects generation time.\n   */\n  gif_image: Image;\n};\nexport type EaselGifswapOutput = {\n  /**\n   * The generated face-swapped GIF\n   */\n  image: Image;\n};\nexport type EditImageInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type EffectInput = {\n  /**\n   * The effect to apply to the video\n   */\n  effect:\n    | \"Kiss Me AI\"\n    | \"Kiss\"\n    | \"Muscle Surge\"\n    | \"Warmth of Jesus\"\n    | \"Anything, Robot\"\n    | \"The Tiger Touch\"\n    | \"Hug\"\n    | \"Holy Wings\"\n    | \"Hulk\"\n    | \"Venom\"\n    | \"Microwave\"\n    | \"Zombie Mode\"\n    | \"Squid Game\"\n    | \"Baby Face\"\n    | \"Black Myth: Wukong\"\n    | \"Long Hair Magic\"\n    | \"Leggy Run\"\n    | \"Fin-tastic Mermaid\"\n    | \"Punch Face\"\n    | \"Creepy Devil Smile\"\n    | \"Thunder God\"\n    | \"Eye Zoom Challenge\"\n    | \"Who's Arrested?\"\n    | \"Baby Arrived\"\n    | \"Werewolf Rage\"\n    | \"Bald Swipe\"\n    | \"BOOM DROP\"\n    | \"Huge Cutie\"\n    | \"Liquid Metal\"\n    | \"Sharksnap!\"\n    | \"Dust Me Away\";\n  /**\n   * Optional URL of the image to use as the first frame. If not provided, generates from text\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type EffectOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ElementsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ElevenlabsAudioIsolationInput = {\n  /**\n   * URL of the audio file to isolate voice from\n   */\n  audio_url: string | Blob | File;\n};\nexport type ElevenlabsAudioIsolationOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.\n   */\n  timestamps?: Array<void>;\n};\nexport type ElevenlabsSoundEffectsInput = {\n  /**\n   * The text describing the sound effect to generate\n   */\n  text: string;\n  /**\n   * Duration in seconds (0.5-22). If None, optimal duration will be determined from prompt.\n   */\n  duration_seconds?: number;\n  /**\n   * How closely to follow the prompt (0-1). Higher values mean less variation. Default value: `0.3`\n   */\n  prompt_influence?: number;\n};\nexport type ElevenlabsSoundEffectsOutput = {\n  /**\n   * The generated sound effect audio file in MP3 format\n   */\n  audio: File;\n};\nexport type ElevenlabsSpeechToTextInput = {\n  /**\n   * URL of the audio file to transcribe\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Language code of the audio\n   */\n  language_code?: string;\n  /**\n   * Tag audio events like laughter, applause, etc. Default value: `true`\n   */\n  tag_audio_events?: boolean;\n  /**\n   * Whether to annotate who is speaking Default value: `true`\n   */\n  diarize?: boolean;\n};\nexport type ElevenlabsSpeechToTextOutput = {\n  /**\n   * The full transcribed text\n   */\n  text: string;\n  /**\n   * Detected or specified language code\n   */\n  language_code: string;\n  /**\n   * Confidence in language detection\n   */\n  language_probability: number;\n  /**\n   * Word-level transcription details\n   */\n  words: Array<TranscriptionWord>;\n};\nexport type ElevenlabsTtsMultilingualV2Input = {\n  /**\n   * The text to convert to speech\n   */\n  text: string;\n  /**\n   * The voice to use for speech generation Default value: `\"Rachel\"`\n   */\n  voice?: string;\n  /**\n   * Voice stability (0-1) Default value: `0.5`\n   */\n  stability?: number;\n  /**\n   * Similarity boost (0-1) Default value: `0.75`\n   */\n  similarity_boost?: number;\n  /**\n   * Style exaggeration (0-1)\n   */\n  style?: number;\n  /**\n   * Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality. Default value: `1`\n   */\n  speed?: number;\n  /**\n   * Whether to return timestamps for each word in the generated speech\n   */\n  timestamps?: boolean;\n  /**\n   * The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.\n   */\n  previous_text?: string;\n  /**\n   * The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.\n   */\n  next_text?: string;\n  /**\n   * Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.\n   */\n  language_code?: string;\n};\nexport type ElevenlabsTtsMultilingualV2Output = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.\n   */\n  timestamps?: Array<void>;\n};\nexport type ElevenlabsTtsTurboV25Input = {\n  /**\n   * The text to convert to speech\n   */\n  text: string;\n  /**\n   * The voice to use for speech generation Default value: `\"Rachel\"`\n   */\n  voice?: string;\n  /**\n   * Voice stability (0-1) Default value: `0.5`\n   */\n  stability?: number;\n  /**\n   * Similarity boost (0-1) Default value: `0.75`\n   */\n  similarity_boost?: number;\n  /**\n   * Style exaggeration (0-1)\n   */\n  style?: number;\n  /**\n   * Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality. Default value: `1`\n   */\n  speed?: number;\n  /**\n   * Whether to return timestamps for each word in the generated speech\n   */\n  timestamps?: boolean;\n  /**\n   * The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.\n   */\n  previous_text?: string;\n  /**\n   * The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.\n   */\n  next_text?: string;\n  /**\n   * Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.\n   */\n  language_code?: string;\n};\nexport type ElevenlabsTtsTurboV25Output = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.\n   */\n  timestamps?: Array<void>;\n};\nexport type EraserInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'. Default value: `\"manual\"`\n   */\n  mask_type?: \"manual\" | \"automatic\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, attempts to preserve the alpha channel of the input image.\n   */\n  preserve_alpha?: boolean;\n};\nexport type EraserOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n};\nexport type EvfSamInput = {\n  /**\n   * The prompt to generate segmentation from.\n   */\n  prompt: string;\n  /**\n   * Areas to exclude from segmentation (will be subtracted from prompt results)\n   */\n  negative_prompt?: string;\n  /**\n   * Enable semantic level segmentation for body parts, background or multi objects\n   */\n  semantic_type?: boolean;\n  /**\n   * URL of the input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * Output only the binary mask instead of masked image Default value: `true`\n   */\n  mask_only?: boolean;\n  /**\n   * Use GroundingDINO instead of SAM for segmentation\n   */\n  use_grounding_dino?: boolean;\n  /**\n   * Invert the mask (background becomes foreground and vice versa)\n   */\n  revert_mask?: boolean;\n  /**\n   * Apply Gaussian blur to the mask. Value determines kernel size (must be odd number)\n   */\n  blur_mask?: number;\n  /**\n   * Expand/dilate the mask by specified pixels\n   */\n  expand_mask?: number;\n  /**\n   * Fill holes in the mask using morphological operations\n   */\n  fill_holes?: boolean;\n};\nexport type EvfSamOutput = {\n  /**\n   * The segmented output image\n   */\n  image: File;\n};\nexport type ExpressionChangeInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The desired facial expression to apply. Default value: `\"sad\"`\n   */\n  prompt?: string;\n};\nexport type ExpressionChangeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ExtendOutput = {\n  /**\n   * The extended video\n   */\n  video: File;\n};\nexport type ExtendVideoConditioningInput = {\n  /**\n   * URL of video to use as conditioning\n   */\n  video_url: string | Blob | File;\n  /**\n   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.\n   */\n  start_frame_num?: number;\n  /**\n   * Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning. Default value: `1`\n   */\n  strength?: number;\n  /**\n   * Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.\n   */\n  limit_num_frames?: boolean;\n  /**\n   * Maximum number of frames to use from the video. If None, all frames will be used. Default value: `1441`\n   */\n  max_num_frames?: number;\n  /**\n   * Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.\n   */\n  resample_fps?: boolean;\n  /**\n   * Target FPS to resample the video to. Only relevant if `resample_fps` is True. Default value: `24`\n   */\n  target_fps?: number;\n  /**\n   * Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.\n   */\n  reverse_video?: boolean;\n};\nexport type ExtendVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type ExtendVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type FaceEnhancementOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type FashionPhotoshootInput = {\n  /**\n   * The garment image to be used for the fashion shoot.\n   */\n  garment_image: Image;\n  /**\n   * The user's face image used for the fashion shoot.\n   */\n  face_image: Image;\n  /**\n   * The model's gender for the fashion shoot.\n   */\n  gender: \"male\" | \"female\";\n  /**\n   * The body size for the fashion shoot. Default value: `\"S\"`\n   */\n  body_size?: \"XS\" | \"S\" | \"M\" | \"L\" | \"XL\";\n  /**\n   * Sets the location / background for the fashion shoot. Contact hello@easelapps.ai if you require additional options. Default value: `\"park\"`\n   */\n  location?: \"park\" | \"city\";\n};\nexport type FashionPhotoshootOutput = {\n  /**\n   * The resulting image after the fashion shoot.\n   */\n  image: Image;\n};\nexport type FashionTryonInput = {\n  /**\n   * The reference person full body image to try clothes on\n   */\n  full_body_image: Image;\n  /**\n   * The clothing item to try on the person\n   */\n  clothing_image: Image;\n  /**\n   * The model's gender for the try-on. Default value: `\"female\"`\n   */\n  gender?: \"male\" | \"female\";\n};\nexport type FashionTryonOutput = {\n  /**\n   * The generated try-on result\n   */\n  image: Image;\n};\nexport type FashnTryonV15Input = {\n  /**\n   * URL or base64 of the model image\n   */\n  model_image: string;\n  /**\n   * URL or base64 of the garment image\n   */\n  garment_image: string;\n  /**\n   * Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment. Default value: `\"auto\"`\n   */\n  category?: \"tops\" | \"bottoms\" | \"one-pieces\" | \"auto\";\n  /**\n   * Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results. Default value: `\"balanced\"`\n   */\n  mode?: \"performance\" | \"balanced\" | \"quality\";\n  /**\n   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `\"auto\"`\n   */\n  garment_photo_type?: \"auto\" | \"model\" | \"flat-lay\";\n  /**\n   * Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear. Default value: `\"permissive\"`\n   */\n  moderation_level?: \"none\" | \"permissive\" | \"conservative\";\n  /**\n   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`\n   */\n  num_samples?: number;\n  /**\n   * Disables human parsing on the model image. Default value: `true`\n   */\n  segmentation_free?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster Default value: `\"png\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n};\nexport type FashnTryonV15Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type FashnTryonV16Input = {\n  /**\n   * URL or base64 of the model image\n   */\n  model_image: string;\n  /**\n   * URL or base64 of the garment image\n   */\n  garment_image: string;\n  /**\n   * Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment. Default value: `\"auto\"`\n   */\n  category?: \"tops\" | \"bottoms\" | \"one-pieces\" | \"auto\";\n  /**\n   * Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results. Default value: `\"balanced\"`\n   */\n  mode?: \"performance\" | \"balanced\" | \"quality\";\n  /**\n   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `\"auto\"`\n   */\n  garment_photo_type?: \"auto\" | \"model\" | \"flat-lay\";\n  /**\n   * Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear. Default value: `\"permissive\"`\n   */\n  moderation_level?: \"none\" | \"permissive\" | \"conservative\";\n  /**\n   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`\n   */\n  num_samples?: number;\n  /**\n   * Disables human parsing on the model image. Default value: `true`\n   */\n  segmentation_free?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster Default value: `\"png\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n};\nexport type FashnTryonV16Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type FastImageToVideoHailuo02Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `\"6\"`\n   */\n  duration?: \"6\" | \"10\";\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type FfmpegApiComposeInput = {\n  /**\n   * List of tracks to be combined into the final media\n   */\n  tracks: Array<Track>;\n};\nexport type FfmpegApiComposeOutput = {\n  /**\n   * URL of the processed video file\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the video's thumbnail image\n   */\n  thumbnail_url: string | Blob | File;\n};\nexport type FfmpegApiExtractFrameInput = {\n  /**\n   * URL of the video file to use as the video track\n   */\n  video_url: string | Blob | File;\n  /**\n   * Type of frame to extract: first, middle, or last frame of the video Default value: `\"first\"`\n   */\n  frame_type?: \"first\" | \"middle\" | \"last\";\n};\nexport type FfmpegApiExtractFrameOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n};\nexport type FfmpegApiLoudnormInput = {\n  /**\n   * URL of the audio file to normalize\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Integrated loudness target in LUFS. Default value: `-18`\n   */\n  integrated_loudness?: number;\n  /**\n   * Maximum true peak in dBTP. Default value: `-0.1`\n   */\n  true_peak?: number;\n  /**\n   * Loudness range target in LU Default value: `7`\n   */\n  loudness_range?: number;\n  /**\n   * Offset gain in dB applied before the true-peak limiter\n   */\n  offset?: number;\n  /**\n   * Use linear normalization mode (single-pass). If false, uses dynamic mode (two-pass for better quality).\n   */\n  linear?: boolean;\n  /**\n   * Treat mono input files as dual-mono for correct EBU R128 measurement on stereo systems\n   */\n  dual_mono?: boolean;\n  /**\n   * Return loudness measurement summary with the normalized audio\n   */\n  print_summary?: boolean;\n  /**\n   * Measured integrated loudness of input file in LUFS. Required for linear mode.\n   */\n  measured_i?: number;\n  /**\n   * Measured loudness range of input file in LU. Required for linear mode.\n   */\n  measured_lra?: number;\n  /**\n   * Measured true peak of input file in dBTP. Required for linear mode.\n   */\n  measured_tp?: number;\n  /**\n   * Measured threshold of input file in LUFS. Required for linear mode.\n   */\n  measured_thresh?: number;\n};\nexport type FfmpegApiLoudnormOutput = {\n  /**\n   * Normalized audio file\n   */\n  audio: File;\n  /**\n   * Structured loudness measurement summary (if requested)\n   */\n  summary?: LoudnormSummary;\n};\nexport type FfmpegApiMergeAudioVideoInput = {\n  /**\n   * URL of the video file to use as the video track\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the audio file to use as the audio track\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Offset in seconds for when the audio should start relative to the video\n   */\n  start_offset?: number;\n};\nexport type FfmpegApiMergeAudioVideoOutput = {\n  /**\n   * Output video with merged audio.\n   */\n  video: File;\n};\nexport type FfmpegApiMetadataInput = {\n  /**\n   * URL of the media file (video or audio) to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Whether to extract the start and end frames for videos. Note that when true the request will be slower.\n   */\n  extract_frames?: boolean;\n};\nexport type FfmpegApiMetadataOutput = {\n  /**\n   * Metadata for the analyzed media file (either Video or Audio)\n   */\n  media: Video | Audio;\n};\nexport type FfmpegApiWaveformInput = {\n  /**\n   * URL of the audio file to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one. Default value: `4`\n   */\n  points_per_second?: number;\n  /**\n   * Number of decimal places for the waveform values. Higher values provide more precision but increase payload size. Default value: `2`\n   */\n  precision?: number;\n  /**\n   * Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number. Default value: `3`\n   */\n  smoothing_window?: number;\n};\nexport type FfmpegApiWaveformOutput = {\n  /**\n   * Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.\n   */\n  waveform: Array<number>;\n  /**\n   * Duration of the audio in seconds\n   */\n  duration: number;\n  /**\n   * Number of points in the waveform data\n   */\n  points: number;\n  /**\n   * Number of decimal places used in the waveform values\n   */\n  precision: number;\n};\nexport type FILMImageInput = {\n  /**\n   * The URL of the first image to use as the starting point for interpolation.\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * The URL of the second image to use as the ending point for interpolation.\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * The type of output to generate; either individual images or a video. Default value: `\"images\"`\n   */\n  output_type?: \"images\" | \"video\";\n  /**\n   * The format of the output images. Only applicable if output_type is 'images'. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n  /**\n   * The number of frames to generate between the input images. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * Whether to include the start image in the output.\n   */\n  include_start?: boolean;\n  /**\n   * Whether to include the end image in the output.\n   */\n  include_end?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the function will wait for images to be generated and uploaded before returning. This will increase the response time but ensures that the images are ready for use immediately without going through the CDN. Does not apply if output_type is 'video'.\n   */\n  sync_mode?: boolean;\n};\nexport type FILMImageOutput = {\n  /**\n   * The generated frames as individual images.\n   */\n  images?: Array<Image>;\n  /**\n   * The generated video file, if output_type is 'video'.\n   */\n  video?: File;\n};\nexport type FilmInput = {\n  /**\n   * The URL of the first image to use as the starting point for interpolation.\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * The URL of the second image to use as the ending point for interpolation.\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * The type of output to generate; either individual images or a video. Default value: `\"images\"`\n   */\n  output_type?: \"images\" | \"video\";\n  /**\n   * The format of the output images. Only applicable if output_type is 'images'. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n  /**\n   * The number of frames to generate between the input images. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * Whether to include the start image in the output.\n   */\n  include_start?: boolean;\n  /**\n   * Whether to include the end image in the output.\n   */\n  include_end?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the function will wait for images to be generated and uploaded before returning. This will increase the response time but ensures that the images are ready for use immediately without going through the CDN. Does not apply if output_type is 'video'.\n   */\n  sync_mode?: boolean;\n};\nexport type FilmOutput = {\n  /**\n   * The generated frames as individual images.\n   */\n  images?: Array<Image>;\n  /**\n   * The generated video file, if output_type is 'video'.\n   */\n  video?: File;\n};\nexport type FilmVideoInput = {\n  /**\n   * The URL of the video to use for interpolation.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The number of frames to generate between the input video frames. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.\n   */\n  use_scene_detection?: boolean;\n  /**\n   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`\n   */\n  use_calculated_fps?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.\n   */\n  loop?: boolean;\n};\nexport type FILMVideoInput = {\n  /**\n   * The URL of the video to use for interpolation.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The number of frames to generate between the input video frames. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.\n   */\n  use_scene_detection?: boolean;\n  /**\n   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`\n   */\n  use_calculated_fps?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.\n   */\n  loop?: boolean;\n};\nexport type FilmVideoOutput = {\n  /**\n   * The generated video file with interpolated frames.\n   */\n  video: File;\n};\nexport type FILMVideoOutput = {\n  /**\n   * The generated video file with interpolated frames.\n   */\n  video: File;\n};\nexport type FinegrainEraserBboxInput = {\n  /**\n   * URL of the image to edit\n   */\n  image_url: string | Blob | File;\n  /**\n   * List of bounding box coordinates to erase (only one box prompt is supported)\n   */\n  box_prompts: Array<BoxPromptBase>;\n  /**\n   * Erase quality mode Default value: `\"standard\"`\n   */\n  mode?: \"express\" | \"standard\" | \"premium\";\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type FinegrainEraserBboxOutput = {\n  /**\n   * The edited image with content erased\n   */\n  image: File;\n  /**\n   * Seed used for generation\n   */\n  used_seed: number;\n};\nexport type FinegrainEraserInput = {\n  /**\n   * URL of the image to edit\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text description of what to erase\n   */\n  prompt: string;\n  /**\n   * Erase quality mode Default value: `\"standard\"`\n   */\n  mode?: \"express\" | \"standard\" | \"premium\";\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type FinegrainEraserMaskInput = {\n  /**\n   * URL of the image to edit\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of the mask image. Should be a binary mask where white (255) indicates areas to erase\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Erase quality mode Default value: `\"standard\"`\n   */\n  mode?: \"express\" | \"standard\" | \"premium\";\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type FinegrainEraserMaskOutput = {\n  /**\n   * The edited image with content erased\n   */\n  image: File;\n  /**\n   * Seed used for generation\n   */\n  used_seed: number;\n};\nexport type FinegrainEraserOutput = {\n  /**\n   * The edited image with content erased\n   */\n  image: File;\n  /**\n   * Seed used for generation\n   */\n  used_seed: number;\n};\nexport type FLiteStandardInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * Negative Prompt for generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FLiteStandardOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FLiteTextureInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * Negative Prompt for generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FLiteTextureOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FloweditInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Prompt of the image to be used.\n   */\n  source_prompt: string;\n  /**\n   * Prompt of the image to be made.\n   */\n  target_prompt: string;\n  /**\n   * Random seed for reproducible generation. If set none, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Steps for which the model should run. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the source. Default value: `1.5`\n   */\n  src_guidance_scale?: number;\n  /**\n   * Guidance scale for target. Default value: `5.5`\n   */\n  tar_guidance_scale?: number;\n  /**\n   * Average step count Default value: `1`\n   */\n  n_avg?: number;\n  /**\n   * Control the strength of the edit Default value: `23`\n   */\n  n_max?: number;\n  /**\n   * Minimum step for improved style edits\n   */\n  n_min?: number;\n};\nexport type FloweditOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type Flux1DevImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1DevImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1DevInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1DevOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1DevReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1DevReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1KreaImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1KreaImageToImageOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1KreaInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1KreaOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1KreaReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1KreaReduxOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1SchnellInput = {\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1SchnellOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Flux1SchnellReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type Flux1SchnellReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraCannyImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxControlLoraCannyImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraCannyInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n};\nexport type FluxControlLoraCannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraDepthImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxControlLoraDepthImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxControlLoraDepthInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * If set to true, the input image will be preprocessed to extract depth information.\n   * This is useful for generating depth maps from images. Default value: `true`\n   */\n  preprocess_depth?: boolean;\n};\nexport type FluxControlLoraDepthOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxDevImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxDevImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxDevInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxDevOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxGeneralInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment.\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * IP-Adapter to use for image generation.\n   */\n  ip_adapters?: Array<IPAdapter>;\n  /**\n   * EasyControl Inputs to use for image generation.\n   */\n  easycontrols?: Array<EasyControlWeight>;\n  /**\n   * Use an image input to influence the generation. Can be used to fill images in masked areas.\n   */\n  fill_image?: ImageFillInput;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  real_cfg_scale?: number;\n  /**\n   * Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true.\n   * If using XLabs IP-Adapter v1, this will be turned on!.\n   */\n  use_real_cfg?: boolean;\n  /**\n   * Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.\n   */\n  use_cfg_zero?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * Sigmas schedule for the denoising process.\n   */\n  sigma_schedule?: \"sgm_uniform\";\n  /**\n   * Scheduler for the denoising process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"dpmpp_2m\";\n  /**\n   * Negative prompt to steer the image generation away from unwanted features.\n   * By default, we will be using NAG for processing the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The scale for NAG. Higher values will result in a image that is more distant\n   * to the negative prompt. Default value: `3`\n   */\n  nag_scale?: number;\n  /**\n   * The tau for NAG. Controls the normalization of the hidden state.\n   * Higher values will result in a less aggressive normalization,\n   * but may also lead to unexpected changes with respect to the original image.\n   * Not recommended to change this value. Default value: `2.5`\n   */\n  nag_tau?: number;\n  /**\n   * The alpha value for NAG. This value is used as a final weighting\n   * factor for steering the normalized guidance (positive and negative prompts)\n   * in the direction of the positive prompt. Higher values will result in less\n   * steering on the normalized guidance where lower values will result in\n   * considering the positive prompt guidance more. Default value: `0.25`\n   */\n  nag_alpha?: number;\n  /**\n   * The proportion of steps to apply NAG. After the specified proportion\n   * of steps has been iterated, the remaining steps will use original\n   * attention processors in FLUX. Default value: `0.25`\n   */\n  nag_end?: number;\n};\nexport type FluxGeneralOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKontextDevInput = {\n  /**\n   * The prompt to edit the image.\n   */\n  prompt: string;\n  /**\n   * The URL of the image to edit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * Determines how the output resolution is set for image editing.\n   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.\n   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).\n   * Apart from these, a few aspect ratios are also supported. Default value: `\"match_input\"`\n   */\n  resolution_mode?:\n    | \"auto\"\n    | \"match_input\"\n    | \"1:1\"\n    | \"16:9\"\n    | \"21:9\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"9:16\"\n    | \"9:21\";\n};\nexport type FluxKontextDevOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKontextInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxKontextLoraInpaintInput = {\n  /**\n   * The URL of the image to be inpainted.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt for the image to image task.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * The URL of the reference image for inpainting.\n   */\n  reference_image_url: string | Blob | File;\n  /**\n   * The URL of the mask for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`\n   */\n  strength?: number;\n};\nexport type FluxKontextLoraInpaintOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKontextLoraInput = {\n  /**\n   * The URL of the image to edit.\n   *\n   * Max width: 14142px, Max height: 14142px, Timeout: 20s\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to edit the image.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * Determines how the output resolution is set for image editing.\n   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.\n   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).\n   * Apart from these, a few aspect ratios are also supported. Default value: `\"match_input\"`\n   */\n  resolution_mode?:\n    | \"auto\"\n    | \"match_input\"\n    | \"1:1\"\n    | \"16:9\"\n    | \"21:9\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"9:16\"\n    | \"9:21\";\n};\nexport type FluxKontextLoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKontextLoraTextToImageInput = {\n  /**\n   * The prompt to generate the image with\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxKontextLoraTextToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKontextMultiInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * Image prompt for the omni model.\n   */\n  image_urls: Array<string>;\n};\nexport type FluxKontextOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<fal__toolkit__image__image__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKontextTrainerInput = {\n  /**\n   * URL to the input data zip archive.\n   *\n   * The zip should contain pairs of images. The images should be named:\n   *\n   * ROOT_start.EXT and ROOT_end.EXT\n   * For example:\n   * photo_start.jpg and photo_end.jpg\n   *\n   * The zip can also contain a text file for each image pair. The text file should be named:\n   * ROOT.txt\n   * For example:\n   * photo.txt\n   *\n   * This text file can be used to specify the edit instructions for the image pair.\n   *\n   * If no text file is provided, the default_caption will be used.\n   *\n   * If no default_caption is provided, the training will fail.\n   */\n  image_data_url: string | Blob | File;\n  /**\n   * Number of steps to train for Default value: `1000`\n   */\n  steps?: number;\n  /**\n   *  Default value: `0.0001`\n   */\n  learning_rate?: number;\n  /**\n   * Default caption to use when caption files are missing. If None, missing captions will cause an error.\n   */\n  default_caption?: string;\n  /**\n   * Dictates the naming scheme for the output weights Default value: `\"fal\"`\n   */\n  output_lora_format?: \"fal\" | \"comfy\";\n};\nexport type FluxKontextTrainerOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the configuration file for the trained model.\n   */\n  config_file: File;\n};\nexport type FluxKreaImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxKreaImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxKreaLoraImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type FluxKreaLoraImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaLoraInpaintingInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type FluxKreaLoraInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaLoraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxKreaLoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaLoraStreamInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxKreaLoraStreamOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxKreaReduxOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxKreaTrainerInput = {\n  /**\n   * URL to zip archive with images. Try to use at least 4 images in general the more the better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Trigger word to be used in the captions. If None, a trigger word will not be used.\n   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.\n   */\n  trigger_word?: string;\n  /**\n   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible. Default value: `true`\n   */\n  create_masks?: boolean;\n  /**\n   * Number of steps to train the LoRA on.\n   */\n  steps?: number;\n  /**\n   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.\n   */\n  is_style?: boolean;\n  /**\n   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.\n   */\n  is_input_format_already_preprocessed?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n};\nexport type FluxKreaTrainerOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the training configuration file.\n   */\n  config_file: File;\n  /**\n   * URL to the preprocessed images.\n   */\n  debug_preprocessed_output?: File;\n};\nexport type FluxLoraCannyInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for canny input\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxLoraCannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraFastTrainingInput = {\n  /**\n   * URL to zip archive with images. Try to use at least 4 images in general the more the better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Trigger word to be used in the captions. If None, a trigger word will not be used.\n   * If no captions are provide the trigger_word will be used instead of captions. If captions are the trigger word will not be used.\n   */\n  trigger_word?: string;\n  /**\n   * If True segmentation masks will be used in the weight the training loss. For people a face mask is used if possible. Default value: `true`\n   */\n  create_masks?: boolean;\n  /**\n   * Number of steps to train the LoRA on.\n   */\n  steps?: number;\n  /**\n   * If True, the training will be for a style. This will deactivate segmentation, captioning and will use trigger word instead. Use the trigger word to specify the style.\n   */\n  is_style?: boolean;\n  /**\n   * Specifies whether the input data is already in a processed format. When set to False (default), the system expects raw input where image files and their corresponding caption files share the same name (e.g., 'photo.jpg' and 'photo.txt'). Set to True if your data is already in a preprocessed format.\n   */\n  is_input_format_already_preprocessed?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n};\nexport type FluxLoraFastTrainingOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the training configuration file.\n   */\n  config_file: File;\n  /**\n   * URL to the preprocessed images.\n   */\n  debug_preprocessed_output?: File;\n};\nexport type FluxLoraFillInput = {\n  /**\n   * The prompt to generate an image from. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for fill operation\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Specifies whether to paste-back the original image onto to the non-inpainted areas of the output Default value: `true`\n   */\n  paste_back?: boolean;\n  /**\n   * Use an image fill input to fill in particular images into the masked area.\n   */\n  fill_image?: ImageFillInput;\n  /**\n   * Resizes the image back to the original size. Use when you wish to preserve the exact image size as the originally provided image.\n   */\n  resize_to_original?: boolean;\n};\nexport type FluxLoraFillOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxLoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxLoraPortraitTrainerInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   *\n   * The captions can include a special string `[trigger]`. If a trigger_word is specified, it will replace `[trigger]` in the captions.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.\n   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.\n   */\n  trigger_phrase?: string;\n  /**\n   * Learning rate to use for training. Default value: `0.00009`\n   */\n  learning_rate?: number;\n  /**\n   * Number of steps to train the LoRA on. Default value: `2500`\n   */\n  steps?: number;\n  /**\n   * If True, multiresolution training will be used. Default value: `true`\n   */\n  multiresolution_training?: boolean;\n  /**\n   * If True, the subject will be cropped from the image. Default value: `true`\n   */\n  subject_crop?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n  /**\n   * URL to a checkpoint to resume training from. Default value: `\"\"`\n   */\n  resume_from_checkpoint?: string;\n  /**\n   * If True, masks will be created for the subject.\n   */\n  create_masks?: boolean;\n};\nexport type FluxLoraPortraitTrainerOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the training configuration file.\n   */\n  config_file: File;\n};\nexport type FluxLoraStreamInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type FluxLoraStreamOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProCannyControlFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProCannyControlInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProDepthControlFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProDepthControlInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProFillFinetunedInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProFillInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n};\nexport type FluxProKontextInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxProKontextMaxInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n};\nexport type FluxProKontextMaxMultiInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * Image prompt for the omni model.\n   */\n  image_urls: Array<string>;\n};\nexport type FluxProKontextMaxMultiOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProKontextMaxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<fal__toolkit__image__image__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProKontextMaxTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n};\nexport type FluxProKontextMaxTextToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProKontextMultiInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * Image prompt for the omni model.\n   */\n  image_urls: Array<string>;\n};\nexport type FluxProKontextMultiOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProKontextOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<fal__toolkit__image__image__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProKontextTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image. Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n};\nexport type FluxProKontextTextToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProOutpaintInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The image URL to expand using outpainting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Pixels to expand at the top\n   */\n  expand_top?: number;\n  /**\n   * Pixels to expand at the bottom\n   */\n  expand_bottom?: number;\n  /**\n   * Pixels to expand on the left\n   */\n  expand_left?: number;\n  /**\n   * Pixels to expand on the right\n   */\n  expand_right?: number;\n};\nexport type FluxProPlusTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n};\nexport type FluxProTextToImageFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n};\nexport type FluxProTrainerInput = {\n  /**\n   * URL to the training data\n   */\n  data_url: string | Blob | File;\n  /**\n   * Determines the finetuning approach based on your concept Default value: `\"character\"`\n   */\n  mode?: \"character\" | \"product\" | \"style\" | \"general\";\n  /**\n   * Descriptive note to identify your fine-tune since names are UUIDs. Will be displayed in finetune_details.\n   */\n  finetune_comment: string;\n  /**\n   * Defines training duration Default value: `300`\n   */\n  iterations?: number;\n  /**\n   * Learning rate for training. Lower values may be needed for certain scenarios. Default is 1e-5 for full and 1e-4 for LoRA.\n   */\n  learning_rate?: number;\n  /**\n   * The speed priority will improve training and inference speed Default value: `\"quality\"`\n   */\n  priority?: \"speed\" | \"quality\" | \"high_res_only\";\n  /**\n   * Enables/disables automatic image captioning Default value: `true`\n   */\n  captioning?: boolean;\n  /**\n   * Unique word/phrase that will be used in the captions, to reference the newly introduced concepts Default value: `\"TOK\"`\n   */\n  trigger_word?: string;\n  /**\n   * Choose between 32 and 16. A lora_rank of 16 can increase training efficiency and decrease loading times. Default value: `32`\n   */\n  lora_rank?: number;\n  /**\n   * Choose between 'full' for a full finetuning + post hoc extraction of the trained weights into a LoRA or 'lora' for a raw LoRA training Default value: `\"full\"`\n   */\n  finetune_type?: \"full\" | \"lora\";\n};\nexport type FluxProTrainerOutput = {\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n};\nexport type FluxProUltraTextToImageFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProUltraTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n};\nexport type FluxProV11Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n};\nexport type FluxProV11Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV11UltraFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV11UltraFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV11UltraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The aspect ratio of the generated image. Default value: `16:9`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\"\n    | string;\n  /**\n   * Generate less processed, more natural-looking images.\n   */\n  raw?: boolean;\n};\nexport type FluxProV11UltraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1CannyFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `30`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV1CannyFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1CannyInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the Canny edge map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProV1CannyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1DepthFinetunedInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `15`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV1DepthFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1DepthInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The control image URL to generate the depth map from.\n   */\n  control_image_url: string | Blob | File;\n};\nexport type FluxProV1DepthOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxProV1FillFinetunedInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * References your specific model\n   */\n  finetune_id: string;\n  /**\n   * Controls finetune influence.\n   * Increase this value if your target concept isn't showing up strongly enough.\n   * The optimal setting depends on your finetune and prompt\n   */\n  finetune_strength: number;\n};\nexport type FluxProV1FillFinetunedOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<registry__image__fast_sdxl__models__Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FluxSchnellReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type FluxSchnellReduxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type FrameInput = {\n  /**\n   * URL of the video file to use as the video track\n   */\n  video_url: string | Blob | File;\n  /**\n   * Type of frame to extract: first, middle, or last frame of the video Default value: `\"first\"`\n   */\n  frame_type?: \"first\" | \"middle\" | \"last\";\n};\nexport type FrameOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n};\nexport type FramepackF1Input = {\n  /**\n   * Text prompt for video generation (max 500 characters).\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `\"480p\"`\n   */\n  resolution?: \"720p\" | \"480p\";\n  /**\n   * Classifier-Free Guidance scale for the generation. Default value: `1`\n   */\n  cfg_scale?: number;\n  /**\n   * Guidance scale for the generation. Default value: `10`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of frames to generate. Default value: `180`\n   */\n  num_frames?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FramepackF1Output = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FramepackFlf2vInput = {\n  /**\n   * Text prompt for video generation (max 500 characters).\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `\"480p\"`\n   */\n  resolution?: \"720p\" | \"480p\";\n  /**\n   * Classifier-Free Guidance scale for the generation. Default value: `1`\n   */\n  cfg_scale?: number;\n  /**\n   * Guidance scale for the generation. Default value: `10`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of frames to generate. Default value: `240`\n   */\n  num_frames?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of the end image input.\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * Determines the influence of the final frame on the generated video. Higher values result in the output being more heavily influenced by the last frame. Default value: `0.8`\n   */\n  strength?: number;\n};\nexport type FramepackFlf2vOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FramepackInput = {\n  /**\n   * Text prompt for video generation (max 500 characters).\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `\"480p\"`\n   */\n  resolution?: \"720p\" | \"480p\";\n  /**\n   * Classifier-Free Guidance scale for the generation. Default value: `1`\n   */\n  cfg_scale?: number;\n  /**\n   * Guidance scale for the generation. Default value: `10`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of frames to generate. Default value: `180`\n   */\n  num_frames?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type FramepackOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type FrenchOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type GeminiFlashEditInput = {\n  /**\n   * The prompt for image generation or editing\n   */\n  prompt: string;\n  /**\n   * Optional URL of an input image for editing. If not provided, generates a new image.\n   */\n  image_url: string | Blob | File;\n};\nexport type GeminiFlashEditMultiInput = {\n  /**\n   * The prompt for image generation or editing\n   */\n  prompt: string;\n  /**\n   * List of URLs of input images for editing\n   */\n  input_image_urls: Array<string>;\n};\nexport type GeminiFlashEditMultiOutput = {\n  /**\n   * The generated or edited image\n   */\n  image: Image;\n  /**\n   * Text description or response from Gemini\n   */\n  description: string;\n};\nexport type GeminiFlashEditOutput = {\n  /**\n   * The generated or edited image\n   */\n  image: Image;\n  /**\n   * Text description or response from Gemini\n   */\n  description: string;\n};\nexport type GenFillInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the binary mask image that represents the area that will be cleaned.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of Images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type GenFillOutput = {\n  /**\n   * Generated Images\n   */\n  images: Array<Image>;\n};\nexport type GhiblifyInput = {\n  /**\n   * The URL of the image to upscale.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for the upscale. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type GhiblifyOutput = {\n  /**\n   * The URL of the generated image.\n   */\n  image: Image;\n};\nexport type GlowInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Glow intensity Default value: `1`\n   */\n  glow_intensity?: number;\n  /**\n   * Glow blur radius Default value: `5`\n   */\n  glow_radius?: number;\n};\nexport type GlowOutput = {\n  /**\n   * The processed images with glow effect\n   */\n  images: Array<Image>;\n};\nexport type GotOcrV2Input = {\n  /**\n   * URL of images.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * Generate the output in formatted mode.\n   */\n  do_format?: boolean;\n  /**\n   * Use provided images to generate a single output.\n   */\n  multi_page?: boolean;\n};\nexport type GotOcrV2Output = {\n  /**\n   * Generated output\n   */\n  outputs: Array<string>;\n};\nexport type GptImage1EditImageByokInput = {\n  /**\n   * The URLs of the images to use as a reference for the generation.\n   */\n  image_urls: Array<string>;\n  /**\n   * The prompt to edit the image from.\n   */\n  prompt: string;\n  /**\n   * The size of the image to generate. Default value: `\"auto\"`\n   */\n  image_size?: \"auto\" | \"1024x1024\" | \"1536x1024\" | \"1024x1536\";\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The quality of the image to generate. Default value: `\"auto\"`\n   */\n  quality?: \"auto\" | \"low\" | \"medium\" | \"high\";\n  /**\n   * How hard to try to preserve distinctive features from the input. Default value: `\"low\"`\n   */\n  input_fidelity?: \"low\" | \"high\";\n  /**\n   * The OpenAI API key to use for the image generation. This endpoint is currently powered by bring-your-own-key system.\n   */\n  openai_api_key: string;\n};\nexport type GptImage1EditImageByokOutput = {\n  /**\n   * The edited images.\n   */\n  images: Array<Image>;\n};\nexport type GptImage1TextToImageByokInput = {\n  /**\n   * The prompt to generate the image from.\n   */\n  prompt: string;\n  /**\n   * The size of the image to generate. Default value: `\"auto\"`\n   */\n  image_size?: \"auto\" | \"1024x1024\" | \"1536x1024\" | \"1024x1536\";\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The quality of the image to generate. Default value: `\"auto\"`\n   */\n  quality?: \"auto\" | \"low\" | \"medium\" | \"high\";\n  /**\n   * The background of the image to generate. Default value: `\"auto\"`\n   */\n  background?: \"auto\" | \"transparent\" | \"opaque\";\n  /**\n   * The OpenAI API key to use for the image generation. This endpoint is currently powered by bring-your-own-key system.\n   */\n  openai_api_key: string;\n};\nexport type GptImage1TextToImageByokOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n};\nexport type GrainInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Film grain intensity Default value: `0.4`\n   */\n  grain_intensity?: number;\n  /**\n   * Film grain scale Default value: `10`\n   */\n  grain_scale?: number;\n  /**\n   * Style of film grain to apply Default value: `\"modern\"`\n   */\n  grain_style?:\n    | \"modern\"\n    | \"analog\"\n    | \"kodak\"\n    | \"fuji\"\n    | \"cinematic\"\n    | \"newspaper\";\n};\nexport type GrainOutput = {\n  /**\n   * The processed images with grain effect\n   */\n  images: Array<Image>;\n};\nexport type GuidanceInput = {\n  /**\n   * The image that should be used as guidance, in base64 format, with the method defined in guidance_method_1. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB. If more then one guidance method is used, all guidance images must be of the same aspect ratio, and this will be the aspect ratio of the generated results. If guidance_method_1 is selected, an image must be provided.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Which guidance type you would like to include in the generation. Up to 4 guidance methods can be combined during a single inference. This parameter is optional.\n   */\n  method?:\n    | \"controlnet_canny\"\n    | \"controlnet_depth\"\n    | \"controlnet_recoloring\"\n    | \"controlnet_color_grid\";\n  /**\n   * Impact of the guidance. Default value: `1`\n   */\n  scale?: number;\n};\nexport type HairChangeInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The desired hair style to apply. Default value: `\"bald\"`\n   */\n  prompt?: string;\n};\nexport type HairChangeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type HidreamE11Input = {\n  /**\n   * The instruction to edit the image.\n   */\n  edit_instruction?: string;\n  /**\n   * The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.\n   */\n  target_image_description?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"low resolution, blur\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of an input image to edit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your initial image when looking for a related image to show you. Default value: `2`\n   */\n  image_guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type HidreamE11Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HidreamE1FullInput = {\n  /**\n   * The instruction to edit the image.\n   */\n  edit_instruction?: string;\n  /**\n   * The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination.\n   */\n  target_image_description?: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"low resolution, blur\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of an input image to edit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your initial image when looking for a related image to show you. Default value: `2`\n   */\n  image_guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type HidreamE1FullOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HidreamI1DevInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type HidreamI1DevOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HidreamI1FastInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `16`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type HidreamI1FastOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HidreamI1FullImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image. Setting to None uses the input image's size.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The image URL to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Denoising strength for image-to-image generation. Default value: `0.75`\n   */\n  strength?: number;\n};\nexport type HidreamI1FullImageToImageOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type HidreamI1FullInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.\n   */\n  loras?: Array<LoraWeight>;\n};\nexport type HindiOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type Hunyuan_worldImageToWorldInput = {\n  /**\n   * The URL of the image to convert to a world.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Labels for the first foreground object.\n   */\n  labels_fg1: string;\n  /**\n   * Labels for the second foreground object.\n   */\n  labels_fg2: string;\n  /**\n   * Classes to use for the world generation.\n   */\n  classes: string;\n  /**\n   * Whether to export DRC (Dynamic Resource Configuration).\n   */\n  export_drc?: boolean;\n};\nexport type Hunyuan_worldImageToWorldOutput = {\n  /**\n   * The generated world.\n   */\n  world_file: File;\n};\nexport type Hunyuan_worldInput = {\n  /**\n   * The URL of the image to convert to a panorama.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to use for the panorama generation.\n   */\n  prompt: string;\n};\nexport type Hunyuan_worldOutput = {\n  /**\n   * The generated panorama image.\n   */\n  image: Image;\n};\nexport type Hunyuan3DInput = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  input_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV21Input = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  input_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV21Output = {\n  /**\n   * Generated 3D object.\n   */\n  model_glb: File;\n  /**\n   * Generated 3D object with PBR materials.\n   */\n  model_glb_pbr?: File;\n  /**\n   * Generated 3D object assets zip.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type Hunyuan3dV2Input = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  input_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV2MiniInput = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  input_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV2MiniOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type Hunyuan3dV2MiniTurboInput = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  input_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV2MiniTurboOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type Hunyuan3dV2MultiViewInput = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  front_image_url: string | Blob | File;\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  back_image_url: string | Blob | File;\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  left_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV2MultiViewOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type Hunyuan3dV2MultiViewTurboInput = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  front_image_url: string | Blob | File;\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  back_image_url: string | Blob | File;\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  left_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV2MultiViewTurboOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type Hunyuan3dV2Output = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type Hunyuan3dV2TurboInput = {\n  /**\n   * URL of image to use while generating the 3D model.\n   */\n  input_image_url: string | Blob | File;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for the model. Default value: `7.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Octree resolution for the model. Default value: `256`\n   */\n  octree_resolution?: number;\n  /**\n   * If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh.\n   */\n  textured_mesh?: boolean;\n};\nexport type Hunyuan3dV2TurboOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type HunyuanAvatarInput = {\n  /**\n   * The URL of the audio file.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * The URL of the reference image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text prompt describing the scene. Default value: `\"A cat is singing.\"`\n   */\n  text?: string;\n  /**\n   * Number of video frames to generate at 25 FPS. If greater than the input audio length, it will capped to the length of the input audio. Default value: `129`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If true, the video will be generated faster with no noticeable degradation in the visual quality. Default value: `true`\n   */\n  turbo_mode?: boolean;\n  /**\n   * Random seed for generation.\n   */\n  seed?: number;\n};\nexport type HunyuanAvatarOutput = {\n  /**\n   * The generated video with the avatar animation.\n   */\n  video: File;\n};\nexport type HunyuanCustomInput = {\n  /**\n   * Text prompt for video generation (max 500 characters).\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border.\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations. Default value: `\"512p\"`\n   */\n  resolution?: \"512p\" | \"720p\";\n  /**\n   * The frames per second of the generated video. Default value: `25`\n   */\n  fps?: number;\n  /**\n   * Classifier-Free Guidance scale for the generation. Default value: `7.5`\n   */\n  cfg_scale?: number;\n  /**\n   * The number of frames to generate. Default value: `129`\n   */\n  num_frames?: number;\n  /**\n   * Whether to enable prompt expansion. Default value: `true`\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type HunyuanCustomOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanPortraitInput = {\n  /**\n   * The URL of the driving video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The URL of the source image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for generation. If None, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Whether to use ArcFace for face recognition. Default value: `true`\n   */\n  use_arcface?: boolean;\n};\nexport type HunyuanPortraitOutput = {\n  /**\n   * The generated video with the portrait animation.\n   */\n  video: File;\n};\nexport type HunyuanVideoImageToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\";\n  /**\n   * Turning on I2V Stability reduces hallucination but also reduces motion.\n   */\n  i2v_stability?: boolean;\n};\nexport type HunyuanVideoImageToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoImg2vidLoraInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n};\nexport type HunyuanVideoImg2vidLoraOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoLoraInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n};\nexport type HunyuanVideoLoraOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoLoraTrainingInput = {\n  /**\n   * URL to zip archive with images. Try to use at least 4 images in general the more the better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Number of steps to train the LoRA on.\n   */\n  steps: number;\n  /**\n   * The trigger word to use. Default value: `\"\"`\n   */\n  trigger_word?: string;\n  /**\n   * Learning rate to use for training. Default value: `0.0001`\n   */\n  learning_rate?: number;\n  /**\n   * Whether to generate captions for the images. Default value: `true`\n   */\n  do_caption?: boolean;\n  /**\n   * The format of the archive. If not specified, the format will be inferred from the URL.\n   */\n  data_archive_format?: string;\n};\nexport type HunyuanVideoLoraTrainingOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the lora configuration file.\n   */\n  config_file: File;\n};\nexport type HunyuanVideoLoraVideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * URL of the video\n   */\n  video_url: string | Blob | File;\n  /**\n   * Strength of video-to-video Default value: `0.75`\n   */\n  strength?: number;\n};\nexport type HunyuanVideoLoraVideoToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type HunyuanVideoVideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to run. Lower gets faster results, higher gets better results. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The seed to use for generating the video.\n   */\n  seed?: number;\n  /**\n   * By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.\n   */\n  pro_mode?: boolean;\n  /**\n   * The aspect ratio of the video to generate. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The resolution of the video to generate. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * The number of frames to generate. Default value: `\"129\"`\n   */\n  num_frames?: \"129\" | \"85\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of the video input.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Strength for Video-to-Video Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type HunyuanVideoVideoToVideoOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generating the video.\n   */\n  seed: number;\n};\nexport type Hyper3dRodinInput = {\n  /**\n   * A textual prompt to guide model generation. Required for Text-to-3D mode. Optional for Image-to-3D mode. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * URL of images to use while generating the 3D model. Required for Image-to-3D mode. Optional for Text-to-3D mode.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * For fuse mode, One or more images are required.It will generate a model by extracting and fusing features of objects from multiple images.For concat mode, need to upload multiple multi-view images of the same object and generate the model.(You can upload multi-view images in any order, regardless of the order of view.) Default value: `\"concat\"`\n   */\n  condition_mode?: \"fuse\" | \"concat\";\n  /**\n   * Seed value for randomization, ranging from 0 to 65535. Optional.\n   */\n  seed?: number;\n  /**\n   * Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb. Default value: `\"glb\"`\n   */\n  geometry_file_format?: \"glb\" | \"usdz\" | \"fbx\" | \"obj\" | \"stl\";\n  /**\n   * Material type. Possible values: PBR, Shaded. Default is PBR. Default value: `\"PBR\"`\n   */\n  material?: \"PBR\" | \"Shaded\";\n  /**\n   * Generation quality. Possible values: high, medium, low, extra-low. Default is medium. Default value: `\"medium\"`\n   */\n  quality?: \"high\" | \"medium\" | \"low\" | \"extra-low\";\n  /**\n   * Whether to export the model using hyper mode. Default is false.\n   */\n  use_hyper?: boolean;\n  /**\n   * Tier of generation. For Rodin Sketch, set to Sketch. For Rodin Regular, set to Regular. Default value: `\"Regular\"`\n   */\n  tier?: \"Regular\" | \"Sketch\";\n  /**\n   * When generating the human-like model, this parameter control the generation result to T/A Pose.\n   */\n  TAPose?: boolean;\n  /**\n   * An array that specifies the dimensions and scaling factor of the bounding box. Typically, this array contains 3 elements, Length(X-axis), Width(Y-axis) and Height(Z-axis).\n   */\n  bbox_condition?: Array<number>;\n  /**\n   * Generation add-on features. Default is []. Possible values are HighPack. The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost triple the billable units.\n   */\n  addons?: \"HighPack\";\n};\nexport type Hyper3dRodinOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n  /**\n   * Generated textures for the 3D object.\n   */\n  textures: Array<Image>;\n};\nexport type I2VDirectorOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type I2VLiveOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type I2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type IdeogramCharacterEditInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes. Default value: `\"AUTO\"`\n   */\n  style?: \"AUTO\" | \"REALISTIC\" | \"FICTION\";\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format\n   */\n  reference_image_urls: Array<string>;\n};\nexport type IdeogramCharacterEditOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramCharacterInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes. Default value: `\"AUTO\"`\n   */\n  style?: \"AUTO\" | \"REALISTIC\" | \"FICTION\";\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The resolution of the generated image Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format\n   */\n  reference_image_urls: Array<string>;\n};\nexport type IdeogramCharacterOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramCharacterRemixInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes. Default value: `\"AUTO\"`\n   */\n  style?: \"AUTO\" | \"REALISTIC\" | \"FICTION\";\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * The resolution of the generated image Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format\n   */\n  reference_image_urls: Array<string>;\n};\nexport type IdeogramCharacterRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramUpscaleInput = {\n  /**\n   * The image URL to upscale\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to upscale the image with Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The resemblance of the upscaled image to the original image Default value: `50`\n   */\n  resemblance?: number;\n  /**\n   * The detail of the upscaled image Default value: `50`\n   */\n  detail?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramUpscaleOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2aOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aRemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2aRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aTurboInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2aTurboOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2aTurboRemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2aTurboRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2EditInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2EditOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV2Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2RemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2RemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2TurboEditInput = {\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2TurboEditOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2TurboInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV2TurboOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV2TurboRemixInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type IdeogramV2TurboRemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV3EditInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The prompt to fill the masked part of the image.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The mask URL to inpaint the image. Needs to match the dimensions of the input image.\n   */\n  mask_url: string | Blob | File;\n};\nexport type IdeogramV3EditOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV3Input = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes.\n   */\n  style?: \"AUTO\" | \"GENERAL\" | \"REALISTIC\" | \"DESIGN\";\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The resolution of the generated image Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV3Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV3ReframeInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes.\n   */\n  style?: \"AUTO\" | \"GENERAL\" | \"REALISTIC\" | \"DESIGN\";\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The image URL to reframe\n   */\n  image_url: string | Blob | File;\n  /**\n   * The resolution for the reframed output image\n   */\n  image_size:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type IdeogramV3ReframeOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV3RemixInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes.\n   */\n  style?: \"AUTO\" | \"GENERAL\" | \"REALISTIC\" | \"DESIGN\";\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * The resolution of the generated image Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type IdeogramV3RemixOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type IdeogramV3ReplaceBackgroundInput = {\n  /**\n   * A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format\n   */\n  image_urls?: Array<string>;\n  /**\n   * The rendering speed to use. Default value: `\"BALANCED\"`\n   */\n  rendering_speed?: \"TURBO\" | \"BALANCED\" | \"QUALITY\";\n  /**\n   * A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)\n   */\n  color_palette?: ColorPalette;\n  /**\n   * A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style\n   */\n  style_codes?: Array<string>;\n  /**\n   * The style type to generate with. Cannot be used with style_codes.\n   */\n  style?: \"AUTO\" | \"GENERAL\" | \"REALISTIC\" | \"DESIGN\";\n  /**\n   * Determine if MagicPrompt should be used in generating the request or not. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Cyber punk city with neon lights and skyscrappers\n   */\n  prompt: string;\n  /**\n   * The image URL whose background needs to be replaced\n   */\n  image_url: string | Blob | File;\n};\nexport type IdeogramV3ReplaceBackgroundOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type Image2svgInput = {\n  /**\n   * The image to convert to SVG\n   */\n  image_url: string | Blob | File;\n  /**\n   * Choose between color or binary (black and white) output Default value: `\"color\"`\n   */\n  colormode?: \"color\" | \"binary\";\n  /**\n   * Hierarchical mode: stacked or cutout Default value: `\"stacked\"`\n   */\n  hierarchical?: \"stacked\" | \"cutout\";\n  /**\n   * Mode: spline (curved) or polygon (straight lines) Default value: `\"spline\"`\n   */\n  mode?: \"spline\" | \"polygon\";\n  /**\n   * Filter out small speckles and noise Default value: `4`\n   */\n  filter_speckle?: number;\n  /**\n   * Color quantization level Default value: `6`\n   */\n  color_precision?: number;\n  /**\n   * Layer difference threshold for hierarchical mode Default value: `16`\n   */\n  layer_difference?: number;\n  /**\n   * Corner detection threshold in degrees Default value: `60`\n   */\n  corner_threshold?: number;\n  /**\n   * Length threshold for curves/lines Default value: `4`\n   */\n  length_threshold?: number;\n  /**\n   * Maximum number of iterations for optimization Default value: `10`\n   */\n  max_iterations?: number;\n  /**\n   * Splice threshold for joining paths Default value: `45`\n   */\n  splice_threshold?: number;\n  /**\n   * Decimal precision for path coordinates Default value: `3`\n   */\n  path_precision?: number;\n};\nexport type Image2svgOutput = {\n  /**\n   * The converted SVG file\n   */\n  images: Array<File>;\n};\nexport type ImageConditioningInput = {\n  /**\n   * URL of image to use as conditioning\n   */\n  image_url: string | Blob | File;\n  /**\n   * Frame number of the image from which the conditioning starts. Must be a multiple of 8.\n   */\n  start_frame_num: number;\n};\nexport type ImageEditingAgeProgressionInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The age change to apply. Default value: `\"20 years older\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingAgeProgressionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingBabyVersionInput = {\n  /**\n   * URL of the image to transform into a baby version.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingBabyVersionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingBackgroundChangeInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The desired background to apply. Default value: `\"beach sunset with palm trees\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingBackgroundChangeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingBroccoliHaircutInput = {\n  /**\n   * URL of the image to apply broccoli haircut style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingBroccoliHaircutOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingCartoonifyInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingCartoonifyOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingColorCorrectionInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingColorCorrectionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingExpressionChangeInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The desired facial expression to apply. Default value: `\"sad\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingExpressionChangeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingFaceEnhancementInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingFaceEnhancementOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingHairChangeInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The desired hair style to apply. Default value: `\"bald\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingHairChangeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingObjectRemovalInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Specify which objects to remove from the image. Default value: `\"background people\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingObjectRemovalOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingPhotoRestorationInput = {\n  /**\n   * URL of the old or damaged photo to restore.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingPhotoRestorationOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingPlushieStyleInput = {\n  /**\n   * URL of the image to convert to plushie style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingPlushieStyleOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingProfessionalPhotoInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingProfessionalPhotoOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingRealismInput = {\n  /**\n   * URL of the image to enhance with realism details.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.6`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingRealismOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingReframeInput = {\n  /**\n   * URL of the old or damaged photo to restore.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The desired aspect ratio for the reframed image. Default value: `\"16:9\"`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingReframeOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingRetouchInput = {\n  /**\n   * URL of the image to retouch.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingRetouchOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingSceneCompositionInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Describe the scene where you want to place the subject. Default value: `\"enchanted forest\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingSceneCompositionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingStyleTransferInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The artistic style to apply. Default value: `\"Van Gogh's Starry Night\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingStyleTransferOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingTextRemovalInput = {\n  /**\n   * URL of the image containing text to be removed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingTextRemovalOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingTimeOfDayInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The time of day to transform the scene to. Default value: `\"golden hour\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingTimeOfDayOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingWeatherEffectInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The weather effect to apply. Default value: `\"heavy snowfall\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingWeatherEffectOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingWojakStyleInput = {\n  /**\n   * URL of the image to convert to wojak style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageEditingWojakStyleOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditingYoutubeThumbnailsInput = {\n  /**\n   * URL of the image to convert to YouTube thumbnail style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.5`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The text to include in the YouTube thumbnail. Default value: `\"Generate youtube thumbnails\"`\n   */\n  prompt?: string;\n};\nexport type ImageEditingYoutubeThumbnailsOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ImageEditInput = {\n  /**\n   * The prompt to edit the image with.\n   */\n  prompt: string;\n  /**\n   * The seed to use for the generation.\n   */\n  seed?: number;\n  /**\n   * Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%.\n   */\n  use_thought?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The image to edit.\n   */\n  image_url: string | Blob | File;\n};\nexport type ImageEditOutput = {\n  /**\n   * The edited images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type ImageExpansionInput = {\n  /**\n   * The URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels.\n   */\n  canvas_size: Array<number>;\n  /**\n   * The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results.\n   */\n  original_image_size: Array<number>;\n  /**\n   * The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped).\n   */\n  original_image_location: Array<number>;\n  /**\n   * Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional.\n   */\n  seed?: number;\n  /**\n   * The negative prompt you would like to use to generate images. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ImageExpansionOutput = {\n  /**\n   * The generated image\n   */\n  image: Image;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type ImageFillInput = {\n  /**\n   * URLs of images to be filled into the masked area.\n   */\n  fill_image_url?: Array<string | Blob | File> | string | Blob | File;\n  /**\n   * Uses the provided fill image in context with the base image to fill in more faithfully. Will increase price.\n   */\n  in_context_fill?: boolean;\n  /**\n   * Whether to use the prompt as well in the generation, along with the redux image.\n   */\n  use_prompt?: boolean;\n};\nexport type ImageGenInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The seed to use for the generation.\n   */\n  seed?: number;\n  /**\n   * Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%.\n   */\n  use_thought?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type ImageInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * Url for the Input image.\n   */\n  image_url: string | Blob | File;\n};\nexport type Imagen3FastInput = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type Imagen3FastOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type Imagen3Input = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type Imagen3Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type Imagen4PreviewInput = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type Imagen4PreviewOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type Imagen4PreviewUltraInput = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type Imagen4PreviewUltraOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type ImageOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type ImageProcessingInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Enable film grain effect\n   */\n  enable_grain?: boolean;\n  /**\n   * Film grain intensity (when enabled) Default value: `0.4`\n   */\n  grain_intensity?: number;\n  /**\n   * Film grain scale (when enabled) Default value: `10`\n   */\n  grain_scale?: number;\n  /**\n   * Style of film grain to apply Default value: `\"modern\"`\n   */\n  grain_style?:\n    | \"modern\"\n    | \"analog\"\n    | \"kodak\"\n    | \"fuji\"\n    | \"cinematic\"\n    | \"newspaper\";\n  /**\n   * Enable color correction\n   */\n  enable_color_correction?: boolean;\n  /**\n   * Color temperature adjustment\n   */\n  temperature?: number;\n  /**\n   * Brightness adjustment\n   */\n  brightness?: number;\n  /**\n   * Contrast adjustment\n   */\n  contrast?: number;\n  /**\n   * Saturation adjustment\n   */\n  saturation?: number;\n  /**\n   * Gamma adjustment Default value: `1`\n   */\n  gamma?: number;\n  /**\n   * Enable chromatic aberration\n   */\n  enable_chromatic?: boolean;\n  /**\n   * Red channel shift amount\n   */\n  red_shift?: number;\n  /**\n   * Red channel shift direction Default value: `\"horizontal\"`\n   */\n  red_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Green channel shift amount\n   */\n  green_shift?: number;\n  /**\n   * Green channel shift direction Default value: `\"horizontal\"`\n   */\n  green_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Blue channel shift amount\n   */\n  blue_shift?: number;\n  /**\n   * Blue channel shift direction Default value: `\"horizontal\"`\n   */\n  blue_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Enable blur effect\n   */\n  enable_blur?: boolean;\n  /**\n   * Type of blur to apply Default value: `\"gaussian\"`\n   */\n  blur_type?: \"gaussian\" | \"kuwahara\";\n  /**\n   * Blur radius Default value: `3`\n   */\n  blur_radius?: number;\n  /**\n   * Sigma for Gaussian blur Default value: `1`\n   */\n  blur_sigma?: number;\n  /**\n   * Enable vignette effect\n   */\n  enable_vignette?: boolean;\n  /**\n   * Vignette strength (when enabled) Default value: `0.5`\n   */\n  vignette_strength?: number;\n  /**\n   * Enable parabolize effect\n   */\n  enable_parabolize?: boolean;\n  /**\n   * Parabolize coefficient Default value: `1`\n   */\n  parabolize_coeff?: number;\n  /**\n   * Vertex X position Default value: `0.5`\n   */\n  vertex_x?: number;\n  /**\n   * Vertex Y position Default value: `0.5`\n   */\n  vertex_y?: number;\n  /**\n   * Enable color tint effect\n   */\n  enable_tint?: boolean;\n  /**\n   * Tint strength Default value: `1`\n   */\n  tint_strength?: number;\n  /**\n   * Tint color mode Default value: `\"sepia\"`\n   */\n  tint_mode?:\n    | \"sepia\"\n    | \"red\"\n    | \"green\"\n    | \"blue\"\n    | \"cyan\"\n    | \"magenta\"\n    | \"yellow\"\n    | \"purple\"\n    | \"orange\"\n    | \"warm\"\n    | \"cool\"\n    | \"lime\"\n    | \"navy\"\n    | \"vintage\"\n    | \"rose\"\n    | \"teal\"\n    | \"maroon\"\n    | \"peach\"\n    | \"lavender\"\n    | \"olive\";\n  /**\n   * Enable dissolve effect\n   */\n  enable_dissolve?: boolean;\n  /**\n   * URL of second image for dissolve Default value: `\"\"`\n   */\n  dissolve_image_url?: string | Blob | File;\n  /**\n   * Dissolve blend factor Default value: `0.5`\n   */\n  dissolve_factor?: number;\n  /**\n   * Enable dodge and burn effect\n   */\n  enable_dodge_burn?: boolean;\n  /**\n   * Dodge and burn intensity Default value: `0.5`\n   */\n  dodge_burn_intensity?: number;\n  /**\n   * Dodge and burn mode Default value: `\"dodge\"`\n   */\n  dodge_burn_mode?:\n    | \"dodge\"\n    | \"burn\"\n    | \"dodge_and_burn\"\n    | \"burn_and_dodge\"\n    | \"color_dodge\"\n    | \"color_burn\"\n    | \"linear_dodge\"\n    | \"linear_burn\";\n  /**\n   * Enable glow effect\n   */\n  enable_glow?: boolean;\n  /**\n   * Glow intensity Default value: `1`\n   */\n  glow_intensity?: number;\n  /**\n   * Glow blur radius Default value: `5`\n   */\n  glow_radius?: number;\n  /**\n   * Enable sharpen effect\n   */\n  enable_sharpen?: boolean;\n  /**\n   * Type of sharpening to apply Default value: `\"basic\"`\n   */\n  sharpen_mode?: \"basic\" | \"smart\" | \"cas\";\n  /**\n   * Sharpen radius (for basic mode) Default value: `1`\n   */\n  sharpen_radius?: number;\n  /**\n   * Sharpen strength (for basic mode) Default value: `1`\n   */\n  sharpen_alpha?: number;\n  /**\n   * Noise radius for smart sharpen Default value: `7`\n   */\n  noise_radius?: number;\n  /**\n   * Edge preservation factor Default value: `0.75`\n   */\n  preserve_edges?: number;\n  /**\n   * Smart sharpen strength Default value: `5`\n   */\n  smart_sharpen_strength?: number;\n  /**\n   * Smart sharpen blend ratio Default value: `0.5`\n   */\n  smart_sharpen_ratio?: number;\n  /**\n   * CAS sharpening amount Default value: `0.8`\n   */\n  cas_amount?: number;\n  /**\n   * Enable solarize effect\n   */\n  enable_solarize?: boolean;\n  /**\n   * Solarize threshold Default value: `0.5`\n   */\n  solarize_threshold?: number;\n  /**\n   * Enable desaturation effect\n   */\n  enable_desaturate?: boolean;\n  /**\n   * Desaturation factor Default value: `1`\n   */\n  desaturate_factor?: number;\n  /**\n   * Desaturation method Default value: `\"luminance (Rec.709)\"`\n   */\n  desaturate_method?:\n    | \"luminance (Rec.709)\"\n    | \"luminance (Rec.601)\"\n    | \"average\"\n    | \"lightness\";\n};\nexport type ImageTo3dInput = {\n  /**\n   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.\n   */\n  seed?: number;\n  /**\n   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.\n   */\n  face_limit?: number;\n  /**\n   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.\n   */\n  pbr?: boolean;\n  /**\n   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `\"standard\"`\n   */\n  texture?: \"no\" | \"standard\" | \"HD\";\n  /**\n   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.\n   */\n  texture_seed?: number;\n  /**\n   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.\n   */\n  auto_size?: boolean;\n  /**\n   * Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.\n   */\n  style?:\n    | \"person:person2cartoon\"\n    | \"object:clay\"\n    | \"object:steampunk\"\n    | \"animal:venom\"\n    | \"object:barbie\"\n    | \"object:christmas\"\n    | \"gold\"\n    | \"ancient_bronze\";\n  /**\n   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.\n   */\n  quad?: boolean;\n  /**\n   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `original_image`\n   */\n  texture_alignment?: \"original_image\" | \"geometry\";\n  /**\n   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `default`\n   */\n  orientation?: \"default\" | \"align_image\";\n  /**\n   * URL of the image to use for model generation.\n   */\n  image_url: string | Blob | File;\n};\nexport type ImageToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The image to use for control lora. This is used to control the style of the generated image.\n   */\n  control_lora_image_url?: string | Blob | File;\n  /**\n   * The strength of the control lora. Default value: `1`\n   */\n  control_lora_strength?: number;\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type ImageToImageOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<File>;\n};\nexport type ImageToImageTurboInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * URL of Image for Image-to-Image\n   */\n  image_url: string | Blob | File;\n  /**\n   * Strength for Image-to-Image. Default value: `0.83`\n   */\n  strength?: number;\n};\nexport type ImageToVideoHailuo02FastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoHailuo02Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video.\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n  /**\n   * The URL to the image to generate the video from.\n   */\n  image_url: string | Blob | File;\n};\nexport type ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoPreviewInput = {\n  /**\n   * The text prompt describing how the image should be animated\n   */\n  prompt: string;\n  /**\n   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 aspect ratio. If the image is not in 16:9 aspect ratio, it will be cropped to fit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"8s\"`\n   */\n  duration?: \"8s\";\n  /**\n   * Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n   */\n  generate_audio?: boolean;\n  /**\n   * Resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n};\nexport type ImageToVideoPreviewOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoV21MasterOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoV21ProOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoV21StandardOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageToVideoV2MasterOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ImageUnderstandingInput = {\n  /**\n   * The image for the query.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to query the image with.\n   */\n  prompt: string;\n  /**\n   * The seed to use for the generation.\n   */\n  seed?: number;\n};\nexport type ImageUpscaleOutput = {\n  /**\n   * The upscaled image.\n   */\n  image: File;\n};\nexport type Img2ImgOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type InpaintInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type InpaintTurboInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * URL of Image for inpainting\n   */\n  image_url: string | Blob | File;\n  /**\n   * Strength for Image-to-Image. Default value: `0.83`\n   */\n  strength?: number;\n  /**\n   * URL of mask image for inpainting.\n   */\n  mask_image_url: string | Blob | File;\n};\nexport type Input = {\n  /**\n   * List of tracks to be combined into the final media\n   */\n  tracks: Array<Track>;\n};\nexport type InstantCharacterInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The LoRAs to use for the image generation. You can use any only 1 lora at a time.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The scale of the subject image. Higher values will make the subject image more prominent in the generated image. Default value: `1`\n   */\n  scale?: number;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type InstantCharacterOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type InvisibleWatermarkInput = {\n  /**\n   * URL of image to be watermarked or decoded\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text to use as watermark (for encoding only) Default value: `\"watermark\"`\n   */\n  watermark?: string;\n  /**\n   * Whether to decode a watermark from the image instead of encoding\n   */\n  decode?: boolean;\n  /**\n   * Length of watermark bits to decode (required when decode=True)\n   */\n  length?: number;\n};\nexport type InvisibleWatermarkOutput = {\n  /**\n   * The watermarked image file info (when encoding)\n   */\n  image?: Image;\n  /**\n   * The extracted watermark text (when decoding)\n   */\n  extracted_watermark?: string;\n  /**\n   * Length of the watermark bits used (helpful for future decoding)\n   */\n  length?: number;\n};\nexport type ItalianOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type JanusInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `square`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Controls randomness in the generation. Higher values make output more random. Default value: `1`\n   */\n  temperature?: number;\n  /**\n   * Classifier Free Guidance scale - how closely to follow the prompt. Default value: `5`\n   */\n  cfg_weight?: number;\n  /**\n   * Number of images to generate in parallel. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JanusOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JapaneseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type JuggernautFluxBaseImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxBaseImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxBaseInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxBaseOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxLightningInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxLightningOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxLoraInpaintingInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type JuggernautFluxLoraInpaintingOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxLoraInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type JuggernautFluxLoraOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxProImageToImageInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.95`\n   */\n  strength?: number;\n  /**\n   * The number of inference steps to perform. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxProImageToImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type JuggernautFluxProInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type JuggernautFluxProOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KlingV15KolorsVirtualTryOnInput = {\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n  /**\n   * Whether to return the result as a base64 encoded data URI.\n   */\n  sync_mode?: boolean;\n};\nexport type KlingV15KolorsVirtualTryOnOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n};\nexport type KlingV1I2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoLipsyncAudioToVideoInput = {\n  /**\n   * The URL of the video to generate the lip sync for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The URL of the audio to generate the lip sync for.\n   */\n  audio_url: string | Blob | File;\n};\nexport type KlingVideoLipsyncAudioToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoLipsyncTextToVideoInput = {\n  /**\n   * The URL of the video to generate the lip sync for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Text content for lip-sync video generation. Max 120 characters.\n   */\n  text: string;\n  /**\n   * Voice ID to use for speech synthesis\n   */\n  voice_id:\n    | \"genshin_vindi2\"\n    | \"zhinen_xuesheng\"\n    | \"AOT\"\n    | \"ai_shatang\"\n    | \"genshin_klee2\"\n    | \"genshin_kirara\"\n    | \"ai_kaiya\"\n    | \"oversea_male1\"\n    | \"ai_chenjiahao_712\"\n    | \"girlfriend_4_speech02\"\n    | \"chat1_female_new-3\"\n    | \"chat_0407_5-1\"\n    | \"cartoon-boy-07\"\n    | \"uk_boy1\"\n    | \"cartoon-girl-01\"\n    | \"PeppaPig_platform\"\n    | \"ai_huangzhong_712\"\n    | \"ai_huangyaoshi_712\"\n    | \"ai_laoguowang_712\"\n    | \"chengshu_jiejie\"\n    | \"you_pingjing\"\n    | \"calm_story1\"\n    | \"uk_man2\"\n    | \"laopopo_speech02\"\n    | \"heainainai_speech02\"\n    | \"reader_en_m-v1\"\n    | \"commercial_lady_en_f-v1\"\n    | \"tiyuxi_xuedi\"\n    | \"tiexin_nanyou\"\n    | \"girlfriend_1_speech02\"\n    | \"girlfriend_2_speech02\"\n    | \"zhuxi_speech02\"\n    | \"uk_oldman3\"\n    | \"dongbeilaotie_speech02\"\n    | \"chongqingxiaohuo_speech02\"\n    | \"chuanmeizi_speech02\"\n    | \"chaoshandashu_speech02\"\n    | \"ai_taiwan_man2_speech02\"\n    | \"xianzhanggui_speech02\"\n    | \"tianjinjiejie_speech02\"\n    | \"diyinnansang_DB_CN_M_04-v2\"\n    | \"yizhipiannan-v1\"\n    | \"guanxiaofang-v2\"\n    | \"tianmeixuemei-v1\"\n    | \"daopianyansang-v1\"\n    | \"mengwa-v1\";\n  /**\n   * The voice language corresponding to the Voice ID Default value: `\"en\"`\n   */\n  voice_language?: \"zh\" | \"en\";\n  /**\n   * Speech rate for Text to Video generation Default value: `1`\n   */\n  voice_speed?: number;\n};\nexport type KlingVideoLipsyncTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV15ProEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene:\n    | \"hug\"\n    | \"kiss\"\n    | \"heart_gesture\"\n    | \"squish\"\n    | \"expansion\"\n    | \"fuzzyfuzzy\"\n    | \"bloombloom\"\n    | \"dizzydizzy\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV15ProEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene:\n    | \"hug\"\n    | \"kiss\"\n    | \"heart_gesture\"\n    | \"squish\"\n    | \"expansion\"\n    | \"fuzzyfuzzy\"\n    | \"bloombloom\"\n    | \"dizzydizzy\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV16ProEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProElementsInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * List of image URLs to use for video generation. Supports up to 4 images.\n   */\n  input_image_urls: Array<string>;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n};\nexport type KlingVideoV16ProElementsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * URL of the image to be used for the end of the video\n   */\n  tail_image_url?: string | Blob | File;\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16ProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16ProTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16ProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene:\n    | \"hug\"\n    | \"kiss\"\n    | \"heart_gesture\"\n    | \"squish\"\n    | \"expansion\"\n    | \"fuzzyfuzzy\"\n    | \"bloombloom\"\n    | \"dizzydizzy\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV16StandardEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardElementsInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * List of image URLs to use for video generation. Supports up to 4 images.\n   */\n  input_image_urls: Array<string>;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n};\nexport type KlingVideoV16StandardElementsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16StandardImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV16StandardTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV16StandardTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1StandardEffectsInput = {\n  /**\n   * URL of images to be used for hug, kiss or heart_gesture video.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The effect scene to use for the video generation\n   */\n  effect_scene:\n    | \"hug\"\n    | \"kiss\"\n    | \"heart_gesture\"\n    | \"squish\"\n    | \"expansion\"\n    | \"fuzzyfuzzy\"\n    | \"bloombloom\"\n    | \"dizzydizzy\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n};\nexport type KlingVideoV1StandardEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV1StandardTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n  /**\n   * Camera control parameters\n   */\n  camera_control?:\n    | \"down_back\"\n    | \"forward_up\"\n    | \"right_turn_forward\"\n    | \"left_turn_forward\";\n  /**\n   * Advanced Camera control parameters\n   */\n  advanced_camera_control?: CameraControl;\n};\nexport type KlingVideoV1StandardTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV21MasterImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to be used for the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV21MasterImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV21MasterTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV21MasterTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV21ProImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to be used for the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV21ProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV21StandardImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to be used for the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV21StandardImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV2MasterImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to be used for the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV2MasterImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KlingVideoV2MasterTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"10\";\n  /**\n   * The aspect ratio of the generated video frame Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   *  Default value: `\"blur, distort, and low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt. Default value: `0.5`\n   */\n  cfg_scale?: number;\n};\nexport type KlingVideoV2MasterTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type KokoroAmericanEnglishInput = {\n  /**\n   *  Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * Voice ID for the desired voice. Default value: `\"af_heart\"`\n   */\n  voice?:\n    | \"af_heart\"\n    | \"af_alloy\"\n    | \"af_aoede\"\n    | \"af_bella\"\n    | \"af_jessica\"\n    | \"af_kore\"\n    | \"af_nicole\"\n    | \"af_nova\"\n    | \"af_river\"\n    | \"af_sarah\"\n    | \"af_sky\"\n    | \"am_adam\"\n    | \"am_echo\"\n    | \"am_eric\"\n    | \"am_fenrir\"\n    | \"am_liam\"\n    | \"am_michael\"\n    | \"am_onyx\"\n    | \"am_puck\"\n    | \"am_santa\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroAmericanEnglishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroBrazilianPortugueseInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"pf_dora\" | \"pm_alex\" | \"pm_santa\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroBrazilianPortugueseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroBritishEnglishInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice:\n    | \"bf_alice\"\n    | \"bf_emma\"\n    | \"bf_isabella\"\n    | \"bf_lily\"\n    | \"bm_daniel\"\n    | \"bm_fable\"\n    | \"bm_george\"\n    | \"bm_lewis\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroBritishEnglishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroFrenchInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"ff_siwis\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroFrenchOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroHindiInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"hf_alpha\" | \"hf_beta\" | \"hm_omega\" | \"hm_psi\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroHindiOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroItalianInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"if_sara\" | \"im_nicola\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroItalianOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroJapaneseInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"jf_alpha\" | \"jf_gongitsune\" | \"jf_nezumi\" | \"jf_tebukuro\" | \"jm_kumo\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroJapaneseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroMandarinChineseInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice:\n    | \"zf_xiaobei\"\n    | \"zf_xiaoni\"\n    | \"zf_xiaoxiao\"\n    | \"zf_xiaoyi\"\n    | \"zm_yunjian\"\n    | \"zm_yunxi\"\n    | \"zm_yunxia\"\n    | \"zm_yunyang\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroMandarinChineseOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KokoroSpanishInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Voice ID for the desired voice.\n   */\n  voice: \"ef_dora\" | \"em_alex\" | \"em_santa\";\n  /**\n   * Speed of the generated audio. Default is 1.0. Default value: `1`\n   */\n  speed?: number;\n};\nexport type KokoroSpanishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type KontextEditOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KontextImg2ImgOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KontextInpaintOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KontextT2IOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KontextText2ImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KreaOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type KreaReduxOutput = {\n  /**\n   * The generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type LatentsyncInput = {\n  /**\n   * The URL of the video to generate the lip sync for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The URL of the audio to generate the lip sync for.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Guidance scale for the model inference Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * Random seed for generation. If None, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Video loop mode when audio is longer than video. Options: pingpong, loop\n   */\n  loop_mode?: \"pingpong\" | \"loop\";\n};\nexport type LatentsyncOutput = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type LDMTTSInput = {\n  /**\n   * The dialogue text with turn prefixes to distinguish speakers.\n   */\n  input: string;\n  /**\n   * A list of voice definitions for each speaker in the dialogue. Must be between 1 and 2 voices.\n   */\n  voices?: Array<LDMVoiceInput>;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type LDMTTSOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type LDMVoiceInput = {\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * A prefix to identify the speaker in multi-turn dialogues. Default value: `\"Speaker 1: \"`\n   */\n  turn_prefix?: string;\n};\nexport type LeffaPoseTransferInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  pose_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  person_image_url: string | Blob | File;\n};\nexport type LeffaPoseTransferOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type LeffaVirtualTryonInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n  /**\n   * The type of the garment used for virtual try-on.\n   */\n  garment_type: \"upper_body\" | \"lower_body\" | \"dresses\";\n};\nexport type LeffaVirtualTryonOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type LipsyncA2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LipsyncInput = {\n  /**\n   *\n   */\n  video_url: string | Blob | File;\n  /**\n   *\n   */\n  audio_url: string | Blob | File;\n};\nexport type LipSyncInput = {\n  /**\n   * The model to use for lipsyncing Default value: `\"lipsync-1.9.0-beta\"`\n   */\n  model?: \"lipsync-1.8.0\" | \"lipsync-1.7.1\" | \"lipsync-1.9.0-beta\";\n  /**\n   * URL of the input video\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the input audio\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Lipsync mode when audio and video durations are out of sync. Default value: `\"cut_off\"`\n   */\n  sync_mode?: \"cut_off\" | \"loop\" | \"bounce\" | \"silence\" | \"remap\";\n};\nexport type LipsyncOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LipSyncOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LipSyncV2Input = {\n  /**\n   * URL of the input video\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the input audio\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Lipsync mode when audio and video durations are out of sync. Default value: `\"cut_off\"`\n   */\n  sync_mode?: \"cut_off\" | \"loop\" | \"bounce\" | \"silence\" | \"remap\";\n};\nexport type LipSyncV2Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LoudnormInput = {\n  /**\n   * URL of the audio file to normalize\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Integrated loudness target in LUFS. Default value: `-18`\n   */\n  integrated_loudness?: number;\n  /**\n   * Maximum true peak in dBTP. Default value: `-0.1`\n   */\n  true_peak?: number;\n  /**\n   * Loudness range target in LU Default value: `7`\n   */\n  loudness_range?: number;\n  /**\n   * Offset gain in dB applied before the true-peak limiter\n   */\n  offset?: number;\n  /**\n   * Use linear normalization mode (single-pass). If false, uses dynamic mode (two-pass for better quality).\n   */\n  linear?: boolean;\n  /**\n   * Treat mono input files as dual-mono for correct EBU R128 measurement on stereo systems\n   */\n  dual_mono?: boolean;\n  /**\n   * Return loudness measurement summary with the normalized audio\n   */\n  print_summary?: boolean;\n  /**\n   * Measured integrated loudness of input file in LUFS. Required for linear mode.\n   */\n  measured_i?: number;\n  /**\n   * Measured loudness range of input file in LU. Required for linear mode.\n   */\n  measured_lra?: number;\n  /**\n   * Measured true peak of input file in dBTP. Required for linear mode.\n   */\n  measured_tp?: number;\n  /**\n   * Measured threshold of input file in LUFS. Required for linear mode.\n   */\n  measured_thresh?: number;\n};\nexport type LoudnormOutput = {\n  /**\n   * Normalized audio file\n   */\n  audio: File;\n  /**\n   * Structured loudness measurement summary (if requested)\n   */\n  summary?: LoudnormSummary;\n};\nexport type Ltxv13b098DistilledExtendInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `24`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.\n   */\n  enable_detail_pass?: boolean;\n  /**\n   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`\n   */\n  temporal_adain_factor?: number;\n  /**\n   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.\n   */\n  tone_map_compression_ratio?: number;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `29`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Video to be extended.\n   */\n  video: ExtendVideoConditioningInput;\n};\nexport type Ltxv13b098DistilledExtendOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type Ltxv13b098DistilledImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `24`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.\n   */\n  enable_detail_pass?: boolean;\n  /**\n   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`\n   */\n  temporal_adain_factor?: number;\n  /**\n   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.\n   */\n  tone_map_compression_ratio?: number;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `29`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type Ltxv13b098DistilledImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type Ltxv13b098DistilledInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `24`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.\n   */\n  enable_detail_pass?: boolean;\n  /**\n   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`\n   */\n  temporal_adain_factor?: number;\n  /**\n   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.\n   */\n  tone_map_compression_ratio?: number;\n};\nexport type Ltxv13b098DistilledMulticonditioningInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `24`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.\n   */\n  enable_detail_pass?: boolean;\n  /**\n   * The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution. Default value: `0.5`\n   */\n  temporal_adain_factor?: number;\n  /**\n   * The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.\n   */\n  tone_map_compression_ratio?: number;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `29`\n   */\n  constant_rate_factor?: number;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type Ltxv13b098DistilledMulticonditioningOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type Ltxv13b098DistilledOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDevExtendInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `30`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `30`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type LtxVideo13bDevExtendOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDevImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `30`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `30`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n};\nexport type LtxVideo13bDevImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDevInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9, 1:1 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `30`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `30`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type LtxVideo13bDevMulticonditioningInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `30`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `3`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `30`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `17`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type LtxVideo13bDevMulticonditioningOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDevOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDistilledExtendInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type LtxVideo13bDistilledExtendOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDistilledImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type LtxVideo13bDistilledImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDistilledInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9, 1:1 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type LtxVideo13bDistilledMulticonditioningInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * LoRA weights to use for generation\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"9:16\" | \"1:1\" | \"16:9\" | \"auto\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The number of frames in the video. Default value: `121`\n   */\n  num_frames?: number;\n  /**\n   * Number of inference steps during the first pass. Default value: `8`\n   */\n  first_pass_num_inference_steps?: number;\n  /**\n   * Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details. Default value: `1`\n   */\n  first_pass_skip_final_steps?: number;\n  /**\n   * Number of inference steps during the second pass. Default value: `8`\n   */\n  second_pass_num_inference_steps?: number;\n  /**\n   * The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes. Default value: `5`\n   */\n  second_pass_skip_initial_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `30`\n   */\n  frame_rate?: number;\n  /**\n   * Whether to expand the prompt using a language model.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality. Default value: `35`\n   */\n  constant_rate_factor?: number;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type LtxVideo13bDistilledMulticonditioningOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideo13bDistilledOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoLoraImageToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Default value: `\"blurry, low quality, low resolution, inconsistent motion, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRA weights to use for generation.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * The resolution of the video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"16:9\" | \"1:1\" | \"9:16\" | \"auto\";\n  /**\n   * The number of frames in the video. Default value: `89`\n   */\n  number_of_frames?: number;\n  /**\n   * The number of inference steps to use. Default value: `30`\n   */\n  number_of_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `25`\n   */\n  frame_rate?: number;\n  /**\n   * The seed to use for generation.\n   */\n  seed?: number;\n  /**\n   * Whether to expand the prompt using the LLM.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The URL of the image to use as input.\n   */\n  image_url: string | Blob | File;\n};\nexport type LtxVideoLoraImageToVideoOutput = {\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n  /**\n   * The generated video.\n   */\n  video: File;\n};\nexport type LtxVideoLoraInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Default value: `\"blurry, low quality, low resolution, inconsistent motion, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRA weights to use for generation.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * The resolution of the video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * The number of frames in the video. Default value: `89`\n   */\n  number_of_frames?: number;\n  /**\n   * The number of inference steps to use. Default value: `30`\n   */\n  number_of_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `25`\n   */\n  frame_rate?: number;\n  /**\n   * The seed to use for generation.\n   */\n  seed?: number;\n  /**\n   * Whether to expand the prompt using the LLM.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type LtxVideoLoraMulticonditioningInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Default value: `\"blurry, low quality, low resolution, inconsistent motion, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRA weights to use for generation.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * The resolution of the video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"16:9\" | \"1:1\" | \"9:16\" | \"auto\";\n  /**\n   * The number of frames in the video. Default value: `89`\n   */\n  number_of_frames?: number;\n  /**\n   * The number of inference steps to use. Default value: `30`\n   */\n  number_of_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `25`\n   */\n  frame_rate?: number;\n  /**\n   * The seed to use for generation.\n   */\n  seed?: number;\n  /**\n   * Whether to expand the prompt using the LLM.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The image conditions to use for generation.\n   */\n  images?: Array<ImageCondition>;\n  /**\n   * The video conditions to use for generation.\n   */\n  videos?: Array<VideoCondition>;\n};\nexport type LtxVideoLoraMulticonditioningOutput = {\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n  /**\n   * The generated video.\n   */\n  video: File;\n};\nexport type LtxVideoLoraOutput = {\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n  /**\n   * The generated video.\n   */\n  video: File;\n};\nexport type LtxVideoTrainerInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.\n   */\n  training_data_url: string | Blob | File;\n  /**\n   * The rank of the LoRA. Default value: `\"128\"`\n   */\n  rank?: \"8\" | \"16\" | \"32\" | \"64\" | \"128\";\n  /**\n   * The number of steps to train for. Default value: `1000`\n   */\n  number_of_steps?: number;\n  /**\n   * The number of frames to use for training. This is the number of frames per second multiplied by the number of seconds. Default value: `81`\n   */\n  number_of_frames?: number;\n  /**\n   * The target frames per second for the video. Default value: `25`\n   */\n  frame_rate?: number;\n  /**\n   * The resolution to use for training. This is the resolution of the video. Default value: `\"medium\"`\n   */\n  resolution?: \"low\" | \"medium\" | \"high\";\n  /**\n   * The aspect ratio to use for training. This is the aspect ratio of the video. Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`\n   */\n  learning_rate?: number;\n  /**\n   * The phrase that will trigger the model to generate an image. Default value: `\"\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * If true, the input will be automatically scale the video to DEFAULT_NUM_FRAMES frames at 16fps.\n   */\n  auto_scale_input?: boolean;\n  /**\n   * If true, input above a certain duration threshold will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. Default value: `true`\n   */\n  split_input_into_scenes?: boolean;\n  /**\n   * The duration threshold in seconds. If a video is longer than this, it will be split into scenes. If you provide captions for a split video, the caption will be applied to each scene. If you do not provide captions, scenes will be auto-captioned. Default value: `30`\n   */\n  split_input_duration_threshold?: number;\n  /**\n   * A list of validation prompts to use during training. When providing an image, _all_ validation inputs must have an image.\n   */\n  validation?: Array<Validation>;\n  /**\n   * A negative prompt to use for validation. Default value: `\"blurry, low quality, bad quality, out of focus\"`\n   */\n  validation_negative_prompt?: string;\n  /**\n   * The number of frames to use for validation. Default value: `81`\n   */\n  validation_number_of_frames?: number;\n  /**\n   * The resolution to use for validation. Default value: `\"high\"`\n   */\n  validation_resolution?: \"low\" | \"medium\" | \"high\";\n  /**\n   * The aspect ratio to use for validation. Default value: `\"1:1\"`\n   */\n  validation_aspect_ratio?: \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * If true, the validation videos will be reversed. This is useful for effects that are learned in reverse and then applied in reverse.\n   */\n  validation_reverse?: boolean;\n};\nexport type LtxVideoTrainerOutput = {\n  /**\n   * The URL to the validations video.\n   */\n  video: File;\n  /**\n   * URL to the trained LoRA weights.\n   */\n  lora_file: File;\n  /**\n   * Configuration used for setting up the inference endpoints.\n   */\n  config_file: File;\n};\nexport type LtxVideoV095ExtendInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type LtxVideoV095ExtendOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095ImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type LtxVideoV095ImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095Input = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type LtxVideoV095MulticonditioningInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type LtxVideoV095MulticonditioningOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV095Output = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV097ExtendInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Video to be extended.\n   */\n  video: VideoConditioningInput;\n};\nexport type LtxVideoV097ExtendOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV097ImageToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Image URL for Image-to-Video task\n   */\n  image_url: string | Blob | File;\n};\nexport type LtxVideoV097ImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV097Input = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type LtxVideoV097MulticonditioningInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type LtxVideoV097MulticonditioningOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LtxVideoV097Output = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type LumaDreamMachineInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n};\nexport type LumaDreamMachineOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2FlashImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Initial image to start the video from. Can be used together with end_image_url.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Final image to end the video with. Can be used together with image_url.\n   */\n  end_image_url?: string | Blob | File;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"9s\";\n};\nexport type LumaDreamMachineRay2FlashImageToVideoOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2FlashInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video (9s costs 2x more) Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"9s\";\n};\nexport type LumaDreamMachineRay2FlashModifyInput = {\n  /**\n   * URL of the input video to modify\n   */\n  video_url: string | Blob | File;\n  /**\n   * Optional URL of the first frame image for modification\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Instruction for modifying the video\n   */\n  prompt?: string;\n  /**\n   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most Default value: `\"flex_1\"`\n   */\n  mode?:\n    | \"adhere_1\"\n    | \"adhere_2\"\n    | \"adhere_3\"\n    | \"flex_1\"\n    | \"flex_2\"\n    | \"flex_3\"\n    | \"reimagine_1\"\n    | \"reimagine_2\"\n    | \"reimagine_3\";\n};\nexport type LumaDreamMachineRay2FlashModifyOutput = {\n  /**\n   * URL of the modified video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2FlashOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2FlashReframeInput = {\n  /**\n   * URL of the input video to reframe\n   */\n  video_url: string | Blob | File;\n  /**\n   * The aspect ratio of the reframed video\n   */\n  aspect_ratio: \"1:1\" | \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Optional URL of the first frame image for reframing\n   */\n  image_url?: string | Blob | File;\n  /**\n   * X position of the grid for reframing\n   */\n  grid_position_x?: number;\n  /**\n   * Y position of the grid for reframing\n   */\n  grid_position_y?: number;\n  /**\n   * Optional prompt for reframing\n   */\n  prompt?: string;\n  /**\n   * End X coordinate for reframing\n   */\n  x_end?: number;\n  /**\n   * Start X coordinate for reframing\n   */\n  x_start?: number;\n  /**\n   * End Y coordinate for reframing\n   */\n  y_end?: number;\n  /**\n   * Start Y coordinate for reframing\n   */\n  y_start?: number;\n};\nexport type LumaDreamMachineRay2FlashReframeOutput = {\n  /**\n   * URL of the reframed video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Initial image to start the video from. Can be used together with end_image_url.\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Final image to end the video with. Can be used together with image_url.\n   */\n  end_image_url?: string | Blob | File;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"9s\";\n};\nexport type LumaDreamMachineRay2ImageToVideoOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Whether the video should loop (end of video is blended with the beginning)\n   */\n  loop?: boolean;\n  /**\n   * The resolution of the generated video (720p costs 2x more, 1080p costs 4x more) Default value: `\"540p\"`\n   */\n  resolution?: \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video (9s costs 2x more) Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"9s\";\n};\nexport type LumaDreamMachineRay2ModifyInput = {\n  /**\n   * URL of the input video to modify\n   */\n  video_url: string | Blob | File;\n  /**\n   * Optional URL of the first frame image for modification\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Instruction for modifying the video\n   */\n  prompt?: string;\n  /**\n   * Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most Default value: `\"flex_1\"`\n   */\n  mode?:\n    | \"adhere_1\"\n    | \"adhere_2\"\n    | \"adhere_3\"\n    | \"flex_1\"\n    | \"flex_2\"\n    | \"flex_3\"\n    | \"reimagine_1\"\n    | \"reimagine_2\"\n    | \"reimagine_3\";\n};\nexport type LumaDreamMachineRay2ModifyOutput = {\n  /**\n   * URL of the modified video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type LumaDreamMachineRay2ReframeInput = {\n  /**\n   * URL of the input video to reframe\n   */\n  video_url: string | Blob | File;\n  /**\n   * The aspect ratio of the reframed video\n   */\n  aspect_ratio: \"1:1\" | \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Optional URL of the first frame image for reframing\n   */\n  image_url?: string | Blob | File;\n  /**\n   * X position of the grid for reframing\n   */\n  grid_position_x?: number;\n  /**\n   * Y position of the grid for reframing\n   */\n  grid_position_y?: number;\n  /**\n   * Optional prompt for reframing\n   */\n  prompt?: string;\n  /**\n   * End X coordinate for reframing\n   */\n  x_end?: number;\n  /**\n   * Start X coordinate for reframing\n   */\n  x_start?: number;\n  /**\n   * End Y coordinate for reframing\n   */\n  y_end?: number;\n  /**\n   * Start Y coordinate for reframing\n   */\n  y_start?: number;\n};\nexport type LumaDreamMachineRay2ReframeOutput = {\n  /**\n   * URL of the reframed video\n   */\n  video: File;\n};\nexport type LumaPhotonFlashInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n};\nexport type LumaPhotonFlashModifyInput = {\n  /**\n   * Instruction for modifying the image\n   */\n  prompt: string;\n  /**\n   * URL of the input image to reframe\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output.\n   */\n  strength: number;\n  /**\n   * The aspect ratio of the reframed image\n   */\n  aspect_ratio: \"1:1\" | \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n};\nexport type LumaPhotonFlashModifyOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LumaPhotonFlashOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LumaPhotonFlashReframeInput = {\n  /**\n   * URL of the input image to reframe\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the reframed image\n   */\n  aspect_ratio: \"1:1\" | \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Optional prompt for reframing\n   */\n  prompt?: string;\n  /**\n   * X position of the grid for reframing\n   */\n  grid_position_x?: number;\n  /**\n   * Y position of the grid for reframing\n   */\n  grid_position_y?: number;\n  /**\n   * End X coordinate for reframing\n   */\n  x_end?: number;\n  /**\n   * Start X coordinate for reframing\n   */\n  x_start?: number;\n  /**\n   * End Y coordinate for reframing\n   */\n  y_end?: number;\n  /**\n   * Start Y coordinate for reframing\n   */\n  y_start?: number;\n};\nexport type LumaPhotonFlashReframeOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LumaPhotonModifyInput = {\n  /**\n   * Instruction for modifying the image\n   */\n  prompt: string;\n  /**\n   * URL of the input image to reframe\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output.\n   */\n  strength: number;\n  /**\n   * The aspect ratio of the reframed image\n   */\n  aspect_ratio: \"1:1\" | \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n};\nexport type LumaPhotonModifyOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LumaPhotonReframeInput = {\n  /**\n   * URL of the input image to reframe\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the reframed image\n   */\n  aspect_ratio: \"1:1\" | \"16:9\" | \"9:16\" | \"4:3\" | \"3:4\" | \"21:9\" | \"9:21\";\n  /**\n   * Optional prompt for reframing\n   */\n  prompt?: string;\n  /**\n   * X position of the grid for reframing\n   */\n  grid_position_x?: number;\n  /**\n   * Y position of the grid for reframing\n   */\n  grid_position_y?: number;\n  /**\n   * End X coordinate for reframing\n   */\n  x_end?: number;\n  /**\n   * Start X coordinate for reframing\n   */\n  x_start?: number;\n  /**\n   * End Y coordinate for reframing\n   */\n  y_end?: number;\n  /**\n   * Start Y coordinate for reframing\n   */\n  y_start?: number;\n};\nexport type LumaPhotonReframeOutput = {\n  /**\n   * The generated image\n   */\n  images: Array<File>;\n};\nexport type LuminaImageV2Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The system prompt to use. Default value: `\"You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts.\"`\n   */\n  system_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to apply normalization-based guidance scale. Default value: `true`\n   */\n  cfg_normalization?: boolean;\n  /**\n   * The ratio of the timestep interval to apply normalization-based guidance scale. Default value: `1`\n   */\n  cfg_trunc_ratio?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type LuminaImageV2Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Lyria2Input = {\n  /**\n   * The text prompt describing the music you want to generate\n   */\n  prompt: string;\n  /**\n   * A description of what to exclude from the generated audio Default value: `\"low quality\"`\n   */\n  negative_prompt?: string;\n  /**\n   * A seed for deterministic generation. If provided, the model will attempt to produce the same audio given the same prompt and other parameters.\n   */\n  seed?: number;\n};\nexport type Lyria2Output = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type MagiDistilledExtendVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`\n   */\n  num_frames?: number;\n  /**\n   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.\n   */\n  start_frame?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `\"16\"`\n   */\n  num_inference_steps?: \"4\" | \"8\" | \"16\" | \"32\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type MagiDistilledExtendVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MagiDistilledImageToVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`\n   */\n  num_frames?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `\"16\"`\n   */\n  num_inference_steps?: \"4\" | \"8\" | \"16\" | \"32\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type MagiDistilledImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MagiDistilledInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`\n   */\n  num_frames?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `\"16\"`\n   */\n  num_inference_steps?: \"4\" | \"8\" | \"16\" | \"32\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type MagiDistilledOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MagiExtendVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * URL of the input video to represent the beginning of the video. If the input video does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`\n   */\n  num_frames?: number;\n  /**\n   * The frame to begin the generation from, with the remaining frames will be treated as the prefix video. The final video will contain the frames up until this number unchanged, followed by the generated frames. The default start frame is 32 frames before the end of the video, which gives optimal results.\n   */\n  start_frame?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `\"16\"`\n   */\n  num_inference_steps?: \"4\" | \"8\" | \"16\" | \"32\" | \"64\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type MagiExtendVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MagiImageToVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * URL of the input image to represent the first frame of the video. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`\n   */\n  num_frames?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `\"16\"`\n   */\n  num_inference_steps?: \"4\" | \"8\" | \"16\" | \"32\" | \"64\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type MagiImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MagiInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 96 and 192 (inclusive). Each additional 24 frames beyond 96 incurs an additional billing unit. Default value: `96`\n   */\n  num_frames?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `\"16\"`\n   */\n  num_inference_steps?: \"4\" | \"8\" | \"16\" | \"32\" | \"64\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type MagiOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MandarinOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type MaskInput = {\n  /**\n   * The URL of the image to remove objects from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask image. White pixels (255) indicate areas to remove.\n   */\n  mask_url: string | Blob | File;\n  /**\n   *  Default value: `\"best_quality\"`\n   */\n  model?: \"low_quality\" | \"medium_quality\" | \"high_quality\" | \"best_quality\";\n  /**\n   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`\n   */\n  mask_expansion?: number;\n};\nexport type MetadataInput = {\n  /**\n   * URL of the media file (video or audio) to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Whether to extract the start and end frames for videos. Note that when true the request will be slower.\n   */\n  extract_frames?: boolean;\n};\nexport type MetadataOutput = {\n  /**\n   * Metadata for the analyzed media file (either Video or Audio)\n   */\n  media: Video | Audio;\n};\nexport type MinimaxHailuo02FastImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `\"6\"`\n   */\n  duration?: \"6\" | \"10\";\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxHailuo02FastImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxHailuo02ProImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxHailuo02ProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxHailuo02ProTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxHailuo02ProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxHailuo02StandardImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `\"6\"`\n   */\n  duration?: \"6\" | \"10\";\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n  /**\n   * The resolution of the generated video. Default value: `\"768P\"`\n   */\n  resolution?: \"512P\" | \"768P\";\n};\nexport type MinimaxHailuo02StandardImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxHailuo02StandardTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `\"6\"`\n   */\n  duration?: \"6\" | \"10\";\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxHailuo02StandardTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxImage01Input = {\n  /**\n   * Text prompt for image generation (max 1500 characters)\n   */\n  prompt: string;\n  /**\n   * Aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"21:9\";\n  /**\n   * Number of images to generate (1-9) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Whether to enable automatic prompt optimization\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxImage01Output = {\n  /**\n   * Generated images\n   */\n  images: Array<File>;\n};\nexport type MinimaxImage01SubjectReferenceInput = {\n  /**\n   * Text prompt for image generation (max 1500 characters)\n   */\n  prompt: string;\n  /**\n   * URL of the subject reference image to use for consistent character appearance\n   */\n  image_url: string | Blob | File;\n  /**\n   * Aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"21:9\";\n  /**\n   * Number of images to generate (1-9) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Whether to enable automatic prompt optimization\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxImage01SubjectReferenceOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<File>;\n};\nexport type MinimaxMusicInput = {\n  /**\n   * Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters.\n   */\n  prompt: string;\n  /**\n   * Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds.\n   */\n  reference_audio_url: string | Blob | File;\n};\nexport type MinimaxMusicOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type MinimaxSpeech02HdInput = {\n  /**\n   * Text to convert to speech (max 5000 characters)\n   */\n  text: string;\n  /**\n   * Voice configuration settings\n   */\n  voice_setting?: VoiceSetting;\n  /**\n   * Audio configuration settings\n   */\n  audio_setting?: AudioSetting;\n  /**\n   * Enhance recognition of specified languages and dialects\n   */\n  language_boost?:\n    | \"Chinese\"\n    | \"Chinese,Yue\"\n    | \"English\"\n    | \"Arabic\"\n    | \"Russian\"\n    | \"Spanish\"\n    | \"French\"\n    | \"Portuguese\"\n    | \"German\"\n    | \"Turkish\"\n    | \"Dutch\"\n    | \"Ukrainian\"\n    | \"Vietnamese\"\n    | \"Indonesian\"\n    | \"Japanese\"\n    | \"Italian\"\n    | \"Korean\"\n    | \"Thai\"\n    | \"Polish\"\n    | \"Romanian\"\n    | \"Greek\"\n    | \"Czech\"\n    | \"Finnish\"\n    | \"Hindi\"\n    | \"auto\";\n  /**\n   * Format of the output content (non-streaming only) Default value: `\"hex\"`\n   */\n  output_format?: \"url\" | \"hex\";\n  /**\n   * Custom pronunciation dictionary for text replacement\n   */\n  pronunciation_dict?: PronunciationDict;\n};\nexport type MinimaxSpeech02HdOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Duration of the audio in milliseconds\n   */\n  duration_ms: number;\n};\nexport type MinimaxSpeech02TurboInput = {\n  /**\n   * Text to convert to speech (max 5000 characters)\n   */\n  text: string;\n  /**\n   * Voice configuration settings\n   */\n  voice_setting?: VoiceSetting;\n  /**\n   * Audio configuration settings\n   */\n  audio_setting?: AudioSetting;\n  /**\n   * Enhance recognition of specified languages and dialects\n   */\n  language_boost?:\n    | \"Chinese\"\n    | \"Chinese,Yue\"\n    | \"English\"\n    | \"Arabic\"\n    | \"Russian\"\n    | \"Spanish\"\n    | \"French\"\n    | \"Portuguese\"\n    | \"German\"\n    | \"Turkish\"\n    | \"Dutch\"\n    | \"Ukrainian\"\n    | \"Vietnamese\"\n    | \"Indonesian\"\n    | \"Japanese\"\n    | \"Italian\"\n    | \"Korean\"\n    | \"Thai\"\n    | \"Polish\"\n    | \"Romanian\"\n    | \"Greek\"\n    | \"Czech\"\n    | \"Finnish\"\n    | \"Hindi\"\n    | \"auto\";\n  /**\n   * Format of the output content (non-streaming only) Default value: `\"hex\"`\n   */\n  output_format?: \"url\" | \"hex\";\n  /**\n   * Custom pronunciation dictionary for text replacement\n   */\n  pronunciation_dict?: PronunciationDict;\n};\nexport type MinimaxSpeech02TurboOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Duration of the audio in milliseconds\n   */\n  duration_ms: number;\n};\nexport type MiniMaxTextToImageOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<File>;\n};\nexport type MiniMaxTextToImageWithReferenceOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<File>;\n};\nexport type MinimaxVideo01DirectorImageToVideoInput = {\n  /**\n   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01DirectorImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01DirectorInput = {\n  /**\n   * Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01DirectorOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01LiveImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01LiveImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01LiveInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01LiveOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVideo01SubjectReferenceInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * URL of the subject reference image to use for consistent subject appearance\n   */\n  subject_reference_image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type MinimaxVideo01SubjectReferenceOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type MinimaxVoiceCloneInput = {\n  /**\n   * URL of the input audio file for voice cloning. Should be at least 10 seconds long.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Enable noise reduction for the cloned voice\n   */\n  noise_reduction?: boolean;\n  /**\n   * Enable volume normalization for the cloned voice\n   */\n  need_volume_normalization?: boolean;\n  /**\n   * Text validation accuracy threshold (0-1)\n   */\n  accuracy?: number;\n  /**\n   * Text to generate a TTS preview with the cloned voice (optional) Default value: `\"Hello, this is a preview of your cloned voice! I hope you like it!\"`\n   */\n  text?: string;\n  /**\n   * TTS model to use for preview. Options: speech-02-hd, speech-02-turbo, speech-01-hd, speech-01-turbo Default value: `\"speech-02-hd\"`\n   */\n  model?:\n    | \"speech-02-hd\"\n    | \"speech-02-turbo\"\n    | \"speech-01-hd\"\n    | \"speech-01-turbo\";\n};\nexport type MinimaxVoiceCloneOutput = {\n  /**\n   * The cloned voice ID for use with TTS\n   */\n  custom_voice_id: string;\n  /**\n   * Preview audio generated with the cloned voice (if requested)\n   */\n  audio?: File;\n};\nexport type MinimaxVoiceDesignInput = {\n  /**\n   * Voice description prompt for generating a personalized voice\n   */\n  prompt: string;\n  /**\n   * Text for audio preview. Limited to 500 characters. A fee of $30 per 1M characters will be charged for the generation of the preview audio.\n   */\n  preview_text: string;\n};\nexport type MinimaxVoiceDesignOutput = {\n  /**\n   * The voice_id of the generated voice\n   */\n  custom_voice_id: string;\n  /**\n   * The preview audio using the generated voice\n   */\n  audio: File;\n};\nexport type MixDehazeNetInput = {\n  /**\n   * URL of image to be used for image enhancement\n   */\n  image_url: string | Blob | File;\n  /**\n   * Model to be used for dehazing Default value: `\"indoor\"`\n   */\n  model?: \"indoor\" | \"outdoor\";\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type MixDehazeNetOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type MmaudioV2Input = {\n  /**\n   * The URL of the video to generate the audio for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The prompt to generate the audio for.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate the audio for. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of steps to generate the audio for. Default value: `25`\n   */\n  num_steps?: number;\n  /**\n   * The duration of the audio to generate. Default value: `8`\n   */\n  duration?: number;\n  /**\n   * The strength of Classifier Free Guidance. Default value: `4.5`\n   */\n  cfg_strength?: number;\n  /**\n   * Whether to mask away the clip.\n   */\n  mask_away_clip?: boolean;\n};\nexport type MmaudioV2Output = {\n  /**\n   * The generated video with the lip sync.\n   */\n  video: File;\n};\nexport type MmaudioV2TextToAudioInput = {\n  /**\n   * The prompt to generate the audio for.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate the audio for. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of steps to generate the audio for. Default value: `25`\n   */\n  num_steps?: number;\n  /**\n   * The duration of the audio to generate. Default value: `8`\n   */\n  duration?: number;\n  /**\n   * The strength of Classifier Free Guidance. Default value: `4.5`\n   */\n  cfg_strength?: number;\n  /**\n   * Whether to mask away the clip.\n   */\n  mask_away_clip?: boolean;\n};\nexport type MmaudioV2TextToAudioOutput = {\n  /**\n   * The generated audio.\n   */\n  audio: File;\n};\nexport type ModifyOutput = {\n  /**\n   * URL of the modified video\n   */\n  video: File;\n};\nexport type Moondream2Input = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n};\nexport type Moondream2ObjectDetectionInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Object to be detected in the image\n   */\n  object: string;\n};\nexport type Moondream2ObjectDetectionOutput = {\n  /**\n   * Objects detected in the image\n   */\n  objects: Array<any>;\n  /**\n   * Image with detected objects\n   */\n  image: Image;\n};\nexport type Moondream2Output = {\n  /**\n   * Output for the given query\n   */\n  output: string;\n};\nexport type Moondream2PointObjectDetectionInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Object to be detected in the image\n   */\n  object: string;\n};\nexport type Moondream2PointObjectDetectionOutput = {\n  /**\n   * Objects detected in the image\n   */\n  objects: Array<any>;\n  /**\n   * Image with detected objects\n   */\n  image: Image;\n};\nexport type Moondream2VisualQueryInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Query to be asked in the image\n   */\n  prompt: string;\n};\nexport type Moondream2VisualQueryOutput = {\n  /**\n   * Output for the given query\n   */\n  output: string;\n};\nexport type MoondreamInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n};\nexport type MoondreamNextBatchInput = {\n  /**\n   * List of image URLs to be processed (maximum 32 images)\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Single prompt to apply to all images\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type MoondreamNextBatchOutput = {\n  /**\n   * URL to the generated captions JSON file containing filename-caption pairs.\n   */\n  captions_file: File;\n  /**\n   * List of generated captions\n   */\n  outputs: Array<string>;\n};\nexport type MoondreamNextDetectionInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of detection to perform\n   */\n  task_type: \"bbox_detection\" | \"point_detection\" | \"gaze_detection\";\n  /**\n   * Text description of what to detect\n   */\n  detection_prompt: string;\n  /**\n   * Whether to use ensemble for gaze detection\n   */\n  use_ensemble?: boolean;\n};\nexport type MoondreamNextDetectionOutput = {\n  /**\n   * Output image with detection visualization\n   */\n  image: Image;\n  /**\n   * Detection results as text\n   */\n  text_output: string;\n};\nexport type MoondreamNextInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of task to perform Default value: `\"caption\"`\n   */\n  task_type?: \"caption\" | \"query\";\n  /**\n   * Prompt for query task\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type MoondreamNextOutput = {\n  /**\n   * Response from the model\n   */\n  output: string;\n};\nexport type MoondreamObjectInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Object to be detected in the image\n   */\n  object: string;\n};\nexport type MoondreamObjectOutput = {\n  /**\n   * Objects detected in the image\n   */\n  objects: Array<any>;\n  /**\n   * Image with detected objects\n   */\n  image: Image;\n};\nexport type MoondreamOutput = {\n  /**\n   * Output for the given query\n   */\n  output: string;\n};\nexport type MoonDreamOutput = {\n  /**\n   * Response from the model\n   */\n  output: string;\n};\nexport type MoondreamQueryInput = {\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Query to be asked in the image\n   */\n  prompt: string;\n};\nexport type MulticonditioningVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Default value: `\"blurry, low quality, low resolution, inconsistent motion, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRA weights to use for generation.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * The resolution of the video. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * The aspect ratio of the video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"16:9\" | \"1:1\" | \"9:16\" | \"auto\";\n  /**\n   * The number of frames in the video. Default value: `89`\n   */\n  number_of_frames?: number;\n  /**\n   * The number of inference steps to use. Default value: `30`\n   */\n  number_of_steps?: number;\n  /**\n   * The frame rate of the video. Default value: `25`\n   */\n  frame_rate?: number;\n  /**\n   * The seed to use for generation.\n   */\n  seed?: number;\n  /**\n   * Whether to expand the prompt using the LLM.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Whether to reverse the video.\n   */\n  reverse_video?: boolean;\n  /**\n   * Whether to enable the safety checker. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The image conditions to use for generation.\n   */\n  images?: Array<ImageCondition>;\n  /**\n   * The video conditions to use for generation.\n   */\n  videos?: Array<VideoCondition>;\n};\nexport type MultiConditioningVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * URL of images to use as conditioning\n   */\n  images?: Array<ImageConditioningInput>;\n  /**\n   * Videos to use as conditioning\n   */\n  videos?: Array<VideoConditioningInput>;\n};\nexport type MulticonditioningVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MultiConditioningVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type MultiViewObjectOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type MultiviewTo3dInput = {\n  /**\n   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.\n   */\n  seed?: number;\n  /**\n   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.\n   */\n  face_limit?: number;\n  /**\n   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.\n   */\n  pbr?: boolean;\n  /**\n   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `\"standard\"`\n   */\n  texture?: \"no\" | \"standard\" | \"HD\";\n  /**\n   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.\n   */\n  texture_seed?: number;\n  /**\n   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.\n   */\n  auto_size?: boolean;\n  /**\n   * Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.\n   */\n  style?:\n    | \"person:person2cartoon\"\n    | \"object:clay\"\n    | \"object:steampunk\"\n    | \"animal:venom\"\n    | \"object:barbie\"\n    | \"object:christmas\"\n    | \"gold\"\n    | \"ancient_bronze\";\n  /**\n   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.\n   */\n  quad?: boolean;\n  /**\n   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `original_image`\n   */\n  texture_alignment?: \"original_image\" | \"geometry\";\n  /**\n   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `default`\n   */\n  orientation?: \"default\" | \"align_image\";\n  /**\n   * Front view image of the object.\n   */\n  front_image_url: string | Blob | File;\n  /**\n   * Left view image of the object.\n   */\n  left_image_url?: string | Blob | File;\n  /**\n   * Back view image of the object.\n   */\n  back_image_url?: string | Blob | File;\n  /**\n   * Right view image of the object.\n   */\n  right_image_url?: string | Blob | File;\n};\nexport type MusicGeneratorInput = {\n  /**\n   * The prompt to generate music from.\n   */\n  prompt: string;\n  /**\n   * The duration of the generated music in seconds.\n   */\n  duration: number;\n};\nexport type MusicGeneratorOutput = {\n  /**\n   * The generated music\n   */\n  audio_file: File;\n};\nexport type NafnetDeblurInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type NafnetDeblurOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type NafnetDenoiseInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type NafnetDenoiseOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type NafnetInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type NafnetOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type ObjectOutput = {\n  /**\n   * Generated 3D object file.\n   */\n  model_mesh: File;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type ObjectRemovalBboxInput = {\n  /**\n   * The URL of the image to remove objects from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * List of bounding box coordinates to erase (only one box prompt is supported)\n   */\n  box_prompts?: Array<BBoxPromptBase>;\n  /**\n   *  Default value: `\"best_quality\"`\n   */\n  model?: \"low_quality\" | \"medium_quality\" | \"high_quality\" | \"best_quality\";\n  /**\n   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`\n   */\n  mask_expansion?: number;\n};\nexport type ObjectRemovalBboxOutput = {\n  /**\n   * The generated images with objects removed.\n   */\n  images: Array<Image>;\n};\nexport type ObjectRemovalInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Specify which objects to remove from the image. Default value: `\"background people\"`\n   */\n  prompt?: string;\n};\nexport type ObjectRemovalMaskInput = {\n  /**\n   * The URL of the image to remove objects from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the mask image. White pixels (255) indicate areas to remove.\n   */\n  mask_url: string | Blob | File;\n  /**\n   *  Default value: `\"best_quality\"`\n   */\n  model?: \"low_quality\" | \"medium_quality\" | \"high_quality\" | \"best_quality\";\n  /**\n   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`\n   */\n  mask_expansion?: number;\n};\nexport type ObjectRemovalMaskOutput = {\n  /**\n   * The generated images with objects removed.\n   */\n  images: Array<Image>;\n};\nexport type ObjectRemovalOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type OmnigenV1Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3`\n   */\n  guidance_scale?: number;\n  /**\n   * The Image Guidance scale is a measure of how close you want\n   * the model to stick to your input image when looking for a related image to show you. Default value: `1.6`\n   */\n  img_guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type OmnigenV1Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type OmnigenV2Input = {\n  /**\n   * The prompt to generate or edit an image. Use specific language like 'Add the bird from image 1 to the desk in image 2' for better results.\n   */\n  prompt: string;\n  /**\n   * URLs of input images to use for image editing or multi-image generation. Support up to 3 images.\n   */\n  input_image_urls?: Array<string>;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The Text Guidance scale controls how closely the model follows the text prompt.\n   * Higher values make the model stick more closely to the prompt. Default value: `5`\n   */\n  text_guidance_scale?: number;\n  /**\n   * The Image Guidance scale controls how closely the model follows the input images.\n   * For image editing: 1.3-2.0, for in-context generation: 2.0-3.0 Default value: `2`\n   */\n  image_guidance_scale?: number;\n  /**\n   * Negative prompt to guide what should not be in the image. Default value: `\"(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar\"`\n   */\n  negative_prompt?: string;\n  /**\n   * CFG range start value.\n   */\n  cfg_range_start?: number;\n  /**\n   * CFG range end value. Default value: `1`\n   */\n  cfg_range_end?: number;\n  /**\n   * The scheduler to use for the diffusion process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"dpmsolver\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type OmnigenV2Output = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type OmniHumanInput = {\n  /**\n   * The URL of the image used to generate the video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the audio file to generate the video. Audio must be under 30s long.\n   */\n  audio_url: string | Blob | File;\n};\nexport type OmniHumanOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Duration of audio input/video output as used for billing.\n   */\n  duration: number;\n};\nexport type OrpheusTtsInput = {\n  /**\n   * The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>\n   */\n  text: string;\n  /**\n   * Voice ID for the desired voice. Default value: `\"tara\"`\n   */\n  voice?: \"tara\" | \"leah\" | \"jess\" | \"leo\" | \"dan\" | \"mia\" | \"zac\" | \"zoe\";\n  /**\n   * Temperature for generation (higher = more creative). Default value: `0.7`\n   */\n  temperature?: number;\n  /**\n   * Repetition penalty (>= 1.1 required for stable generations). Default value: `1.2`\n   */\n  repetition_penalty?: number;\n};\nexport type OrpheusTtsOutput = {\n  /**\n   * The generated speech audio\n   */\n  audio: File;\n};\nexport type Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type ParabolizeInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Parabolize coefficient Default value: `1`\n   */\n  parabolize_coeff?: number;\n  /**\n   * Vertex X position Default value: `0.5`\n   */\n  vertex_x?: number;\n  /**\n   * Vertex Y position Default value: `0.5`\n   */\n  vertex_y?: number;\n};\nexport type ParabolizeOutput = {\n  /**\n   * The processed images with parabolize effect\n   */\n  images: Array<Image>;\n};\nexport type PasdInput = {\n  /**\n   * Input image to super-resolve\n   */\n  image_url: string | Blob | File;\n  /**\n   * Upscaling factor (1-4x) Default value: `2`\n   */\n  scale?: number;\n  /**\n   * Number of inference steps (10-50) Default value: `25`\n   */\n  steps?: number;\n  /**\n   * Guidance scale for diffusion (1.0-20.0) Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * ControlNet conditioning scale (0.1-1.0) Default value: `0.8`\n   */\n  conditioning_scale?: number;\n  /**\n   * Additional prompt to guide super-resolution Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * Negative prompt to avoid unwanted artifacts Default value: `\"blurry, dirty, messy, frames, deformed, dotted, noise, raster lines, unclear, lowres, over-smoothed, painting, ai generated\"`\n   */\n  negative_prompt?: string;\n};\nexport type PasdOutput = {\n  /**\n   * The generated super-resolved images\n   */\n  images: Array<Image>;\n  /**\n   * Timing information for different processing stages\n   */\n  timings?: any;\n};\nexport type PhotoLoraI2IInput = {\n  /**\n   * LoRA Scale of the photo lora model Default value: `0.75`\n   */\n  photo_lora_scale?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n};\nexport type PhotoLoraInpaintInput = {\n  /**\n   * LoRA Scale of the photo lora model Default value: `0.75`\n   */\n  photo_lora_scale?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * URL of image to use for inpainting. or img2img\n   */\n  image_url: string | Blob | File;\n  /**\n   * The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original. Default value: `0.85`\n   */\n  strength?: number;\n  /**\n   * The mask to area to Inpaint in.\n   */\n  mask_url: string | Blob | File;\n};\nexport type PhotoRestorationInput = {\n  /**\n   * URL of the old or damaged photo to restore.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type PhotoRestorationOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type Pika22ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Pika22TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikadditionsOutput = {\n  /**\n   * The generated video with added objects/images\n   */\n  video: File;\n};\nexport type PikaffectsOutput = {\n  /**\n   * The generated video with applied effect\n   */\n  video: File;\n};\nexport type PikaswapsOutput = {\n  /**\n   * The generated video with swapped regions\n   */\n  video: File;\n};\nexport type PikaV15PikaffectsInput = {\n  /**\n   * URL of the input image\n   */\n  image_url: string | Blob | File;\n  /**\n   * The Pikaffect to apply\n   */\n  pikaffect:\n    | \"Cake-ify\"\n    | \"Crumble\"\n    | \"Crush\"\n    | \"Decapitate\"\n    | \"Deflate\"\n    | \"Dissolve\"\n    | \"Explode\"\n    | \"Eye-pop\"\n    | \"Inflate\"\n    | \"Levitate\"\n    | \"Melt\"\n    | \"Peel\"\n    | \"Poke\"\n    | \"Squish\"\n    | \"Ta-da\"\n    | \"Tear\";\n  /**\n   * Text prompt to guide the effect\n   */\n  prompt?: string;\n  /**\n   * Negative prompt to guide the model\n   */\n  negative_prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n};\nexport type PikaV15PikaffectsOutput = {\n  /**\n   * The generated video with applied effect\n   */\n  video: File;\n};\nexport type PikaV21ImageToVideoInput = {\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV21ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV21TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV21TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV22ImageToVideoInput = {\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV22ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV22PikascenesInput = {\n  /**\n   * List of images to use for video generation\n   */\n  images: Array<PikaImage>;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n  /**\n   * Mode for integrating multiple images Default value: `\"creative\"`\n   */\n  ingredients_mode?: \"creative\" | \"precise\";\n};\nexport type PikaV22PikascenesOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV22TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV22TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV2TurboImageToVideoInput = {\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV2TurboImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PikaV2TurboTextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * A negative prompt to guide the model Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\" | \"4:5\" | \"5:4\" | \"3:2\" | \"2:3\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `5`\n   */\n  duration?: number;\n};\nexport type PikaV2TurboTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseExtendFastInput = {\n  /**\n   * URL of the input video to extend\n   */\n  video_url: string | Blob | File;\n  /**\n   * Prompt describing how to extend the video\n   */\n  prompt: string;\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the extended video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"day\" | \"cyberpunk\" | \"comic\";\n  /**\n   * The resolution of the generated video. Fast mode doesn't support 1080p Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * The model version to use for generation Default value: `\"v4.5\"`\n   */\n  model?: \"v3.5\" | \"v4\" | \"v4.5\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n};\nexport type PixverseExtendFastOutput = {\n  /**\n   * The extended video\n   */\n  video: File;\n};\nexport type PixverseExtendInput = {\n  /**\n   * URL of the input video to extend\n   */\n  video_url: string | Blob | File;\n  /**\n   * Prompt describing how to extend the video\n   */\n  prompt: string;\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the extended video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"day\" | \"cyberpunk\" | \"comic\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * The model version to use for generation Default value: `\"v4.5\"`\n   */\n  model?: \"v3.5\" | \"v4\" | \"v4.5\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n};\nexport type PixverseExtendOutput = {\n  /**\n   * The extended video\n   */\n  video: File;\n};\nexport type PixverseLipsyncInput = {\n  /**\n   * URL of the input video\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the input audio. If not provided, TTS will be used.\n   */\n  audio_url?: string | Blob | File;\n  /**\n   * Voice to use for TTS when audio_url is not provided Default value: `\"Auto\"`\n   */\n  voice_id?:\n    | \"Emily\"\n    | \"James\"\n    | \"Isabella\"\n    | \"Liam\"\n    | \"Chloe\"\n    | \"Adrian\"\n    | \"Harper\"\n    | \"Ava\"\n    | \"Sophia\"\n    | \"Julia\"\n    | \"Mason\"\n    | \"Jack\"\n    | \"Oliver\"\n    | \"Ethan\"\n    | \"Auto\";\n  /**\n   * Text content for TTS when audio_url is not provided\n   */\n  text?: string;\n};\nexport type PixverseLipsyncOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseSoundEffectsInput = {\n  /**\n   * URL of the input video to add sound effects to\n   */\n  video_url: string | Blob | File;\n  /**\n   * Whether to keep the original audio from the video\n   */\n  original_sound_switch?: boolean;\n  /**\n   * Description of the sound effect to generate. If empty, a random sound effect will be generated Default value: `\"\"`\n   */\n  prompt?: string;\n};\nexport type PixverseSoundEffectsOutput = {\n  /**\n   * The video with added sound effects\n   */\n  video: File;\n};\nexport type PixverseV35EffectsInput = {\n  /**\n   * The effect to apply to the video\n   */\n  effect:\n    | \"Kiss Me AI\"\n    | \"Kiss\"\n    | \"Muscle Surge\"\n    | \"Warmth of Jesus\"\n    | \"Anything, Robot\"\n    | \"The Tiger Touch\"\n    | \"Hug\"\n    | \"Holy Wings\"\n    | \"Hulk\"\n    | \"Venom\"\n    | \"Microwave\"\n    | \"Zombie Mode\"\n    | \"Squid Game\"\n    | \"Baby Face\"\n    | \"Black Myth: Wukong\"\n    | \"Long Hair Magic\"\n    | \"Leggy Run\"\n    | \"Fin-tastic Mermaid\"\n    | \"Punch Face\"\n    | \"Creepy Devil Smile\"\n    | \"Thunder God\"\n    | \"Eye Zoom Challenge\"\n    | \"Who's Arrested?\"\n    | \"Baby Arrived\"\n    | \"Werewolf Rage\"\n    | \"Bald Swipe\"\n    | \"BOOM DROP\"\n    | \"Huge Cutie\"\n    | \"Liquid Metal\"\n    | \"Sharksnap!\"\n    | \"Dust Me Away\";\n  /**\n   * Optional URL of the image to use as the first frame. If not provided, generates from text\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type PixverseV35EffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35ImageToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV35ImageToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV35ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35TextToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV35TextToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV35TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV35TransitionInput = {\n  /**\n   * The prompt for the transition\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  first_image_url: string | Blob | File;\n  /**\n   * URL of the image to use as the last frame\n   */\n  last_image_url: string | Blob | File;\n};\nexport type PixverseV35TransitionOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV45EffectsInput = {\n  /**\n   * The effect to apply to the video\n   */\n  effect:\n    | \"Kiss Me AI\"\n    | \"Kiss\"\n    | \"Muscle Surge\"\n    | \"Warmth of Jesus\"\n    | \"Anything, Robot\"\n    | \"The Tiger Touch\"\n    | \"Hug\"\n    | \"Holy Wings\"\n    | \"Hulk\"\n    | \"Venom\"\n    | \"Microwave\"\n    | \"Zombie Mode\"\n    | \"Squid Game\"\n    | \"Baby Face\"\n    | \"Black Myth: Wukong\"\n    | \"Long Hair Magic\"\n    | \"Leggy Run\"\n    | \"Fin-tastic Mermaid\"\n    | \"Punch Face\"\n    | \"Creepy Devil Smile\"\n    | \"Thunder God\"\n    | \"Eye Zoom Challenge\"\n    | \"Who's Arrested?\"\n    | \"Baby Arrived\"\n    | \"Werewolf Rage\"\n    | \"Bald Swipe\"\n    | \"BOOM DROP\"\n    | \"Huge Cutie\"\n    | \"Liquid Metal\"\n    | \"Sharksnap!\"\n    | \"Dust Me Away\";\n  /**\n   * Optional URL of the image to use as the first frame. If not provided, generates from text\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type PixverseV45EffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV45ImageToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV45ImageToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV45ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV45ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV45TextToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV45TextToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV45TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV45TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV45TransitionInput = {\n  /**\n   * The prompt for the transition\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  first_image_url: string | Blob | File;\n  /**\n   * URL of the image to use as the last frame\n   */\n  last_image_url: string | Blob | File;\n};\nexport type PixverseV45TransitionOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV4EffectsInput = {\n  /**\n   * The effect to apply to the video\n   */\n  effect:\n    | \"Kiss Me AI\"\n    | \"Kiss\"\n    | \"Muscle Surge\"\n    | \"Warmth of Jesus\"\n    | \"Anything, Robot\"\n    | \"The Tiger Touch\"\n    | \"Hug\"\n    | \"Holy Wings\"\n    | \"Hulk\"\n    | \"Venom\"\n    | \"Microwave\"\n    | \"Zombie Mode\"\n    | \"Squid Game\"\n    | \"Baby Face\"\n    | \"Black Myth: Wukong\"\n    | \"Long Hair Magic\"\n    | \"Leggy Run\"\n    | \"Fin-tastic Mermaid\"\n    | \"Punch Face\"\n    | \"Creepy Devil Smile\"\n    | \"Thunder God\"\n    | \"Eye Zoom Challenge\"\n    | \"Who's Arrested?\"\n    | \"Baby Arrived\"\n    | \"Werewolf Rage\"\n    | \"Bald Swipe\"\n    | \"BOOM DROP\"\n    | \"Huge Cutie\"\n    | \"Liquid Metal\"\n    | \"Sharksnap!\"\n    | \"Dust Me Away\";\n  /**\n   * Optional URL of the image to use as the first frame. If not provided, generates from text\n   */\n  image_url?: string | Blob | File;\n  /**\n   * The resolution of the generated video. Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type PixverseV4EffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV4ImageToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV4ImageToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV4ImageToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n};\nexport type PixverseV4ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV4TextToVideoFastInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV4TextToVideoFastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PixverseV4TextToVideoInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"360p\" | \"540p\" | \"720p\" | \"1080p\";\n  /**\n   * The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds Default value: `\"5\"`\n   */\n  duration?: \"5\" | \"8\";\n  /**\n   * Negative prompt to be used for the generation Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The style of the generated video\n   */\n  style?: \"anime\" | \"3d_animation\" | \"clay\" | \"comic\" | \"cyberpunk\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n};\nexport type PixverseV4TextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type PlayaiInpaintDiffusionInput = {\n  /**\n   * The URL of the audio file.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Transcription of the input audio.\n   */\n  text: string;\n  /**\n   * Desired audio text. This is the text that will be spoken in the output audio. The model will find the difference between the input and output text and inpaint the audio accordingly.\n   */\n  output_text: string;\n  /**\n   * Word timestamps for the input audio. Each word is represented by an object containing the word text and its start/end timestamps. You can generate these timestamps using an ASR (Automatic Speech Recognition) on https://fal.ai/models/fal-ai/whisper.\n   */\n  chunks: Array<WordTime>;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n};\nexport type PlayaiInpaintDiffusionOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlayaiTtsDialogInput = {\n  /**\n   * The dialogue text with turn prefixes to distinguish speakers.\n   */\n  input: string;\n  /**\n   * A list of voice definitions for each speaker in the dialogue. Must be between 1 and 2 voices.\n   */\n  voices?: Array<LDMVoiceInput>;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type PlayaiTtsDialogOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlayaiTtsV3Input = {\n  /**\n   * The text to be converted to speech.\n   */\n  input: string;\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type PlayaiTtsV3Output = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlayDiffusionInpaintInput = {\n  /**\n   * The URL of the audio file.\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Transcription of the input audio.\n   */\n  text: string;\n  /**\n   * Desired audio text. This is the text that will be spoken in the output audio. The model will find the difference between the input and output text and inpaint the audio accordingly.\n   */\n  output_text: string;\n  /**\n   * Word timestamps for the input audio. Each word is represented by an object containing the word text and its start/end timestamps. You can generate these timestamps using an ASR (Automatic Speech Recognition) on https://fal.ai/models/fal-ai/whisper.\n   */\n  chunks: Array<WordTime>;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n};\nexport type PlayDiffusionInpaintOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlayDiffusionTTSInput = {\n  /**\n   * The text to be converted to speech.\n   */\n  input: string;\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type PlayDiffusionTTSOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type PlushieStyleInput = {\n  /**\n   * URL of the image to convert to plushie style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type PlushieStyleOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type PlushifyInput = {\n  /**\n   * URL of the image to apply cartoon style to\n   */\n  image_url: string | Blob | File;\n  /**\n   * Prompt for the generation. Default is empty which is usually best, but sometimes it can help to add a description of the subject. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * Scale factor for the Cartoon effect Default value: `1`\n   */\n  scale?: number;\n  /**\n   * Guidance scale for the generation Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to use CFG zero\n   */\n  use_cfg_zero?: boolean;\n  /**\n   * The seed for image generation. Same seed with same parameters will generate same image.\n   */\n  seed?: number;\n  /**\n   * Number of images to generate Default value: `1`\n   */\n  num_images?: number;\n};\nexport type PlushifyOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type PoseTransferInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  pose_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  person_image_url: string | Blob | File;\n};\nexport type PoseTransferOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type PostProcessingBlurInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of blur to apply Default value: `\"gaussian\"`\n   */\n  blur_type?: \"gaussian\" | \"kuwahara\";\n  /**\n   * Blur radius Default value: `3`\n   */\n  blur_radius?: number;\n  /**\n   * Sigma for Gaussian blur Default value: `1`\n   */\n  blur_sigma?: number;\n};\nexport type PostProcessingBlurOutput = {\n  /**\n   * The processed images with blur effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingChromaticAberrationInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Red channel shift amount\n   */\n  red_shift?: number;\n  /**\n   * Red channel shift direction Default value: `\"horizontal\"`\n   */\n  red_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Green channel shift amount\n   */\n  green_shift?: number;\n  /**\n   * Green channel shift direction Default value: `\"horizontal\"`\n   */\n  green_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Blue channel shift amount\n   */\n  blue_shift?: number;\n  /**\n   * Blue channel shift direction Default value: `\"horizontal\"`\n   */\n  blue_direction?: \"horizontal\" | \"vertical\";\n};\nexport type PostProcessingChromaticAberrationOutput = {\n  /**\n   * The processed images with chromatic aberration effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingColorCorrectionInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Color temperature adjustment\n   */\n  temperature?: number;\n  /**\n   * Brightness adjustment\n   */\n  brightness?: number;\n  /**\n   * Contrast adjustment\n   */\n  contrast?: number;\n  /**\n   * Saturation adjustment\n   */\n  saturation?: number;\n  /**\n   * Gamma adjustment Default value: `1`\n   */\n  gamma?: number;\n};\nexport type PostProcessingColorCorrectionOutput = {\n  /**\n   * The processed images with color correction\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingColorTintInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Tint strength Default value: `1`\n   */\n  tint_strength?: number;\n  /**\n   * Tint color mode Default value: `\"sepia\"`\n   */\n  tint_mode?:\n    | \"sepia\"\n    | \"red\"\n    | \"green\"\n    | \"blue\"\n    | \"cyan\"\n    | \"magenta\"\n    | \"yellow\"\n    | \"purple\"\n    | \"orange\"\n    | \"warm\"\n    | \"cool\"\n    | \"lime\"\n    | \"navy\"\n    | \"vintage\"\n    | \"rose\"\n    | \"teal\"\n    | \"maroon\"\n    | \"peach\"\n    | \"lavender\"\n    | \"olive\";\n};\nexport type PostProcessingColorTintOutput = {\n  /**\n   * The processed images with color tint effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingDesaturateInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Desaturation factor Default value: `1`\n   */\n  desaturate_factor?: number;\n  /**\n   * Desaturation method Default value: `\"luminance (Rec.709)\"`\n   */\n  desaturate_method?:\n    | \"luminance (Rec.709)\"\n    | \"luminance (Rec.601)\"\n    | \"average\"\n    | \"lightness\";\n};\nexport type PostProcessingDesaturateOutput = {\n  /**\n   * The processed images with desaturation effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingDissolveInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * URL of second image for dissolve\n   */\n  dissolve_image_url: string | Blob | File;\n  /**\n   * Dissolve blend factor Default value: `0.5`\n   */\n  dissolve_factor?: number;\n};\nexport type PostProcessingDissolveOutput = {\n  /**\n   * The processed images with dissolve effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingDodgeBurnInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Dodge and burn intensity Default value: `0.5`\n   */\n  dodge_burn_intensity?: number;\n  /**\n   * Dodge and burn mode Default value: `\"dodge\"`\n   */\n  dodge_burn_mode?:\n    | \"dodge\"\n    | \"burn\"\n    | \"dodge_and_burn\"\n    | \"burn_and_dodge\"\n    | \"color_dodge\"\n    | \"color_burn\"\n    | \"linear_dodge\"\n    | \"linear_burn\";\n};\nexport type PostProcessingDodgeBurnOutput = {\n  /**\n   * The processed images with dodge and burn effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingGrainInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Film grain intensity Default value: `0.4`\n   */\n  grain_intensity?: number;\n  /**\n   * Film grain scale Default value: `10`\n   */\n  grain_scale?: number;\n  /**\n   * Style of film grain to apply Default value: `\"modern\"`\n   */\n  grain_style?:\n    | \"modern\"\n    | \"analog\"\n    | \"kodak\"\n    | \"fuji\"\n    | \"cinematic\"\n    | \"newspaper\";\n};\nexport type PostProcessingGrainOutput = {\n  /**\n   * The processed images with grain effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Enable film grain effect\n   */\n  enable_grain?: boolean;\n  /**\n   * Film grain intensity (when enabled) Default value: `0.4`\n   */\n  grain_intensity?: number;\n  /**\n   * Film grain scale (when enabled) Default value: `10`\n   */\n  grain_scale?: number;\n  /**\n   * Style of film grain to apply Default value: `\"modern\"`\n   */\n  grain_style?:\n    | \"modern\"\n    | \"analog\"\n    | \"kodak\"\n    | \"fuji\"\n    | \"cinematic\"\n    | \"newspaper\";\n  /**\n   * Enable color correction\n   */\n  enable_color_correction?: boolean;\n  /**\n   * Color temperature adjustment\n   */\n  temperature?: number;\n  /**\n   * Brightness adjustment\n   */\n  brightness?: number;\n  /**\n   * Contrast adjustment\n   */\n  contrast?: number;\n  /**\n   * Saturation adjustment\n   */\n  saturation?: number;\n  /**\n   * Gamma adjustment Default value: `1`\n   */\n  gamma?: number;\n  /**\n   * Enable chromatic aberration\n   */\n  enable_chromatic?: boolean;\n  /**\n   * Red channel shift amount\n   */\n  red_shift?: number;\n  /**\n   * Red channel shift direction Default value: `\"horizontal\"`\n   */\n  red_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Green channel shift amount\n   */\n  green_shift?: number;\n  /**\n   * Green channel shift direction Default value: `\"horizontal\"`\n   */\n  green_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Blue channel shift amount\n   */\n  blue_shift?: number;\n  /**\n   * Blue channel shift direction Default value: `\"horizontal\"`\n   */\n  blue_direction?: \"horizontal\" | \"vertical\";\n  /**\n   * Enable blur effect\n   */\n  enable_blur?: boolean;\n  /**\n   * Type of blur to apply Default value: `\"gaussian\"`\n   */\n  blur_type?: \"gaussian\" | \"kuwahara\";\n  /**\n   * Blur radius Default value: `3`\n   */\n  blur_radius?: number;\n  /**\n   * Sigma for Gaussian blur Default value: `1`\n   */\n  blur_sigma?: number;\n  /**\n   * Enable vignette effect\n   */\n  enable_vignette?: boolean;\n  /**\n   * Vignette strength (when enabled) Default value: `0.5`\n   */\n  vignette_strength?: number;\n  /**\n   * Enable parabolize effect\n   */\n  enable_parabolize?: boolean;\n  /**\n   * Parabolize coefficient Default value: `1`\n   */\n  parabolize_coeff?: number;\n  /**\n   * Vertex X position Default value: `0.5`\n   */\n  vertex_x?: number;\n  /**\n   * Vertex Y position Default value: `0.5`\n   */\n  vertex_y?: number;\n  /**\n   * Enable color tint effect\n   */\n  enable_tint?: boolean;\n  /**\n   * Tint strength Default value: `1`\n   */\n  tint_strength?: number;\n  /**\n   * Tint color mode Default value: `\"sepia\"`\n   */\n  tint_mode?:\n    | \"sepia\"\n    | \"red\"\n    | \"green\"\n    | \"blue\"\n    | \"cyan\"\n    | \"magenta\"\n    | \"yellow\"\n    | \"purple\"\n    | \"orange\"\n    | \"warm\"\n    | \"cool\"\n    | \"lime\"\n    | \"navy\"\n    | \"vintage\"\n    | \"rose\"\n    | \"teal\"\n    | \"maroon\"\n    | \"peach\"\n    | \"lavender\"\n    | \"olive\";\n  /**\n   * Enable dissolve effect\n   */\n  enable_dissolve?: boolean;\n  /**\n   * URL of second image for dissolve Default value: `\"\"`\n   */\n  dissolve_image_url?: string | Blob | File;\n  /**\n   * Dissolve blend factor Default value: `0.5`\n   */\n  dissolve_factor?: number;\n  /**\n   * Enable dodge and burn effect\n   */\n  enable_dodge_burn?: boolean;\n  /**\n   * Dodge and burn intensity Default value: `0.5`\n   */\n  dodge_burn_intensity?: number;\n  /**\n   * Dodge and burn mode Default value: `\"dodge\"`\n   */\n  dodge_burn_mode?:\n    | \"dodge\"\n    | \"burn\"\n    | \"dodge_and_burn\"\n    | \"burn_and_dodge\"\n    | \"color_dodge\"\n    | \"color_burn\"\n    | \"linear_dodge\"\n    | \"linear_burn\";\n  /**\n   * Enable glow effect\n   */\n  enable_glow?: boolean;\n  /**\n   * Glow intensity Default value: `1`\n   */\n  glow_intensity?: number;\n  /**\n   * Glow blur radius Default value: `5`\n   */\n  glow_radius?: number;\n  /**\n   * Enable sharpen effect\n   */\n  enable_sharpen?: boolean;\n  /**\n   * Type of sharpening to apply Default value: `\"basic\"`\n   */\n  sharpen_mode?: \"basic\" | \"smart\" | \"cas\";\n  /**\n   * Sharpen radius (for basic mode) Default value: `1`\n   */\n  sharpen_radius?: number;\n  /**\n   * Sharpen strength (for basic mode) Default value: `1`\n   */\n  sharpen_alpha?: number;\n  /**\n   * Noise radius for smart sharpen Default value: `7`\n   */\n  noise_radius?: number;\n  /**\n   * Edge preservation factor Default value: `0.75`\n   */\n  preserve_edges?: number;\n  /**\n   * Smart sharpen strength Default value: `5`\n   */\n  smart_sharpen_strength?: number;\n  /**\n   * Smart sharpen blend ratio Default value: `0.5`\n   */\n  smart_sharpen_ratio?: number;\n  /**\n   * CAS sharpening amount Default value: `0.8`\n   */\n  cas_amount?: number;\n  /**\n   * Enable solarize effect\n   */\n  enable_solarize?: boolean;\n  /**\n   * Solarize threshold Default value: `0.5`\n   */\n  solarize_threshold?: number;\n  /**\n   * Enable desaturation effect\n   */\n  enable_desaturate?: boolean;\n  /**\n   * Desaturation factor Default value: `1`\n   */\n  desaturate_factor?: number;\n  /**\n   * Desaturation method Default value: `\"luminance (Rec.709)\"`\n   */\n  desaturate_method?:\n    | \"luminance (Rec.709)\"\n    | \"luminance (Rec.601)\"\n    | \"average\"\n    | \"lightness\";\n};\nexport type PostProcessingOutput = {\n  /**\n   * The processed images\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingParabolizeInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Parabolize coefficient Default value: `1`\n   */\n  parabolize_coeff?: number;\n  /**\n   * Vertex X position Default value: `0.5`\n   */\n  vertex_x?: number;\n  /**\n   * Vertex Y position Default value: `0.5`\n   */\n  vertex_y?: number;\n};\nexport type PostProcessingParabolizeOutput = {\n  /**\n   * The processed images with parabolize effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingSharpenInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of sharpening to apply Default value: `\"basic\"`\n   */\n  sharpen_mode?: \"basic\" | \"smart\" | \"cas\";\n  /**\n   * Sharpen radius (for basic mode) Default value: `1`\n   */\n  sharpen_radius?: number;\n  /**\n   * Sharpen strength (for basic mode) Default value: `1`\n   */\n  sharpen_alpha?: number;\n  /**\n   * Noise radius for smart sharpen Default value: `7`\n   */\n  noise_radius?: number;\n  /**\n   * Edge preservation factor Default value: `0.75`\n   */\n  preserve_edges?: number;\n  /**\n   * Smart sharpen strength Default value: `5`\n   */\n  smart_sharpen_strength?: number;\n  /**\n   * Smart sharpen blend ratio Default value: `0.5`\n   */\n  smart_sharpen_ratio?: number;\n  /**\n   * CAS sharpening amount Default value: `0.8`\n   */\n  cas_amount?: number;\n};\nexport type PostProcessingSharpenOutput = {\n  /**\n   * The processed images with sharpen effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingSolarizeInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Solarize threshold Default value: `0.5`\n   */\n  solarize_threshold?: number;\n};\nexport type PostProcessingSolarizeOutput = {\n  /**\n   * The processed images with solarize effect\n   */\n  images: Array<Image>;\n};\nexport type PostProcessingVignetteInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Vignette strength Default value: `0.5`\n   */\n  vignette_strength?: number;\n};\nexport type PostProcessingVignetteOutput = {\n  /**\n   * The processed images with vignette effect\n   */\n  images: Array<Image>;\n};\nexport type ProcessedOutput = {\n  /**\n   * The processed images\n   */\n  images: Array<Image>;\n};\nexport type ProductShotInput = {\n  /**\n   * The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters.\n   */\n  scene_description?: string;\n  /**\n   * The URL of the reference image to be used for generating the new scene or background for the product shot. Use \"\" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  ref_image_url?: string | Blob | File;\n  /**\n   * Whether to optimize the scene description Default value: `true`\n   */\n  optimize_description?: boolean;\n  /**\n   * The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product. Default value: `\"manual_placement\"`\n   */\n  placement_type?:\n    | \"original\"\n    | \"automatic\"\n    | \"manual_placement\"\n    | \"manual_padding\";\n  /**\n   * This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio.\n   */\n  original_quality?: boolean;\n  /**\n   * The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement.\n   */\n  shot_size?: Array<number>;\n  /**\n   * If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request. Default value: `\"bottom_center\"`\n   */\n  manual_placement_selection?:\n    | \"upper_left\"\n    | \"upper_right\"\n    | \"bottom_left\"\n    | \"bottom_right\"\n    | \"right_center\"\n    | \"left_center\"\n    | \"upper_center\"\n    | \"bottom_center\"\n    | \"center_vertical\"\n    | \"center_horizontal\";\n  /**\n   * The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API.\n   */\n  padding_values?: Array<number>;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ProductShotOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n};\nexport type ProfessionalPhotoOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type ProImageToVideoHailuo02Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type PromptInput = {\n  /**\n   * The URL of the image to remove objects from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Text description of the object to remove.\n   */\n  prompt: string;\n  /**\n   *  Default value: `\"best_quality\"`\n   */\n  model?: \"low_quality\" | \"medium_quality\" | \"high_quality\" | \"best_quality\";\n  /**\n   * Amount of pixels to expand the mask by. Range: 0-50 Default value: `15`\n   */\n  mask_expansion?: number;\n};\nexport type ProTextToVideoHailuo02Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type Q1ImageToVideoOutput = {\n  /**\n   * The generated video using the Q1 model from a single image\n   */\n  video: File;\n};\nexport type Q1ReferenceToVideoOutput = {\n  /**\n   * The generated video with consistent subjects from reference images using the Q1 model\n   */\n  video: File;\n};\nexport type Q1StartEndToVideoOutput = {\n  /**\n   * The generated transition video between start and end frames using the Q1 model\n   */\n  video: File;\n};\nexport type Q1TextToVideoOutput = {\n  /**\n   * The generated video using the Q1 model\n   */\n  video: File;\n};\nexport type QueryInput = {\n  /**\n   * Image URL to be processed\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of task to perform Default value: `\"caption\"`\n   */\n  task_type?: \"caption\" | \"query\";\n  /**\n   * Prompt for query task\n   */\n  prompt: string;\n  /**\n   * Maximum number of tokens to generate Default value: `64`\n   */\n  max_tokens?: number;\n};\nexport type QwenImageInput = {\n  /**\n   * The prompt to generate the image with\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The negative prompt for the generation Default value: `\" \"`\n   */\n  negative_prompt?: string;\n  /**\n   * Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type QwenImageOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Ray2I2VOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type Ray2T2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type RealismInput = {\n  /**\n   * URL of the image to enhance with realism details.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.6`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type RealismOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type Recraft20bInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *  Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The style of the generated images. Vector images cost 2X as much. Default value: `\"realistic_image\"`\n   */\n  style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/3d\"\n    | \"digital_illustration/80s\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/glow\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/kawaii\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/psychedelic\"\n    | \"digital_illustration/seamless\"\n    | \"digital_illustration/voxel\"\n    | \"digital_illustration/watercolor\"\n    | \"vector_illustration/cartoon\"\n    | \"vector_illustration/doodle_line_art\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/flat_2\"\n    | \"vector_illustration/kawaii\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\"\n    | \"vector_illustration/seamless\"\n    | \"icon/broken_line\"\n    | \"icon/colored_outline\"\n    | \"icon/colored_shapes\"\n    | \"icon/colored_shapes_gradient\"\n    | \"icon/doodle_fill\"\n    | \"icon/doodle_offset_fill\"\n    | \"icon/offset_fill\"\n    | \"icon/outline\"\n    | \"icon/outline_gradient\"\n    | \"icon/uneven_fill\";\n  /**\n   * An array of preferable colors\n   */\n  colors?: Array<RGBColor>;\n  /**\n   * The ID of the custom style reference (optional)\n   */\n  style_id?: string;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type Recraft20bOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type Recraft20BTextToImageInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *  Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The style of the generated images. Vector images cost 2X as much. Default value: `\"realistic_image\"`\n   */\n  style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/3d\"\n    | \"digital_illustration/80s\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/glow\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/kawaii\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/psychedelic\"\n    | \"digital_illustration/seamless\"\n    | \"digital_illustration/voxel\"\n    | \"digital_illustration/watercolor\"\n    | \"vector_illustration/cartoon\"\n    | \"vector_illustration/doodle_line_art\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/flat_2\"\n    | \"vector_illustration/kawaii\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\"\n    | \"vector_illustration/seamless\"\n    | \"icon/broken_line\"\n    | \"icon/colored_outline\"\n    | \"icon/colored_shapes\"\n    | \"icon/colored_shapes_gradient\"\n    | \"icon/doodle_fill\"\n    | \"icon/doodle_offset_fill\"\n    | \"icon/offset_fill\"\n    | \"icon/outline\"\n    | \"icon/outline_gradient\"\n    | \"icon/uneven_fill\";\n  /**\n   * An array of preferable colors\n   */\n  colors?: Array<RGBColor>;\n  /**\n   * The ID of the custom style reference (optional)\n   */\n  style_id?: string;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type RecraftUpscaleCreativeInput = {\n  /**\n   * The URL of the image to be upscaled. Must be in PNG format.\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type RecraftUpscaleCreativeOutput = {\n  /**\n   * The upscaled image.\n   */\n  image: File;\n};\nexport type RecraftUpscaleCrispInput = {\n  /**\n   * The URL of the image to be upscaled. Must be in PNG format.\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type RecraftUpscaleCrispOutput = {\n  /**\n   * The upscaled image.\n   */\n  image: File;\n};\nexport type RecraftV3CreateStyleInput = {\n  /**\n   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * The base style of the generated images, this topic is covered above. Default value: `\"digital_illustration\"`\n   */\n  base_style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n};\nexport type RecraftV3CreateStyleOutput = {\n  /**\n   * The ID of the created style, this ID can be used to reference the style in the future.\n   */\n  style_id: string;\n};\nexport type RecraftV3ImageToImageInput = {\n  /**\n   * A text description of areas to change.\n   */\n  prompt: string;\n  /**\n   * The URL of the image to modify. Must be less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity Default value: `0.5`\n   */\n  strength?: number;\n  /**\n   * The style of the generated images. Vector images cost 2X as much. Default value: `\"realistic_image\"`\n   */\n  style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n  /**\n   * An array of preferable colors\n   */\n  colors?: Array<RGBColor>;\n  /**\n   * The ID of the custom style reference (optional)\n   */\n  style_id?: string;\n  /**\n   * A text description of undesired elements on an image\n   */\n  negative_prompt?: string;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type RecraftV3ImageToImageOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<File>;\n};\nexport type RecraftV3TextToImageInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *  Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The style of the generated images. Vector images cost 2X as much. Default value: `\"realistic_image\"`\n   */\n  style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n  /**\n   * An array of preferable colors\n   */\n  colors?: Array<RGBColor>;\n  /**\n   * The ID of the custom style reference (optional)\n   */\n  style_id?: string;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n};\nexport type RecraftV3TextToImageOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type RecraftVectorizeInput = {\n  /**\n   * The URL of the image to be vectorized. Must be in PNG, JPG or WEBP format, less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels, min dimension more than 256 pixels.\n   */\n  image_url: string | Blob | File;\n};\nexport type RecraftVectorizeOutput = {\n  /**\n   * The vectorized image.\n   */\n  image: File;\n};\nexport type ReferenceToVideoOutput = {\n  /**\n   * The generated video with consistent subjects from reference images\n   */\n  video: File;\n};\nexport type ReframeInput = {\n  /**\n   * URL of the old or damaged photo to restore.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The desired aspect ratio for the reframed image. Default value: `\"16:9\"`\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ReframeOutput = {\n  /**\n   * URL of the reframed video\n   */\n  video: File;\n};\nexport type ReimagineInput = {\n  /**\n   * The prompt you would like to use to generate images.\n   */\n  prompt: string;\n  /**\n   * The URL of the structure reference image. Use \"\" to leave empty. Accepted formats are jpeg, jpg, png, webp. Default value: `\"\"`\n   */\n  structure_image_url?: string | Blob | File;\n  /**\n   * The influence of the structure reference on the generated image. Default value: `0.75`\n   */\n  structure_ref_influence?: number;\n  /**\n   * How many images you would like to generate. When using any Guidance Method, Value is set to 1. Default value: `1`\n   */\n  num_results?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * Whether to use the fast model Default value: `true`\n   */\n  fast?: boolean;\n  /**\n   * The number of iterations the model goes through to refine the generated image. This parameter is optional. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type ReimagineOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed value used for generation.\n   */\n  seed: number;\n};\nexport type RembgEnhanceInput = {\n  /**\n   * URL of the input image\n   */\n  image_url: string | Blob | File;\n};\nexport type RembgEnhanceOutput = {\n  /**\n   * The segmented output image\n   */\n  image: File;\n};\nexport type RemixImageInput = {\n  /**\n   * The prompt to remix the image with\n   */\n  prompt: string;\n  /**\n   * The image URL to remix\n   */\n  image_url: string | Blob | File;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Strength of the input image in the remix Default value: `0.8`\n   */\n  strength?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type RetouchInput = {\n  /**\n   * URL of the image to retouch.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type RetouchOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type RFInversionInput = {\n  /**\n   * The prompt to edit the image with\n   */\n  prompt: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  control_loras?: Array<ControlLoraWeight>;\n  /**\n   * The controlnets to use for the image generation. Only one controlnet is supported at the moment.\n   */\n  controlnets?: Array<ControlNet>;\n  /**\n   * The controlnet unions to use for the image generation. Only one controlnet is supported at the moment.\n   */\n  controlnet_unions?: Array<ControlNetUnion>;\n  /**\n   * EasyControl Inputs to use for image generation.\n   */\n  easycontrols?: Array<EasyControlWeight>;\n  /**\n   * Use an image input to influence the generation. Can be used to fill images in masked areas.\n   */\n  fill_image?: ImageFillInput;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886.\n   */\n  use_cfg_zero?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. This is always set to 1 for streaming output. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * URL of Image for Reference-Only\n   */\n  reference_image_url?: string | Blob | File;\n  /**\n   * Strength of reference_only generation. Only used if a reference image is provided. Default value: `0.65`\n   */\n  reference_strength?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to bestarted.\n   */\n  reference_start?: number;\n  /**\n   * The percentage of the total timesteps when the reference guidance is to be ended. Default value: `1`\n   */\n  reference_end?: number;\n  /**\n   * Base shift for the scheduled timesteps Default value: `0.5`\n   */\n  base_shift?: number;\n  /**\n   * Max shift for the scheduled timesteps Default value: `1.15`\n   */\n  max_shift?: number;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Specifies whether beta sigmas ought to be used.\n   */\n  use_beta_schedule?: boolean;\n  /**\n   * Sigmas schedule for the denoising process.\n   */\n  sigma_schedule?: \"sgm_uniform\";\n  /**\n   * Scheduler for the denoising process. Default value: `\"euler\"`\n   */\n  scheduler?: \"euler\" | \"dpmpp_2m\";\n  /**\n   * Negative prompt to steer the image generation away from unwanted features.\n   * By default, we will be using NAG for processing the negative prompt. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The scale for NAG. Higher values will result in a image that is more distant\n   * to the negative prompt. Default value: `3`\n   */\n  nag_scale?: number;\n  /**\n   * The tau for NAG. Controls the normalization of the hidden state.\n   * Higher values will result in a less aggressive normalization,\n   * but may also lead to unexpected changes with respect to the original image.\n   * Not recommended to change this value. Default value: `2.5`\n   */\n  nag_tau?: number;\n  /**\n   * The alpha value for NAG. This value is used as a final weighting\n   * factor for steering the normalized guidance (positive and negative prompts)\n   * in the direction of the positive prompt. Higher values will result in less\n   * steering on the normalized guidance where lower values will result in\n   * considering the positive prompt guidance more. Default value: `0.25`\n   */\n  nag_alpha?: number;\n  /**\n   * The proportion of steps to apply NAG. After the specified proportion\n   * of steps has been iterated, the remaining steps will use original\n   * attention processors in FLUX. Default value: `0.25`\n   */\n  nag_end?: number;\n  /**\n   * URL of image to be edited\n   */\n  image_url: string | Blob | File;\n  /**\n   * The controller guidance (gamma) used in the creation of structured noise. Default value: `0.6`\n   */\n  controller_guidance_forward?: number;\n  /**\n   * The controller guidance (eta) used in the denoising process.Using values closer to 1 will result in an image closer to input. Default value: `0.75`\n   */\n  controller_guidance_reverse?: number;\n  /**\n   * Timestep to start guidance during reverse process.\n   */\n  reverse_guidance_start?: number;\n  /**\n   * Timestep to stop guidance during reverse process. Default value: `8`\n   */\n  reverse_guidance_end?: number;\n  /**\n   * Scheduler for applying reverse guidance. Default value: `\"constant\"`\n   */\n  reverse_guidance_schedule?:\n    | \"constant\"\n    | \"linear_increase\"\n    | \"linear_decrease\";\n};\nexport type RIFEImageInput = {\n  /**\n   * The URL of the first image to use as the starting point for interpolation.\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * The URL of the second image to use as the ending point for interpolation.\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * The type of output to generate; either individual images or a video. Default value: `\"images\"`\n   */\n  output_type?: \"images\" | \"video\";\n  /**\n   * The format of the output images. Only applicable if output_type is 'images'. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n  /**\n   * The number of frames to generate between the input images. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * Whether to include the start image in the output.\n   */\n  include_start?: boolean;\n  /**\n   * Whether to include the end image in the output.\n   */\n  include_end?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the function will wait for images to be generated and uploaded before returning. This will increase the response time but ensures that the images are ready for use immediately without going through the CDN. Does not apply if output_type is 'video'.\n   */\n  sync_mode?: boolean;\n};\nexport type RIFEImageOutput = {\n  /**\n   * The generated frames as individual images.\n   */\n  images?: Array<Image>;\n  /**\n   * The generated video file, if output_type is 'video'.\n   */\n  video?: File;\n};\nexport type RifeInput = {\n  /**\n   * The URL of the first image to use as the starting point for interpolation.\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * The URL of the second image to use as the ending point for interpolation.\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * The type of output to generate; either individual images or a video. Default value: `\"images\"`\n   */\n  output_type?: \"images\" | \"video\";\n  /**\n   * The format of the output images. Only applicable if output_type is 'images'. Default value: `\"jpeg\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n  /**\n   * The number of frames to generate between the input images. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * Whether to include the start image in the output.\n   */\n  include_start?: boolean;\n  /**\n   * Whether to include the end image in the output.\n   */\n  include_end?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if output_type is 'video'. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the function will wait for images to be generated and uploaded before returning. This will increase the response time but ensures that the images are ready for use immediately without going through the CDN. Does not apply if output_type is 'video'.\n   */\n  sync_mode?: boolean;\n};\nexport type RifeOutput = {\n  /**\n   * The generated frames as individual images.\n   */\n  images?: Array<Image>;\n  /**\n   * The generated video file, if output_type is 'video'.\n   */\n  video?: File;\n};\nexport type RifeVideoInput = {\n  /**\n   * The URL of the video to use for interpolation.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The number of frames to generate between the input video frames. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.\n   */\n  use_scene_detection?: boolean;\n  /**\n   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`\n   */\n  use_calculated_fps?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.\n   */\n  loop?: boolean;\n};\nexport type RIFEVideoInput = {\n  /**\n   * The URL of the video to use for interpolation.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The number of frames to generate between the input video frames. Default value: `1`\n   */\n  num_frames?: number;\n  /**\n   * If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.\n   */\n  use_scene_detection?: boolean;\n  /**\n   * If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used. Default value: `true`\n   */\n  use_calculated_fps?: boolean;\n  /**\n   * Frames per second for the output video. Only applicable if use_calculated_fps is False. Default value: `8`\n   */\n  fps?: number;\n  /**\n   * If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.\n   */\n  loop?: boolean;\n};\nexport type RifeVideoOutput = {\n  /**\n   * The generated video file with interpolated frames.\n   */\n  video: File;\n};\nexport type RIFEVideoOutput = {\n  /**\n   * The generated video file with interpolated frames.\n   */\n  video: File;\n};\nexport type RundiffusionPhotoFluxInput = {\n  /**\n   * LoRA Scale of the photo lora model Default value: `0.75`\n   */\n  photo_lora_scale?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type RundiffusionPhotoFluxOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type Sa2va4bImageInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * Url for the Input image.\n   */\n  image_url: string | Blob | File;\n};\nexport type Sa2va4bImageOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type Sa2va4bVideoInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * The URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to sample from the video. If not provided, all frames are sampled.\n   */\n  num_frames_to_sample?: number;\n};\nexport type Sa2va4bVideoOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type Sa2va8bImageInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * Url for the Input image.\n   */\n  image_url: string | Blob | File;\n};\nexport type Sa2va8bImageOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type Sa2va8bVideoInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * The URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to sample from the video. If not provided, all frames are sampled.\n   */\n  num_frames_to_sample?: number;\n};\nexport type Sa2va8bVideoOutput = {\n  /**\n   * Generated output\n   */\n  output: string;\n  /**\n   * Dictionary of label: mask image\n   */\n  masks: Array<Image>;\n};\nexport type SadTalkerInput = {\n  /**\n   * URL of the source image\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * URL of the driven audio\n   */\n  driven_audio_url: string | Blob | File;\n  /**\n   * The style of the pose\n   */\n  pose_style?: number;\n  /**\n   * The resolution of the face model Default value: `\"256\"`\n   */\n  face_model_resolution?: \"256\" | \"512\";\n  /**\n   * The scale of the expression Default value: `1`\n   */\n  expression_scale?: number;\n  /**\n   * The type of face enhancer to use\n   */\n  face_enhancer?: \"gfpgan\";\n  /**\n   * Whether to use still mode. Fewer head motion, works with preprocess `full`.\n   */\n  still_mode?: boolean;\n  /**\n   * The type of preprocessing to use Default value: `\"crop\"`\n   */\n  preprocess?: \"crop\" | \"extcrop\" | \"resize\" | \"full\" | \"extfull\";\n};\nexport type SadtalkerReferenceInput = {\n  /**\n   * URL of the source image\n   */\n  source_image_url: string | Blob | File;\n  /**\n   * URL of the driven audio\n   */\n  driven_audio_url: string | Blob | File;\n  /**\n   * URL of the reference video\n   */\n  reference_pose_video_url: string | Blob | File;\n  /**\n   * The style of the pose\n   */\n  pose_style?: number;\n  /**\n   * The resolution of the face model Default value: `\"256\"`\n   */\n  face_model_resolution?: \"256\" | \"512\";\n  /**\n   * The scale of the expression Default value: `1`\n   */\n  expression_scale?: number;\n  /**\n   * The type of face enhancer to use\n   */\n  face_enhancer?: \"gfpgan\";\n  /**\n   * Whether to use still mode. Fewer head motion, works with preprocess `full`.\n   */\n  still_mode?: boolean;\n  /**\n   * The type of preprocessing to use Default value: `\"crop\"`\n   */\n  preprocess?: \"crop\" | \"extcrop\" | \"resize\" | \"full\" | \"extfull\";\n};\nexport type SadtalkerReferenceOutput = {\n  /**\n   * URL of the generated video\n   */\n  video: File;\n};\nexport type Sam2AutoSegmentInput = {\n  /**\n   * URL of the image to be automatically segmented\n   */\n  image_url: string | Blob | File;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Number of points to sample along each side of the image. Default value: `32`\n   */\n  points_per_side?: number;\n  /**\n   * Threshold for predicted IOU score. Default value: `0.88`\n   */\n  pred_iou_thresh?: number;\n  /**\n   * Threshold for stability score. Default value: `0.95`\n   */\n  stability_score_thresh?: number;\n  /**\n   * Minimum area of a mask region. Default value: `100`\n   */\n  min_mask_region_area?: number;\n};\nexport type Sam2AutoSegmentOutput = {\n  /**\n   * Combined segmentation mask.\n   */\n  combined_mask: Image;\n  /**\n   * Individual segmentation masks.\n   */\n  individual_masks: Array<Image>;\n};\nexport type SAM2ImageInput = {\n  /**\n   * URL of the image to be segmented\n   */\n  image_url: string | Blob | File;\n  /**\n   * List of prompts to segment the image\n   */\n  prompts?: Array<PointPrompt>;\n  /**\n   * Coordinates for boxes\n   */\n  box_prompts?: Array<BoxPrompt>;\n  /**\n   * Apply the mask on the image.\n   */\n  apply_mask?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type SAM2ImageOutput = {\n  /**\n   * Segmented image.\n   */\n  image: Image;\n};\nexport type SAM2RLEFileOutput = {\n  /**\n   * Run Length Encoding of the mask.\n   */\n  rle: File;\n};\nexport type SAM2RLEOutput = {\n  /**\n   * Run Length Encoding of the mask.\n   */\n  rle: string | Array<string>;\n};\nexport type SAM2VideoOutput = {\n  /**\n   * The segmented video.\n   */\n  video: File;\n};\nexport type SAM2VideoRLEInput = {\n  /**\n   * The URL of the video to be segmented.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The URL of the mask to be applied initially.\n   */\n  mask_url?: string | Blob | File;\n  /**\n   * List of prompts to segment the video\n   */\n  prompts?: Array<PointPrompt>;\n  /**\n   * Coordinates for boxes\n   */\n  box_prompts?: Array<BoxPrompt>;\n  /**\n   * Apply the mask on the video.\n   */\n  apply_mask?: boolean;\n};\nexport type SanaSprintInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The style to generate the image in. Default value: `\"(No style)\"`\n   */\n  style_name?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Photographic\"\n    | \"Anime\"\n    | \"Manga\"\n    | \"Digital Art\"\n    | \"Pixel art\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"3D Model\";\n};\nexport type SanaSprintOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SanaV1516bInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `18`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The style to generate the image in. Default value: `\"(No style)\"`\n   */\n  style_name?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Photographic\"\n    | \"Anime\"\n    | \"Manga\"\n    | \"Digital Art\"\n    | \"Pixel art\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"3D Model\";\n};\nexport type SanaV1516bOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SanaV1548bInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `18`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The style to generate the image in. Default value: `\"(No style)\"`\n   */\n  style_name?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Photographic\"\n    | \"Anime\"\n    | \"Manga\"\n    | \"Digital Art\"\n    | \"Pixel art\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"3D Model\";\n};\nexport type SanaV1548bOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SceneCompositionInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Describe the scene where you want to place the subject. Default value: `\"enchanted forest\"`\n   */\n  prompt?: string;\n};\nexport type SceneCompositionOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type SchnellFlux1ReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type SchnellFlux1TextToImageInput = {\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type SchnellReduxInput = {\n  /**\n   * The URL of the image to generate an image from.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type SchnellTextToImageInput = {\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type SeedanceImageToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n  /**\n   * The URL of the image used to generate video\n   */\n  image_url: string | Blob | File;\n  /**\n   * The URL of the image the video ends with. Defaults to None.\n   */\n  end_image_url?: string | Blob | File;\n};\nexport type SeedanceProI2VVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type SeedanceProImageToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `\"1080p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n  /**\n   * The URL of the image used to generate video\n   */\n  image_url: string | Blob | File;\n};\nexport type SeedanceProT2VVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type SeedanceProTextToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"21:9\" | \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\";\n  /**\n   * Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality Default value: `\"1080p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n};\nexport type SeedanceTextToVideoInput = {\n  /**\n   * The text prompt used to generate the video\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"4:3\" | \"1:1\" | \"3:4\" | \"9:16\" | \"9:21\";\n  /**\n   * Video resolution - 480p for faster generation, 720p for higher quality Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\" | \"1080p\";\n  /**\n   * Duration of the video in seconds Default value: `\"5\"`\n   */\n  duration?: \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" | \"10\" | \"11\" | \"12\";\n  /**\n   * Whether to fix the camera position\n   */\n  camera_fixed?: boolean;\n  /**\n   * Random seed to control video generation. Use -1 for random.\n   */\n  seed?: number;\n};\nexport type SeedanceVideoOutput = {\n  /**\n   * Generated video file\n   */\n  video: File;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type SeedDreamInput = {\n  /**\n   * The text prompt used to generate the image\n   */\n  prompt: string;\n  /**\n   * Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of images to generate Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed to control the stochasticity of image generation.\n   */\n  seed?: number;\n};\nexport type SeedDreamOutput = {\n  /**\n   * Generated images\n   */\n  images: Array<Image>;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type SeedEditInput = {\n  /**\n   * The text prompt used to edit the image\n   */\n  prompt: string;\n  /**\n   * URL of the image to be edited.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation. Default value: `0.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Random seed to control the stochasticity of image generation.\n   */\n  seed?: number;\n};\nexport type SeedEditOutput = {\n  /**\n   * Generated image\n   */\n  image: Image;\n  /**\n   * Seed used for generation\n   */\n  seed: number;\n};\nexport type SharpenInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Type of sharpening to apply Default value: `\"basic\"`\n   */\n  sharpen_mode?: \"basic\" | \"smart\" | \"cas\";\n  /**\n   * Sharpen radius (for basic mode) Default value: `1`\n   */\n  sharpen_radius?: number;\n  /**\n   * Sharpen strength (for basic mode) Default value: `1`\n   */\n  sharpen_alpha?: number;\n  /**\n   * Noise radius for smart sharpen Default value: `7`\n   */\n  noise_radius?: number;\n  /**\n   * Edge preservation factor Default value: `0.75`\n   */\n  preserve_edges?: number;\n  /**\n   * Smart sharpen strength Default value: `5`\n   */\n  smart_sharpen_strength?: number;\n  /**\n   * Smart sharpen blend ratio Default value: `0.5`\n   */\n  smart_sharpen_ratio?: number;\n  /**\n   * CAS sharpening amount Default value: `0.8`\n   */\n  cas_amount?: number;\n};\nexport type SharpenOutput = {\n  /**\n   * The processed images with sharpen effect\n   */\n  images: Array<Image>;\n};\nexport type SkyRaccoonInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * If true, the video will be generated faster with no noticeable degradation in the visual quality.\n   */\n  turbo_mode?: boolean;\n};\nexport type SkyRaccoonOutput = {\n  /**\n   * The generated image file.\n   */\n  image: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type SkyreelsI2vInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * URL of the image input.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for generation. If not provided, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * Guidance scale for generation (between 1.0 and 20.0) Default value: `6`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of denoising steps (between 1 and 50). Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Negative prompt to guide generation away from certain attributes.\n   */\n  negative_prompt?: string;\n  /**\n   * Aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n};\nexport type SkyreelsI2vOutput = {\n  /**\n   *\n   */\n  video: File;\n  /**\n   * The seed used for generation\n   */\n  seed: number;\n};\nexport type SmartTurnInput = {\n  /**\n   * The URL of the audio file to be processed.\n   */\n  audio_url: string | Blob | File;\n};\nexport type SmartTurnOutput = {\n  /**\n   * The predicted turn type. 1 for Complete, 0 for Incomplete.\n   */\n  prediction: number;\n  /**\n   * The probability of the predicted turn type.\n   */\n  probability: number;\n  /**\n   * The metrics of the inference.\n   */\n  metrics: any;\n};\nexport type SolarizeInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Solarize threshold Default value: `0.5`\n   */\n  solarize_threshold?: number;\n};\nexport type SolarizeOutput = {\n  /**\n   * The processed images with solarize effect\n   */\n  images: Array<Image>;\n};\nexport type SoundEffectOutput = {\n  /**\n   * The video with added sound effects\n   */\n  video: File;\n};\nexport type SoundEffectsGeneratorInput = {\n  /**\n   * The prompt to generate SFX.\n   */\n  prompt: string;\n  /**\n   * The duration of the generated SFX in seconds.\n   */\n  duration: number;\n};\nexport type SoundEffectsGeneratorOutput = {\n  /**\n   * The generated SFX\n   */\n  audio_file: File;\n};\nexport type SpanishOutput = {\n  /**\n   * The generated music\n   */\n  audio: File;\n};\nexport type SpeechOutput = {\n  /**\n   * The partial or final transcription output from Canary\n   */\n  output: string;\n  /**\n   * Indicates if this is a partial (in-progress) transcript\n   */\n  partial?: boolean;\n};\nexport type SpeechToTextInput = {\n  /**\n   * Local filesystem path (or remote URL) to a long audio file\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`\n   */\n  use_pnc?: boolean;\n};\nexport type SpeechToTextOutput = {\n  /**\n   * The partial or final transcription output from Canary\n   */\n  output: string;\n  /**\n   * Indicates if this is a partial (in-progress) transcript\n   */\n  partial?: boolean;\n};\nexport type SpeechToTextStreamInput = {\n  /**\n   * Local filesystem path (or remote URL) to a long audio file\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`\n   */\n  use_pnc?: boolean;\n};\nexport type SpeechToTextTurboInput = {\n  /**\n   * Local filesystem path (or remote URL) to a long audio file\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`\n   */\n  use_pnc?: boolean;\n};\nexport type SpeechToTextTurboOutput = {\n  /**\n   * The partial or final transcription output from Canary\n   */\n  output: string;\n  /**\n   * Indicates if this is a partial (in-progress) transcript\n   */\n  partial?: boolean;\n};\nexport type SpeechToTextTurboStreamInput = {\n  /**\n   * Local filesystem path (or remote URL) to a long audio file\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Whether to use Canary's built-in punctuation & capitalization Default value: `true`\n   */\n  use_pnc?: boolean;\n};\nexport type SprintInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `2`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The style to generate the image in. Default value: `\"(No style)\"`\n   */\n  style_name?:\n    | \"(No style)\"\n    | \"Cinematic\"\n    | \"Photographic\"\n    | \"Anime\"\n    | \"Manga\"\n    | \"Digital Art\"\n    | \"Pixel art\"\n    | \"Fantasy art\"\n    | \"Neonpunk\"\n    | \"3D Model\";\n};\nexport type StableDiffusionV35LargeInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * ControlNet for inference.\n   */\n  controlnet?: ControlNet;\n  /**\n   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * IP-Adapter to use during inference.\n   */\n  ip_adapter?: IPAdapter;\n};\nexport type StableDiffusionV35LargeOutput = {\n  /**\n   * The generated image files info.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StandardImageToVideoHailuo02Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   *\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `\"6\"`\n   */\n  duration?: \"6\" | \"10\";\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n  /**\n   * The resolution of the generated video. Default value: `\"768P\"`\n   */\n  resolution?: \"512P\" | \"768P\";\n};\nexport type StandardTextToVideoHailuo02Input = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution. Default value: `\"6\"`\n   */\n  duration?: \"6\" | \"10\";\n  /**\n   * Whether to use the model's prompt optimizer Default value: `true`\n   */\n  prompt_optimizer?: boolean;\n};\nexport type StartEndToVideoOutput = {\n  /**\n   * The generated transition video between start and end frames\n   */\n  video: File;\n};\nexport type StarVectorInput = {\n  /**\n   * URL of image to be used for relighting\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n};\nexport type StarVectorOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: File;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type Step1xEditInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The image URL to generate an image from. Needs to match the dimensions of the mask.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type Step1xEditOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type StreamingDevTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type StreamingFastTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `16`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type StreamingFlux1Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type StreamingFullTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The size of the generated image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name.\n   */\n  loras?: Array<LoraWeight>;\n};\nexport type StreamingInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type StreamingKontextEditInput = {\n  /**\n   * The prompt to edit the image.\n   */\n  prompt: string;\n  /**\n   * The URL of the image to edit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * Determines how the output resolution is set for image editing.\n   * - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on.\n   * - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits).\n   * Apart from these, a few aspect ratios are also supported. Default value: `\"match_input\"`\n   */\n  resolution_mode?:\n    | \"auto\"\n    | \"match_input\"\n    | \"1:1\"\n    | \"16:9\"\n    | \"21:9\"\n    | \"3:2\"\n    | \"2:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"9:16\"\n    | \"9:21\";\n};\nexport type StreamingKontextImg2ImgInput = {\n  /**\n   * The prompt for the image to image task.\n   */\n  prompt: string;\n  /**\n   * The URL of the image for image-to-image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`\n   */\n  strength?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type StreamingKontextInpaintInput = {\n  /**\n   * The URL of the image to be inpainted.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt for the image to image task.\n   */\n  prompt: string;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The LoRAs to use for the image generation. You can use any number of LoRAs\n   * and they will be merged together to generate the final image.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n  /**\n   * The URL of the reference image for inpainting.\n   */\n  reference_image_url: string | Blob | File;\n  /**\n   * The URL of the mask for inpainting.\n   */\n  mask_url: string | Blob | File;\n  /**\n   * The strength of the initial image. Higher strength values are better for this model. Default value: `0.88`\n   */\n  strength?: number;\n};\nexport type StreamingKontextInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Output format Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type StreamingKreaFlux1Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type StreamingKreaInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The speed of the generation. The higher the speed, the faster the generation. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\" | \"high\";\n};\nexport type StreamingTextToImageInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated image. Default value: `landscape_4_3`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The system prompt to use. Default value: `\"You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts.\"`\n   */\n  system_prompt?: string;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `4`\n   */\n  guidance_scale?: number;\n  /**\n   * The number of inference steps to perform. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to apply normalization-based guidance scale. Default value: `true`\n   */\n  cfg_normalization?: boolean;\n  /**\n   * The ratio of the timestep interval to apply normalization-based guidance scale. Default value: `1`\n   */\n  cfg_trunc_ratio?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type STSInput = {\n  /**\n   * URL to the source audio file to be voice-converted.\n   */\n  source_audio_url: string | Blob | File;\n  /**\n   * The voice to use for the speech-to-speech request. If neither target_voice nor target_voice_audio_url are provided, a random target voice will be used.\n   */\n  target_voice?:\n    | \"Aurora\"\n    | \"Blade\"\n    | \"Britney\"\n    | \"Carl\"\n    | \"Cliff\"\n    | \"Richard\"\n    | \"Rico\"\n    | \"Siobhan\"\n    | \"Vicky\";\n  /**\n   * URL to the audio file which represents the voice of the output audio. If provided, this will override the target_voice setting. If neither target_voice nor target_voice_audio_url are provided, the default target voice will be used.\n   */\n  target_voice_audio_url?: string | Blob | File;\n  /**\n   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.\n   */\n  high_quality_audio?: boolean;\n};\nexport type STSOutput = {\n  /**\n   * The generated voice-converted audio file.\n   */\n  audio: Audio;\n};\nexport type StyleReferenceInput = {\n  /**\n   * URL to zip archive with images, use PNG format. Maximum 5 images are allowed.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * The base style of the generated images, this topic is covered above. Default value: `\"digital_illustration\"`\n   */\n  base_style?:\n    | \"any\"\n    | \"realistic_image\"\n    | \"digital_illustration\"\n    | \"vector_illustration\"\n    | \"realistic_image/b_and_w\"\n    | \"realistic_image/hard_flash\"\n    | \"realistic_image/hdr\"\n    | \"realistic_image/natural_light\"\n    | \"realistic_image/studio_portrait\"\n    | \"realistic_image/enterprise\"\n    | \"realistic_image/motion_blur\"\n    | \"realistic_image/evening_light\"\n    | \"realistic_image/faded_nostalgia\"\n    | \"realistic_image/forest_life\"\n    | \"realistic_image/mystic_naturalism\"\n    | \"realistic_image/natural_tones\"\n    | \"realistic_image/organic_calm\"\n    | \"realistic_image/real_life_glow\"\n    | \"realistic_image/retro_realism\"\n    | \"realistic_image/retro_snapshot\"\n    | \"realistic_image/urban_drama\"\n    | \"realistic_image/village_realism\"\n    | \"realistic_image/warm_folk\"\n    | \"digital_illustration/pixel_art\"\n    | \"digital_illustration/hand_drawn\"\n    | \"digital_illustration/grain\"\n    | \"digital_illustration/infantile_sketch\"\n    | \"digital_illustration/2d_art_poster\"\n    | \"digital_illustration/handmade_3d\"\n    | \"digital_illustration/hand_drawn_outline\"\n    | \"digital_illustration/engraving_color\"\n    | \"digital_illustration/2d_art_poster_2\"\n    | \"digital_illustration/antiquarian\"\n    | \"digital_illustration/bold_fantasy\"\n    | \"digital_illustration/child_book\"\n    | \"digital_illustration/child_books\"\n    | \"digital_illustration/cover\"\n    | \"digital_illustration/crosshatch\"\n    | \"digital_illustration/digital_engraving\"\n    | \"digital_illustration/expressionism\"\n    | \"digital_illustration/freehand_details\"\n    | \"digital_illustration/grain_20\"\n    | \"digital_illustration/graphic_intensity\"\n    | \"digital_illustration/hard_comics\"\n    | \"digital_illustration/long_shadow\"\n    | \"digital_illustration/modern_folk\"\n    | \"digital_illustration/multicolor\"\n    | \"digital_illustration/neon_calm\"\n    | \"digital_illustration/noir\"\n    | \"digital_illustration/nostalgic_pastel\"\n    | \"digital_illustration/outline_details\"\n    | \"digital_illustration/pastel_gradient\"\n    | \"digital_illustration/pastel_sketch\"\n    | \"digital_illustration/pop_art\"\n    | \"digital_illustration/pop_renaissance\"\n    | \"digital_illustration/street_art\"\n    | \"digital_illustration/tablet_sketch\"\n    | \"digital_illustration/urban_glow\"\n    | \"digital_illustration/urban_sketching\"\n    | \"digital_illustration/vanilla_dreams\"\n    | \"digital_illustration/young_adult_book\"\n    | \"digital_illustration/young_adult_book_2\"\n    | \"vector_illustration/bold_stroke\"\n    | \"vector_illustration/chemistry\"\n    | \"vector_illustration/colored_stencil\"\n    | \"vector_illustration/contour_pop_art\"\n    | \"vector_illustration/cosmics\"\n    | \"vector_illustration/cutout\"\n    | \"vector_illustration/depressive\"\n    | \"vector_illustration/editorial\"\n    | \"vector_illustration/emotional_flat\"\n    | \"vector_illustration/infographical\"\n    | \"vector_illustration/marker_outline\"\n    | \"vector_illustration/mosaic\"\n    | \"vector_illustration/naivector\"\n    | \"vector_illustration/roundish_flat\"\n    | \"vector_illustration/segmented_colors\"\n    | \"vector_illustration/sharp_contrast\"\n    | \"vector_illustration/thin\"\n    | \"vector_illustration/vector_photo\"\n    | \"vector_illustration/vivid_shapes\"\n    | \"vector_illustration/engraving\"\n    | \"vector_illustration/line_art\"\n    | \"vector_illustration/line_circuit\"\n    | \"vector_illustration/linocut\";\n};\nexport type StyleReferenceOutput = {\n  /**\n   * The ID of the created style, this ID can be used to reference the style in the future.\n   */\n  style_id: string;\n};\nexport type StyleTransferInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The artistic style to apply. Default value: `\"Van Gogh's Starry Night\"`\n   */\n  prompt?: string;\n};\nexport type StyleTransferOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type SubjectCustomizeInput = {\n  /**\n   * The text prompt describing what you want to see, using [1] to reference the subject\n   */\n  prompt: string;\n  /**\n   * Type of subject in the reference images\n   */\n  subject_type: \"product\" | \"animal\";\n  /**\n   * 1-4 reference images of the subject to customize\n   */\n  reference_images: Array<ReferenceImage>;\n  /**\n   * Optional description of the subject in the reference images\n   */\n  subject_description?: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type SubjectReferenceOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Swin2srInput = {\n  /**\n   * URL of image to be used for image enhancement\n   */\n  image_url: string | Blob | File;\n  /**\n   * seed to be used for generation\n   */\n  seed?: number;\n  /**\n   * Task to perform Default value: `\"classical_sr\"`\n   */\n  task?: \"classical_sr\" | \"compressed_sr\" | \"real_sr\";\n};\nexport type Swin2srOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n};\nexport type Switti512Input = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of top-k tokens to sample from. Default value: `400`\n   */\n  sampling_top_k?: number;\n  /**\n   * The top-p probability to sample from. Default value: `0.95`\n   */\n  sampling_top_p?: number;\n  /**\n   * Smoothing with Gumbel softmax sampling Default value: `true`\n   */\n  more_smooth?: boolean;\n  /**\n   * More diverse sampling\n   */\n  more_diverse?: boolean;\n  /**\n   * Smoothing starting scale Default value: `2`\n   */\n  smooth_start_si?: number;\n  /**\n   * Disable CFG starting scale Default value: `8`\n   */\n  turn_off_cfg_start_si?: number;\n  /**\n   * Temperature after disabling CFG Default value: `0.1`\n   */\n  last_scale_temp?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `6`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type Switti512Output = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SwittiInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of top-k tokens to sample from. Default value: `400`\n   */\n  sampling_top_k?: number;\n  /**\n   * The top-p probability to sample from. Default value: `0.95`\n   */\n  sampling_top_p?: number;\n  /**\n   * Smoothing with Gumbel softmax sampling Default value: `true`\n   */\n  more_smooth?: boolean;\n  /**\n   * More diverse sampling\n   */\n  more_diverse?: boolean;\n  /**\n   * Smoothing starting scale Default value: `2`\n   */\n  smooth_start_si?: number;\n  /**\n   * Disable CFG starting scale Default value: `8`\n   */\n  turn_off_cfg_start_si?: number;\n  /**\n   * Temperature after disabling CFG Default value: `0.1`\n   */\n  last_scale_temp?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `6`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n};\nexport type SwittiOutput = {\n  /**\n   * The generated images\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used for generating the image.\n   */\n  prompt: string;\n};\nexport type SyncLipsyncInput = {\n  /**\n   * The model to use for lipsyncing Default value: `\"lipsync-1.9.0-beta\"`\n   */\n  model?: \"lipsync-1.8.0\" | \"lipsync-1.7.1\" | \"lipsync-1.9.0-beta\";\n  /**\n   * URL of the input video\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the input audio\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Lipsync mode when audio and video durations are out of sync. Default value: `\"cut_off\"`\n   */\n  sync_mode?: \"cut_off\" | \"loop\" | \"bounce\" | \"silence\" | \"remap\";\n};\nexport type SyncLipsyncOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type SyncLipsyncV2Input = {\n  /**\n   * URL of the input video\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL of the input audio\n   */\n  audio_url: string | Blob | File;\n  /**\n   * Lipsync mode when audio and video durations are out of sync. Default value: `\"cut_off\"`\n   */\n  sync_mode?: \"cut_off\" | \"loop\" | \"bounce\" | \"silence\" | \"remap\";\n};\nexport type SyncLipsyncV2Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2VDirectorOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2VLiveOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type T2VOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TavusHummingbirdLipsyncV0Input = {\n  /**\n   * A direct link to the video that will be modified. This should be a publicly accessible / presigned S3 URL.\n   */\n  video_url: string | Blob | File;\n  /**\n   * A direct link to the audio file that will be synchronized with the video. This should be a publicly accessible / presigned S3 URL.\n   */\n  audio_url: string | Blob | File;\n};\nexport type TavusHummingbirdLipsyncV0Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TemplateToVideoOutput = {\n  /**\n   * The generated video using a predefined template\n   */\n  video: File;\n};\nexport type Text2VideoInput = {\n  /**\n   * The avatar to use for the video\n   */\n  avatar_id:\n    | \"emily_vertical_primary\"\n    | \"emily_vertical_secondary\"\n    | \"marcus_vertical_primary\"\n    | \"marcus_vertical_secondary\"\n    | \"mira_vertical_primary\"\n    | \"mira_vertical_secondary\"\n    | \"jasmine_vertical_primary\"\n    | \"jasmine_vertical_secondary\"\n    | \"jasmine_vertical_walking\"\n    | \"aisha_vertical_walking\"\n    | \"elena_vertical_primary\"\n    | \"elena_vertical_secondary\"\n    | \"any_male_vertical_primary\"\n    | \"any_female_vertical_primary\"\n    | \"any_male_vertical_secondary\"\n    | \"any_female_vertical_secondary\"\n    | \"any_female_vertical_walking\"\n    | \"emily_primary\"\n    | \"emily_side\"\n    | \"marcus_primary\"\n    | \"marcus_side\"\n    | \"aisha_walking\"\n    | \"elena_primary\"\n    | \"elena_side\"\n    | \"any_male_primary\"\n    | \"any_female_primary\"\n    | \"any_male_side\"\n    | \"any_female_side\";\n  /**\n   *\n   */\n  text: string;\n};\nexport type TextOutput = {\n  /**\n   * The answer to the query.\n   */\n  text: string;\n  /**\n   * The seed used for the generation.\n   */\n  seed: number;\n  /**\n   * The query used for the generation.\n   */\n  prompt: string;\n  /**\n   * The timings of the generation.\n   */\n  timings: any;\n};\nexport type TextRemovalInput = {\n  /**\n   * URL of the image containing text to be removed.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type TextRemovalOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type TextTo3dInput = {\n  /**\n   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.\n   */\n  seed?: number;\n  /**\n   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.\n   */\n  face_limit?: number;\n  /**\n   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.\n   */\n  pbr?: boolean;\n  /**\n   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `\"standard\"`\n   */\n  texture?: \"no\" | \"standard\" | \"HD\";\n  /**\n   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.\n   */\n  texture_seed?: number;\n  /**\n   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.\n   */\n  auto_size?: boolean;\n  /**\n   * Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.\n   */\n  style?:\n    | \"person:person2cartoon\"\n    | \"object:clay\"\n    | \"object:steampunk\"\n    | \"animal:venom\"\n    | \"object:barbie\"\n    | \"object:christmas\"\n    | \"gold\"\n    | \"ancient_bronze\";\n  /**\n   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.\n   */\n  quad?: boolean;\n  /**\n   * Text input that directs the model generation. The maximum prompt length is 1024 characters, equivalent to approximately 100 words. The API supports multiple languages. However, emojis and certain special Unicode characters are not supported.\n   */\n  prompt: string;\n  /**\n   * Unlike prompt, it provides a reverse direction to assist in generating content contrasting with the original prompt. The maximum length is 255 characters.\n   */\n  negative_prompt?: string;\n  /**\n   * This is the random seed used for the process based on the prompt. This parameter is an integer and is randomly chosen if not set.\n   */\n  image_seed?: number;\n};\nexport type TextToImage32Input = {\n  /**\n   * Prompt for image generation.\n   */\n  prompt: string;\n  /**\n   * Number of inference steps. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Random seed for reproducibility. Default value: `5555`\n   */\n  seed?: number;\n  /**\n   * Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9 Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:2\"\n    | \"3:4\"\n    | \"4:3\"\n    | \"4:5\"\n    | \"5:4\"\n    | \"9:16\"\n    | \"16:9\";\n  /**\n   * Negative prompt for image generation. Default value: `\"Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Guidance scale for text. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * Whether to truncate the prompt. Default value: `true`\n   */\n  truncate_prompt?: boolean;\n  /**\n   * Whether to improve the prompt. Default value: `true`\n   */\n  prompt_enhancer?: boolean;\n  /**\n   * If true, returns the image directly in the response (increases latency).\n   */\n  sync_mode?: boolean;\n};\nexport type TextToImage32Output = {\n  /**\n   * Generated image.\n   */\n  image: Image;\n};\nexport type TextToImageInput = {\n  /**\n   *\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?:\n    | \"10:16\"\n    | \"16:10\"\n    | \"9:16\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:4\"\n    | \"1:1\"\n    | \"1:3\"\n    | \"3:1\"\n    | \"3:2\"\n    | \"2:3\";\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality. Default value: `true`\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The style of the generated image Default value: `\"auto\"`\n   */\n  style?: \"auto\" | \"general\" | \"realistic\" | \"design\" | \"render_3D\" | \"anime\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * A negative prompt to avoid in the generated image Default value: `\"\"`\n   */\n  negative_prompt?: string;\n};\nexport type TextToImageOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type TextToImageTurboInput = {\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to use. Use it to address details that you don't want\n   * in the image. This could be colors, objects, scenery and even the small details\n   * (e.g. moustache, blurry, low resolution). Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `4`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you.\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type TextToImageV4FastInput = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type TextToImageV4Input = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type TextToImageV4UltraInput = {\n  /**\n   * The text prompt describing what you want to see\n   */\n  prompt: string;\n  /**\n   * A description of what to discourage in the generated images Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The aspect ratio of the generated image Default value: `\"1:1\"`\n   */\n  aspect_ratio?: \"1:1\" | \"16:9\" | \"9:16\" | \"3:4\" | \"4:3\";\n  /**\n   * Number of images to generate (1-4) Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * Random seed for reproducible generation\n   */\n  seed?: number;\n};\nexport type TextToSpeechOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Duration of the audio in milliseconds\n   */\n  duration_ms: number;\n};\nexport type TextToVideoHailuo02Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TextToVideoInput = {\n  /**\n   * Text prompt to guide generation\n   */\n  prompt: string;\n  /**\n   * Negative prompt for generation Default value: `\"worst quality, inconsistent motion, blurry, jittery, distorted\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Resolution of the generated video (480p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * Number of inference steps Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to expand the prompt using the model's own capabilities. Default value: `true`\n   */\n  expand_prompt?: boolean;\n};\nexport type TextToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type TextToVideoPreviewInput = {\n  /**\n   * The text prompt describing the video you want to generate\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"8s\"`\n   */\n  duration?: \"8s\";\n  /**\n   * A negative prompt to guide the video generation\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to enhance the video generation Default value: `true`\n   */\n  enhance_prompt?: boolean;\n  /**\n   * A seed to use for the video generation\n   */\n  seed?: number;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n   */\n  generate_audio?: boolean;\n};\nexport type TextToVideoPreviewOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TextToVideoV21MasterOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TextToVideoV2MasterOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TheraInput = {\n  /**\n   * URL of image to be used for upscaling\n   */\n  image_url: string | Blob | File;\n  /**\n   *  Default value: `2`\n   */\n  upscaling?: number;\n  /**\n   * Backbone to use for upscaling\n   */\n  backbone: \"edsr\" | \"rdn\";\n  /**\n   * Random seed for reproducible generation.\n   */\n  seed?: number;\n};\nexport type TheraOutput = {\n  /**\n   * The generated image file info.\n   */\n  image: Image;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n};\nexport type ThinksoundAudioInput = {\n  /**\n   * The URL of the video to generate the audio for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * A prompt to guide the audio generation. If not provided, it will be extracted from the video. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of inference steps for audio generation. Default value: `24`\n   */\n  num_inference_steps?: number;\n  /**\n   * The classifier-free guidance scale for audio generation. Default value: `5`\n   */\n  cfg_scale?: number;\n};\nexport type ThinksoundAudioOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: File;\n  /**\n   * The prompt used to generate the audio.\n   */\n  prompt: string;\n};\nexport type ThinksoundInput = {\n  /**\n   * The URL of the video to generate the audio for.\n   */\n  video_url: string | Blob | File;\n  /**\n   * A prompt to guide the audio generation. If not provided, it will be extracted from the video. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The number of inference steps for audio generation. Default value: `24`\n   */\n  num_inference_steps?: number;\n  /**\n   * The classifier-free guidance scale for audio generation. Default value: `5`\n   */\n  cfg_scale?: number;\n};\nexport type ThinksoundOutput = {\n  /**\n   * The generated video with audio.\n   */\n  video: File;\n  /**\n   * The prompt used to generate the audio.\n   */\n  prompt: string;\n};\nexport type TimeOfDayInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The time of day to transform the scene to. Default value: `\"golden hour\"`\n   */\n  prompt?: string;\n};\nexport type TimeOfDayOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type TopazUpscaleVideoInput = {\n  /**\n   * URL of the video to upscale\n   */\n  video_url: string | Blob | File;\n  /**\n   * Factor to upscale the video by (e.g. 2.0 doubles width and height) Default value: `2`\n   */\n  upscale_factor?: number;\n  /**\n   * Target FPS for frame interpolation. If set, frame interpolation will be enabled.\n   */\n  target_fps?: number;\n};\nexport type TopazUpscaleVideoOutput = {\n  /**\n   * The upscaled video file\n   */\n  video: File;\n};\nexport type TrainingInput = {\n  /**\n   * The name of the training job (required, max 255 characters).\n   */\n  name: string;\n  /**\n   * A list of audio URLs used for training (must be between 1 and 5 URLs).\n   */\n  training_data: Array<AudioInput>;\n};\nexport type TranscriptionOutput = {\n  /**\n   * The full transcribed text\n   */\n  text: string;\n  /**\n   * Detected or specified language code\n   */\n  language_code: string;\n  /**\n   * Confidence in language detection\n   */\n  language_probability: number;\n  /**\n   * Word-level transcription details\n   */\n  words: Array<TranscriptionWord>;\n};\nexport type TransitionOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type TranspixarInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The number of inference steps to perform. Default value: `24`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * The target FPS of the video Default value: `8`\n   */\n  export_fps?: number;\n};\nexport type TranspixarOutput = {\n  /**\n   * The URL to the generated video\n   */\n  videos: Array<File>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated video. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * The prompt used for generating the video.\n   */\n  prompt: string;\n};\nexport type TrellisInput = {\n  /**\n   * URL of the input image to convert to 3D\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for reproducibility\n   */\n  seed?: number;\n  /**\n   * Guidance strength for sparse structure generation Default value: `7.5`\n   */\n  ss_guidance_strength?: number;\n  /**\n   * Sampling steps for sparse structure generation Default value: `12`\n   */\n  ss_sampling_steps?: number;\n  /**\n   * Guidance strength for structured latent generation Default value: `3`\n   */\n  slat_guidance_strength?: number;\n  /**\n   * Sampling steps for structured latent generation Default value: `12`\n   */\n  slat_sampling_steps?: number;\n  /**\n   * Mesh simplification factor Default value: `0.95`\n   */\n  mesh_simplify?: number;\n  /**\n   * Texture resolution Default value: `\"1024\"`\n   */\n  texture_size?: \"512\" | \"1024\" | \"2048\";\n};\nexport type TrellisMultiInput = {\n  /**\n   * List of URLs of input images to convert to 3D\n   */\n  image_urls: Array<string>;\n  /**\n   * Random seed for reproducibility\n   */\n  seed?: number;\n  /**\n   * Guidance strength for sparse structure generation Default value: `7.5`\n   */\n  ss_guidance_strength?: number;\n  /**\n   * Sampling steps for sparse structure generation Default value: `12`\n   */\n  ss_sampling_steps?: number;\n  /**\n   * Guidance strength for structured latent generation Default value: `3`\n   */\n  slat_guidance_strength?: number;\n  /**\n   * Sampling steps for structured latent generation Default value: `12`\n   */\n  slat_sampling_steps?: number;\n  /**\n   * Mesh simplification factor Default value: `0.95`\n   */\n  mesh_simplify?: number;\n  /**\n   * Texture resolution Default value: `\"1024\"`\n   */\n  texture_size?: \"512\" | \"1024\" | \"2048\";\n  /**\n   * Algorithm for multi-image generation Default value: `\"stochastic\"`\n   */\n  multiimage_algo?: \"stochastic\" | \"multidiffusion\";\n};\nexport type TrellisMultiOutput = {\n  /**\n   * Generated 3D mesh file\n   */\n  model_mesh: File;\n  /**\n   * Processing timings\n   */\n  timings: any;\n};\nexport type TrellisOutput = {\n  /**\n   * Generated 3D mesh file\n   */\n  model_mesh: File;\n  /**\n   * Processing timings\n   */\n  timings: any;\n};\nexport type TripoV25ImageTo3dInput = {\n  /**\n   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.\n   */\n  seed?: number;\n  /**\n   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.\n   */\n  face_limit?: number;\n  /**\n   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.\n   */\n  pbr?: boolean;\n  /**\n   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `\"standard\"`\n   */\n  texture?: \"no\" | \"standard\" | \"HD\";\n  /**\n   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.\n   */\n  texture_seed?: number;\n  /**\n   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.\n   */\n  auto_size?: boolean;\n  /**\n   * Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.\n   */\n  style?:\n    | \"person:person2cartoon\"\n    | \"object:clay\"\n    | \"object:steampunk\"\n    | \"animal:venom\"\n    | \"object:barbie\"\n    | \"object:christmas\"\n    | \"gold\"\n    | \"ancient_bronze\";\n  /**\n   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.\n   */\n  quad?: boolean;\n  /**\n   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `original_image`\n   */\n  texture_alignment?: \"original_image\" | \"geometry\";\n  /**\n   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `default`\n   */\n  orientation?: \"default\" | \"align_image\";\n  /**\n   * URL of the image to use for model generation.\n   */\n  image_url: string | Blob | File;\n};\nexport type TripoV25ImageTo3dOutput = {\n  /**\n   * The task id of the 3D model generation.\n   */\n  task_id: string;\n  /**\n   * Model\n   */\n  model_mesh?: File;\n  /**\n   * Base model\n   */\n  base_model?: File;\n  /**\n   * Pbr model\n   */\n  pbr_model?: File;\n  /**\n   * A preview image of the model\n   */\n  rendered_image?: File;\n};\nexport type TripoV25MultiviewTo3dInput = {\n  /**\n   * This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set.\n   */\n  seed?: number;\n  /**\n   * Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined.\n   */\n  face_limit?: number;\n  /**\n   * A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True.\n   */\n  pbr?: boolean;\n  /**\n   * An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures. Default value: `\"standard\"`\n   */\n  texture?: \"no\" | \"standard\" | \"HD\";\n  /**\n   * This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed.\n   */\n  texture_seed?: number;\n  /**\n   * Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False.\n   */\n  auto_size?: boolean;\n  /**\n   * Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance.\n   */\n  style?:\n    | \"person:person2cartoon\"\n    | \"object:clay\"\n    | \"object:steampunk\"\n    | \"animal:venom\"\n    | \"object:barbie\"\n    | \"object:christmas\"\n    | \"gold\"\n    | \"ancient_bronze\";\n  /**\n   * Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model.\n   */\n  quad?: boolean;\n  /**\n   * Determines the prioritization of texture alignment in the 3D model. The default value is original_image. Default value: `original_image`\n   */\n  texture_alignment?: \"original_image\" | \"geometry\";\n  /**\n   * Set orientation=align_image to automatically rotate the model to align the original image. The default value is default. Default value: `default`\n   */\n  orientation?: \"default\" | \"align_image\";\n  /**\n   * Front view image of the object.\n   */\n  front_image_url: string | Blob | File;\n  /**\n   * Left view image of the object.\n   */\n  left_image_url?: string | Blob | File;\n  /**\n   * Back view image of the object.\n   */\n  back_image_url?: string | Blob | File;\n  /**\n   * Right view image of the object.\n   */\n  right_image_url?: string | Blob | File;\n};\nexport type TripoV25MultiviewTo3dOutput = {\n  /**\n   * The task id of the 3D model generation.\n   */\n  task_id: string;\n  /**\n   * Model\n   */\n  model_mesh?: File;\n  /**\n   * Base model\n   */\n  base_model?: File;\n  /**\n   * Pbr model\n   */\n  pbr_model?: File;\n  /**\n   * A preview image of the model\n   */\n  rendered_image?: File;\n};\nexport type TTSInput = {\n  /**\n   * Text to synthesize into speech. Default value: `\"My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next.\"`\n   */\n  text?: string;\n  /**\n   * The voice to use for the TTS request. If neither voice nor audio are provided, a random voice will be used.\n   */\n  voice?:\n    | \"Aurora\"\n    | \"Blade\"\n    | \"Britney\"\n    | \"Carl\"\n    | \"Cliff\"\n    | \"Richard\"\n    | \"Rico\"\n    | \"Siobhan\"\n    | \"Vicky\";\n  /**\n   * URL to the audio sample to use as a voice prompt for zero-shot TTS voice cloning. Providing a audio sample will override the voice setting. If neither voice nor audio_url are provided, a random voice will be used.\n   */\n  audio_url?: string | Blob | File;\n  /**\n   * Controls emotion exaggeration. Range typically 0.25 to 2.0. Default value: `0.5`\n   */\n  exaggeration?: number;\n  /**\n   * Classifier-free guidance scale (CFG) controls the conditioning factor. Range typically 0.2 to 1.0. For expressive or dramatic speech, try lower cfg values (e.g. ~0.3) and increase exaggeration to around 0.7 or higher. If the reference speaker has a fast speaking style, lowering cfg to around 0.3 can improve pacing. Default value: `0.5`\n   */\n  cfg?: number;\n  /**\n   * If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz.\n   */\n  high_quality_audio?: boolean;\n  /**\n   * Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed.\n   */\n  seed?: number;\n  /**\n   * Controls the randomness of generation. Range typically 0.05 to 5. Default value: `0.8`\n   */\n  temperature?: number;\n};\nexport type TTSOutput = {\n  /**\n   * The generated audio file\n   */\n  audio: File;\n  /**\n   * Timestamps for each word in the generated speech. Only returned if `timestamps` is set to True in the request.\n   */\n  timestamps?: Array<void>;\n};\nexport type TurboFluxTrainerInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.\n   */\n  images_data_url: string | Blob | File;\n  /**\n   * Trigger phrase to be used in the captions. If None, a trigger word will not be used.\n   * If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions. Default value: `\"ohwx\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * Number of steps to train the LoRA on. Default value: `1000`\n   */\n  steps?: number;\n  /**\n   * Learning rate for the training. Default value: `0.00115`\n   */\n  learning_rate?: number;\n  /**\n   * Training style to use. Default value: `\"subject\"`\n   */\n  training_style?: \"subject\" | \"style\";\n  /**\n   * Whether to try to detect the face and crop the images to the face. Default value: `true`\n   */\n  face_crop?: boolean;\n};\nexport type TurboFluxTrainerOutput = {\n  /**\n   * URL to the trained diffusers lora weights.\n   */\n  diffusers_lora_file: File;\n  /**\n   * URL to the trained diffusers config file.\n   */\n  config_file: File;\n};\nexport type UnoInput = {\n  /**\n   * URL of images to use while generating the image.\n   */\n  input_image_urls: Array<string>;\n  /**\n   * The size of the generated image. You can choose between some presets or custom height and width\n   * that **must be multiples of 8**. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The prompt to generate an image from.\n   */\n  prompt: string;\n  /**\n   * Random seed for reproducible generation. If set none, a random seed will be used.\n   */\n  seed?: number;\n  /**\n   * The number of images to generate. Default value: `1`\n   */\n  num_images?: number;\n  /**\n   * The number of inference steps to perform. Default value: `28`\n   */\n  num_inference_steps?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type UnoOutput = {\n  /**\n   * The URLs of the generated images.\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  timings: any;\n  /**\n   * Seed of the generated Image. It will be the same value of the one passed in the\n   * input or the randomly generated that was used in case none was passed.\n   */\n  seed: number;\n  /**\n   * Whether the generated images contain NSFW concepts.\n   */\n  has_nsfw_concepts: Array<boolean>;\n  /**\n   * The prompt used to generate the image.\n   */\n  prompt: string;\n};\nexport type UpscaleImageInput = {\n  /**\n   * The image URL to upscale\n   */\n  image_url: string | Blob | File;\n  /**\n   * The prompt to upscale the image with Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The resemblance of the upscaled image to the original image Default value: `50`\n   */\n  resemblance?: number;\n  /**\n   * The detail of the upscaled image Default value: `50`\n   */\n  detail?: number;\n  /**\n   * Whether to expand the prompt with MagicPrompt functionality.\n   */\n  expand_prompt?: boolean;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type UpscaleInput = {\n  /**\n   * Input Image to erase from\n   */\n  image_url: string | Blob | File;\n  /**\n   * The desired increase in resolution. Default value: `\"2\"`\n   */\n  desired_increase?: \"2\" | \"4\";\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type UpscaleOutput = {\n  /**\n   *\n   */\n  images: Array<File>;\n  /**\n   * Seed used for the random number generator\n   */\n  seed: number;\n};\nexport type V16Input = {\n  /**\n   * URL or base64 of the model image\n   */\n  model_image: string;\n  /**\n   * URL or base64 of the garment image\n   */\n  garment_image: string;\n  /**\n   * Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment. Default value: `\"auto\"`\n   */\n  category?: \"tops\" | \"bottoms\" | \"one-pieces\" | \"auto\";\n  /**\n   * Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results. Default value: `\"balanced\"`\n   */\n  mode?: \"performance\" | \"balanced\" | \"quality\";\n  /**\n   * Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type. Default value: `\"auto\"`\n   */\n  garment_photo_type?: \"auto\" | \"model\" | \"flat-lay\";\n  /**\n   * Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear. Default value: `\"permissive\"`\n   */\n  moderation_level?: \"none\" | \"permissive\" | \"conservative\";\n  /**\n   * Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results. Default value: `42`\n   */\n  seed?: number;\n  /**\n   * Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result. Default value: `1`\n   */\n  num_samples?: number;\n  /**\n   * Disables human parsing on the model image. Default value: `true`\n   */\n  segmentation_free?: boolean;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster Default value: `\"png\"`\n   */\n  output_format?: \"png\" | \"jpeg\";\n};\nexport type V16Output = {\n  /**\n   *\n   */\n  images: Array<File>;\n};\nexport type V3TTSInput = {\n  /**\n   * The text to be converted to speech.\n   */\n  input: string;\n  /**\n   * The unique ID of a PlayHT or Cloned Voice, or a name from the available presets.\n   */\n  voice: string;\n  /**\n   * The format of the response. Default value: `\"url\"`\n   */\n  response_format?: \"url\" | \"bytes\";\n  /**\n   * An integer number greater than or equal to 0. If equal to null or not provided, a random seed will be used. Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file.\n   */\n  seed?: number;\n};\nexport type V3TTSOutput = {\n  /**\n   * The generated audio file.\n   */\n  audio: AudioFile;\n};\nexport type VectorizeInput = {\n  /**\n   * The URL of the image to be vectorized. Must be in PNG, JPG or WEBP format, less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels, min dimension more than 256 pixels.\n   */\n  image_url: string | Blob | File;\n};\nexport type VectorizeOutput = {\n  /**\n   * The vectorized image.\n   */\n  image: File;\n};\nexport type Veo2ImageToVideoInput = {\n  /**\n   * The text prompt describing how the image should be animated\n   */\n  prompt: string;\n  /**\n   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 aspect ratio. If the image is not in 16:9 aspect ratio, it will be cropped to fit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"6s\" | \"7s\" | \"8s\";\n};\nexport type Veo2ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Veo2Input = {\n  /**\n   * The text prompt describing the video you want to generate\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"5s\"`\n   */\n  duration?: \"5s\" | \"6s\" | \"7s\" | \"8s\";\n  /**\n   * A negative prompt to guide the video generation\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to enhance the video generation Default value: `true`\n   */\n  enhance_prompt?: boolean;\n  /**\n   * A seed to use for the video generation\n   */\n  seed?: number;\n};\nexport type Veo2Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Veo3FastImageToVideoInput = {\n  /**\n   * The text prompt describing how the image should be animated\n   */\n  prompt: string;\n  /**\n   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 aspect ratio. If the image is not in 16:9 aspect ratio, it will be cropped to fit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"8s\"`\n   */\n  duration?: \"8s\";\n  /**\n   * Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n   */\n  generate_audio?: boolean;\n  /**\n   * Resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n};\nexport type Veo3FastImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Veo3FastInput = {\n  /**\n   * The text prompt describing the video you want to generate\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"8s\"`\n   */\n  duration?: \"8s\";\n  /**\n   * A negative prompt to guide the video generation\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to enhance the video generation Default value: `true`\n   */\n  enhance_prompt?: boolean;\n  /**\n   * A seed to use for the video generation\n   */\n  seed?: number;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n   */\n  generate_audio?: boolean;\n};\nexport type Veo3FastOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Veo3ImageToVideoInput = {\n  /**\n   * The text prompt describing how the image should be animated\n   */\n  prompt: string;\n  /**\n   * URL of the input image to animate. Should be 720p or higher resolution in 16:9 aspect ratio. If the image is not in 16:9 aspect ratio, it will be cropped to fit.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The duration of the generated video in seconds Default value: `\"8s\"`\n   */\n  duration?: \"8s\";\n  /**\n   * Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n   */\n  generate_audio?: boolean;\n  /**\n   * Resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n};\nexport type Veo3ImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type Veo3Input = {\n  /**\n   * The text prompt describing the video you want to generate\n   */\n  prompt: string;\n  /**\n   * The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The duration of the generated video in seconds Default value: `\"8s\"`\n   */\n  duration?: \"8s\";\n  /**\n   * A negative prompt to guide the video generation\n   */\n  negative_prompt?: string;\n  /**\n   * Whether to enhance the video generation Default value: `true`\n   */\n  enhance_prompt?: boolean;\n  /**\n   * A seed to use for the video generation\n   */\n  seed?: number;\n  /**\n   * The resolution of the generated video Default value: `\"720p\"`\n   */\n  resolution?: \"720p\" | \"1080p\";\n  /**\n   * Whether to generate audio for the video. If false, %33 less credits will be used. Default value: `true`\n   */\n  generate_audio?: boolean;\n};\nexport type Veo3Output = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type VideoBackgroundRemovalInput = {\n  /**\n   * Input video to remove background from. Size should be less than 14142x14142 and duration less than 30s.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Background color. Options: Transparent, Black, White, Gray, Red, Green, Blue, Yellow, Cyan, Magenta, Orange. Default value: `\"Black\"`\n   */\n  background_color?:\n    | \"Transparent\"\n    | \"Black\"\n    | \"White\"\n    | \"Gray\"\n    | \"Red\"\n    | \"Green\"\n    | \"Blue\"\n    | \"Yellow\"\n    | \"Cyan\"\n    | \"Magenta\"\n    | \"Orange\";\n};\nexport type VideoBackgroundRemovalOutput = {\n  /**\n   * Video with removed background and audio.\n   */\n  video: Video | File;\n};\nexport type VideoConditioningInput = {\n  /**\n   * URL of video to be extended\n   */\n  video_url: string | Blob | File;\n  /**\n   * Frame number of the video from which the conditioning starts. Must be a multiple of 8.\n   */\n  start_frame_num: number;\n};\nexport type VideoEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type VideoInput = {\n  /**\n   * Prompt to be used for the chat completion\n   */\n  prompt: string;\n  /**\n   * The URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * Number of frames to sample from the video. If not provided, all frames are sampled.\n   */\n  num_frames_to_sample?: number;\n};\nexport type VideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type VideoPromptGeneratorInput = {\n  /**\n   * Core concept or thematic input for the video prompt\n   */\n  input_concept: string;\n  /**\n   * Style of the video prompt Default value: `\"Simple\"`\n   */\n  style?:\n    | \"Minimalist\"\n    | \"Simple\"\n    | \"Detailed\"\n    | \"Descriptive\"\n    | \"Dynamic\"\n    | \"Cinematic\"\n    | \"Documentary\"\n    | \"Animation\"\n    | \"Action\"\n    | \"Experimental\";\n  /**\n   * Camera movement style Default value: `\"None\"`\n   */\n  camera_style?:\n    | \"None\"\n    | \"Steadicam flow\"\n    | \"Drone aerials\"\n    | \"Handheld urgency\"\n    | \"Crane elegance\"\n    | \"Dolly precision\"\n    | \"VR 360\"\n    | \"Multi-angle rig\"\n    | \"Static tripod\"\n    | \"Gimbal smoothness\"\n    | \"Slider motion\"\n    | \"Jib sweep\"\n    | \"POV immersion\"\n    | \"Time-slice array\"\n    | \"Macro extreme\"\n    | \"Tilt-shift miniature\"\n    | \"Snorricam character\"\n    | \"Whip pan dynamics\"\n    | \"Dutch angle tension\"\n    | \"Underwater housing\"\n    | \"Periscope lens\";\n  /**\n   * Camera direction Default value: `\"None\"`\n   */\n  camera_direction?:\n    | \"None\"\n    | \"Zoom in\"\n    | \"Zoom out\"\n    | \"Pan left\"\n    | \"Pan right\"\n    | \"Tilt up\"\n    | \"Tilt down\"\n    | \"Orbital rotation\"\n    | \"Push in\"\n    | \"Pull out\"\n    | \"Track forward\"\n    | \"Track backward\"\n    | \"Spiral in\"\n    | \"Spiral out\"\n    | \"Arc movement\"\n    | \"Diagonal traverse\"\n    | \"Vertical rise\"\n    | \"Vertical descent\";\n  /**\n   * Pacing rhythm Default value: `\"None\"`\n   */\n  pacing?:\n    | \"None\"\n    | \"Slow burn\"\n    | \"Rhythmic pulse\"\n    | \"Frantic energy\"\n    | \"Ebb and flow\"\n    | \"Hypnotic drift\"\n    | \"Time-lapse rush\"\n    | \"Stop-motion staccato\"\n    | \"Gradual build\"\n    | \"Quick cut rhythm\"\n    | \"Long take meditation\"\n    | \"Jump cut energy\"\n    | \"Match cut flow\"\n    | \"Cross-dissolve dreamscape\"\n    | \"Parallel action\"\n    | \"Slow motion impact\"\n    | \"Ramping dynamics\"\n    | \"Montage tempo\"\n    | \"Continuous flow\"\n    | \"Episodic breaks\";\n  /**\n   * Special effects approach Default value: `\"None\"`\n   */\n  special_effects?:\n    | \"None\"\n    | \"Practical effects\"\n    | \"CGI enhancement\"\n    | \"Analog glitches\"\n    | \"Light painting\"\n    | \"Projection mapping\"\n    | \"Nanosecond exposures\"\n    | \"Double exposure\"\n    | \"Smoke diffusion\"\n    | \"Lens flare artistry\"\n    | \"Particle systems\"\n    | \"Holographic overlay\"\n    | \"Chromatic aberration\"\n    | \"Digital distortion\"\n    | \"Wire removal\"\n    | \"Motion capture\"\n    | \"Miniature integration\"\n    | \"Weather simulation\"\n    | \"Color grading\"\n    | \"Mixed media composite\"\n    | \"Neural style transfer\";\n  /**\n   * Custom technical elements (optional) Default value: `\"\"`\n   */\n  custom_elements?: string;\n  /**\n   * URL of an image to analyze and incorporate into the video prompt (optional)\n   */\n  image_url?: string | Blob | File;\n  /**\n   * Model to use Default value: `\"google/gemini-2.0-flash-001\"`\n   */\n  model?:\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-5-haiku\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"google/gemini-2.0-flash-001\"\n    | \"meta-llama/llama-3.2-1b-instruct\"\n    | \"meta-llama/llama-3.2-3b-instruct\"\n    | \"meta-llama/llama-3.1-8b-instruct\"\n    | \"meta-llama/llama-3.1-70b-instruct\"\n    | \"openai/gpt-4o-mini\"\n    | \"openai/gpt-4o\"\n    | \"deepseek/deepseek-r1\";\n  /**\n   * Length of the prompt Default value: `\"Medium\"`\n   */\n  prompt_length?: \"Short\" | \"Medium\" | \"Long\";\n};\nexport type VideoPromptGeneratorOutput = {\n  /**\n   * Generated video prompt\n   */\n  prompt: string;\n};\nexport type VideoSoundEffectsGeneratorInput = {\n  /**\n   * A video file to analyze & re-sound with generated SFX.\n   */\n  video_url: Video;\n};\nexport type VideoSoundEffectsGeneratorOutput = {\n  /**\n   * The final video with the newly generated SFX track.\n   */\n  video: File;\n};\nexport type VideoToVideoInput = {\n  /**\n   * The prompt to generate the video from.\n   */\n  prompt: string;\n  /**\n   * The size of the generated video.\n   */\n  video_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n  /**\n   * The negative prompt to generate video from Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * The LoRAs to use for the image generation. We currently support one lora.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model\n   * will output the same video every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your prompt when looking for a related video to show you. Default value: `7`\n   */\n  guidance_scale?: number;\n  /**\n   * Use RIFE for video interpolation Default value: `true`\n   */\n  use_rife?: boolean;\n  /**\n   * The target FPS of the video Default value: `16`\n   */\n  export_fps?: number;\n  /**\n   * The video to generate the video from.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The strength to use for Video to Video.  1.0 completely remakes the video while 0.0 preserves the original. Default value: `0.8`\n   */\n  strength?: number;\n};\nexport type VideoUnderstandingInput = {\n  /**\n   * URL of the video to analyze\n   */\n  video_url: string | Blob | File;\n  /**\n   * The question or prompt about the video content\n   */\n  prompt: string;\n  /**\n   * Whether to request a more detailed analysis of the video\n   */\n  detailed_analysis?: boolean;\n};\nexport type VideoUnderstandingOutput = {\n  /**\n   * The analysis of the video content based on the prompt\n   */\n  output: string;\n};\nexport type VideoUpscalerInput = {\n  /**\n   * The URL of the video to upscale\n   */\n  video_url: string | Blob | File;\n  /**\n   * The scale factor Default value: `2`\n   */\n  scale?: number;\n};\nexport type VideoUpscalerOutput = {\n  /**\n   * The stitched video\n   */\n  video: File;\n};\nexport type ViduImageToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type ViduQ1ImageToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  image_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduQ1ImageToVideoOutput = {\n  /**\n   * The generated video using the Q1 model from a single image\n   */\n  video: File;\n};\nexport type ViduQ1ReferenceToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URLs of the reference images to use for consistent subject appearance. Q1 model supports up to 7 reference images.\n   */\n  reference_image_urls: Array<string>;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n  /**\n   * Whether to add background music to the generated video\n   */\n  bgm?: boolean;\n};\nexport type ViduQ1ReferenceToVideoOutput = {\n  /**\n   * The generated video with consistent subjects from reference images using the Q1 model\n   */\n  video: File;\n};\nexport type ViduQ1StartEndToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * URL of the image to use as the last frame\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduQ1StartEndToVideoOutput = {\n  /**\n   * The generated transition video between start and end frames using the Q1 model\n   */\n  video: File;\n};\nexport type ViduQ1TextToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * The style of output video Default value: `\"general\"`\n   */\n  style?: \"general\" | \"anime\";\n  /**\n   * Seed for the random number generator\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduQ1TextToVideoOutput = {\n  /**\n   * The generated video using the Q1 model\n   */\n  video: File;\n};\nexport type ViduReferenceToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URLs of the reference images to use for consistent subject appearance\n   */\n  reference_image_urls: Array<string>;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduReferenceToVideoOutput = {\n  /**\n   * The generated video with consistent subjects from reference images\n   */\n  video: File;\n};\nexport type ViduStartEndToVideoInput = {\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * URL of the image to use as the first frame\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * URL of the image to use as the last frame\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The movement amplitude of objects in the frame Default value: `\"auto\"`\n   */\n  movement_amplitude?: \"auto\" | \"small\" | \"medium\" | \"large\";\n};\nexport type ViduStartEndToVideoOutput = {\n  /**\n   * The generated transition video between start and end frames\n   */\n  video: File;\n};\nexport type ViduTemplateToVideoInput = {\n  /**\n   * AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50). Default value: `\"hug\"`\n   */\n  template?:\n    | \"dreamy_wedding\"\n    | \"romantic_lift\"\n    | \"sweet_proposal\"\n    | \"couple_arrival\"\n    | \"cupid_arrow\"\n    | \"pet_lovers\"\n    | \"lunar_newyear\"\n    | \"hug\"\n    | \"kiss\"\n    | \"dynasty_dress\"\n    | \"wish_sender\"\n    | \"love_pose\"\n    | \"hair_swap\"\n    | \"youth_rewind\"\n    | \"morphlab\"\n    | \"live_photo\"\n    | \"emotionlab\"\n    | \"live_memory\"\n    | \"interaction\"\n    | \"christmas\";\n  /**\n   * URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image.\n   */\n  input_image_urls: Array<string>;\n  /**\n   * Text prompt for video generation, max 1500 characters\n   */\n  prompt: string;\n  /**\n   * Random seed for generation\n   */\n  seed?: number;\n  /**\n   * The aspect ratio of the output video Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\";\n};\nexport type ViduTemplateToVideoOutput = {\n  /**\n   * The generated video using a predefined template\n   */\n  video: File;\n};\nexport type VignetteInput = {\n  /**\n   * URL of image to process\n   */\n  image_url: string | Blob | File;\n  /**\n   * Vignette strength Default value: `0.5`\n   */\n  vignette_strength?: number;\n};\nexport type VignetteOutput = {\n  /**\n   * The processed images with vignette effect\n   */\n  images: Array<Image>;\n};\nexport type VisionInput = {\n  /**\n   * Prompt to be used for the image\n   */\n  prompt: string;\n  /**\n   * System prompt to provide context or instructions to the model\n   */\n  system_prompt?: string;\n  /**\n   * Should reasoning be the part of the final answer.\n   */\n  reasoning?: boolean;\n  /**\n   * Name of the model to use. Premium models are charged at 3x the rate of standard models, they include: openai/gpt-5-chat, anthropic/claude-3.5-sonnet, google/gemini-pro-1.5, meta-llama/llama-3.2-90b-vision-instruct, anthropic/claude-3.7-sonnet, openai/gpt-4o. Default value: `\"google/gemini-flash-1.5\"`\n   */\n  model?:\n    | \"anthropic/claude-3.7-sonnet\"\n    | \"anthropic/claude-3.5-sonnet\"\n    | \"anthropic/claude-3-haiku\"\n    | \"google/gemini-pro-1.5\"\n    | \"google/gemini-flash-1.5\"\n    | \"google/gemini-flash-1.5-8b\"\n    | \"google/gemini-2.0-flash-001\"\n    | \"openai/gpt-4o\"\n    | \"openai/gpt-5-chat\"\n    | \"meta-llama/llama-3.2-90b-vision-instruct\"\n    | \"meta-llama/llama-4-maverick\"\n    | \"meta-llama/llama-4-scout\";\n  /**\n   * URL of the image to be processed\n   */\n  image_url: string | Blob | File;\n};\nexport type VoiceCloneOutput = {\n  /**\n   * The cloned voice ID for use with TTS\n   */\n  custom_voice_id: string;\n  /**\n   * Preview audio generated with the cloned voice (if requested)\n   */\n  audio?: File;\n};\nexport type VoiceCloningOutput = {\n  /**\n   * The id of the cloned voice\n   */\n  voice_id: string;\n};\nexport type VoiceDeleteOutput = {\n  /**\n   * The voice_id of the voice that was deleted\n   */\n  voice_id: string;\n};\nexport type VoiceDesignOutput = {\n  /**\n   * The voice_id of the generated voice\n   */\n  custom_voice_id: string;\n  /**\n   * The preview audio using the generated voice\n   */\n  audio: File;\n};\nexport type VTONInput = {\n  /**\n   * The number of inference steps to perform. Default value: `50`\n   */\n  num_inference_steps?: number;\n  /**\n   * The same seed and the same input given to the same version of the model\n   * will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want\n   * the model to stick to your input when generating the image. Default value: `2.5`\n   */\n  guidance_scale?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded\n   * before returning the response. This will increase the latency of the function but\n   * it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * If set to true, the safety checker will be enabled. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The format of the generated image. Default value: `\"png\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * Url for the human image.\n   */\n  human_image_url: string | Blob | File;\n  /**\n   * Url to the garment image.\n   */\n  garment_image_url: string | Blob | File;\n  /**\n   * The type of the garment used for virtual try-on.\n   */\n  garment_type: \"upper_body\" | \"lower_body\" | \"dresses\";\n};\nexport type VTONOutput = {\n  /**\n   * The output image.\n   */\n  image: Image;\n  /**\n   * The seed for the inference.\n   */\n  seed: number;\n  /**\n   * Whether the image contains NSFW concepts.\n   */\n  has_nsfw_concepts: boolean;\n};\nexport type WanEffectsInput = {\n  /**\n   * The subject to insert into the predefined prompt template for the selected effect.\n   */\n  subject: string;\n  /**\n   * URL of the input image.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The type of effect to apply to the video. Default value: `\"cakeify\"`\n   */\n  effect_type?:\n    | \"squish\"\n    | \"muscle\"\n    | \"inflate\"\n    | \"crush\"\n    | \"rotate\"\n    | \"gun-shooting\"\n    | \"deflate\"\n    | \"cakeify\"\n    | \"hulk\"\n    | \"baby\"\n    | \"bride\"\n    | \"classy\"\n    | \"puppy\"\n    | \"snow-white\"\n    | \"disney-princess\"\n    | \"mona-lisa\"\n    | \"painting\"\n    | \"pirate-captain\"\n    | \"princess\"\n    | \"jungle\"\n    | \"samurai\"\n    | \"vip\"\n    | \"warrior\"\n    | \"zen\"\n    | \"assassin\"\n    | \"timelapse\"\n    | \"tsunami\"\n    | \"fire\"\n    | \"zoom-call\"\n    | \"doom-fps\"\n    | \"fus-ro-dah\"\n    | \"hug-jesus\"\n    | \"robot-face-reveal\"\n    | \"super-saiyan\"\n    | \"jumpscare\"\n    | \"laughing\"\n    | \"cartoon-jaw-drop\"\n    | \"crying\"\n    | \"kissing\"\n    | \"angry-face\"\n    | \"selfie-younger-self\"\n    | \"animeify\"\n    | \"blast\";\n  /**\n   * Number of frames to generate. Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Aspect ratio of the output video. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The scale of the LoRA weight. Used to adjust effect intensity. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * Whether to use turbo mode. If True, the video will be generated faster but with lower quality.\n   */\n  turbo_mode?: boolean;\n};\nexport type WanEffectsOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type WanFlf2vInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the starting image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  start_image_url: string | Blob | File;\n  /**\n   * URL of the ending image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  end_image_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`\n   */\n  guide_scale?: number;\n  /**\n   * Shift parameter for video generation. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type WanFlf2vOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanI2vInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`\n   */\n  guide_scale?: number;\n  /**\n   * Shift parameter for video generation. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'. Default value: `\"regular\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n};\nexport type WanI2vLoraInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units. Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit. Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"720p\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `5`\n   */\n  guide_scale?: number;\n  /**\n   * Shift parameter for video generation. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Aspect ratio of the output video. Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * LoRA weights to be used in the inference.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If true, the video will be reversed.\n   */\n  reverse_video?: boolean;\n  /**\n   * If true, the video will be generated faster with no noticeable degradation in the visual quality. Default value: `true`\n   */\n  turbo_mode?: boolean;\n};\nexport type WanI2vLoraOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanI2vOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanProImageToVideoInput = {\n  /**\n   * The prompt to generate the video\n   */\n  prompt: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Whether to enable the safety checker Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The URL of the image to generate the video from\n   */\n  image_url: string | Blob | File;\n};\nexport type WanProImageToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type WanProTextToVideoInput = {\n  /**\n   * The prompt to generate the video\n   */\n  prompt: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Whether to enable the safety checker Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n};\nexport type WanProTextToVideoOutput = {\n  /**\n   * The generated video\n   */\n  video: File;\n};\nexport type WanT2vInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * If true, the video will be generated faster with no noticeable degradation in the visual quality.\n   */\n  turbo_mode?: boolean;\n};\nexport type WanT2vLoraInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"480p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"9:16\" | \"16:9\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * If true, the video will be generated faster with no noticeable degradation in the visual quality. Default value: `true`\n   */\n  turbo_mode?: boolean;\n  /**\n   * LoRA weights to be used in the inference.\n   */\n  loras?: Array<LoraWeight>;\n  /**\n   * If true, the video will be reversed.\n   */\n  reverse_video?: boolean;\n};\nexport type WanT2vLoraOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanT2vOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanTrainerFlf2v720pInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.\n   */\n  training_data_url: string | Blob | File;\n  /**\n   * The number of steps to train for. Default value: `400`\n   */\n  number_of_steps?: number;\n  /**\n   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`\n   */\n  learning_rate?: number;\n  /**\n   * The phrase that will trigger the model to generate an image. Default value: `\"\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * If true, the input will be automatically scale the video to 81 frames at 16fps.\n   */\n  auto_scale_input?: boolean;\n};\nexport type WanTrainerFlf2v720pOutput = {\n  /**\n   * URL to the trained LoRA weights.\n   */\n  lora_file: File;\n  /**\n   * Configuration used for setting up the inference endpoints.\n   */\n  config_file: File;\n};\nexport type WanTrainerI2v720pInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.\n   */\n  training_data_url: string | Blob | File;\n  /**\n   * The number of steps to train for. Default value: `400`\n   */\n  number_of_steps?: number;\n  /**\n   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`\n   */\n  learning_rate?: number;\n  /**\n   * The phrase that will trigger the model to generate an image. Default value: `\"\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * If true, the input will be automatically scale the video to 81 frames at 16fps.\n   */\n  auto_scale_input?: boolean;\n};\nexport type WanTrainerI2v720pOutput = {\n  /**\n   * URL to the trained LoRA weights.\n   */\n  lora_file: File;\n  /**\n   * Configuration used for setting up the inference endpoints.\n   */\n  config_file: File;\n};\nexport type WanTrainerInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.\n   */\n  training_data_url: string | Blob | File;\n  /**\n   * The number of steps to train for. Default value: `400`\n   */\n  number_of_steps?: number;\n  /**\n   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`\n   */\n  learning_rate?: number;\n  /**\n   * The phrase that will trigger the model to generate an image. Default value: `\"\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * If true, the input will be automatically scale the video to 81 frames at 16fps.\n   */\n  auto_scale_input?: boolean;\n};\nexport type WanTrainerOutput = {\n  /**\n   * URL to the trained LoRA weights.\n   */\n  lora_file: File;\n  /**\n   * Configuration used for setting up the inference endpoints.\n   */\n  config_file: File;\n};\nexport type WanTrainerT2v14bInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.\n   */\n  training_data_url: string | Blob | File;\n  /**\n   * The number of steps to train for. Default value: `400`\n   */\n  number_of_steps?: number;\n  /**\n   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`\n   */\n  learning_rate?: number;\n  /**\n   * The phrase that will trigger the model to generate an image. Default value: `\"\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * If true, the input will be automatically scale the video to 81 frames at 16fps.\n   */\n  auto_scale_input?: boolean;\n};\nexport type WanTrainerT2v14bOutput = {\n  /**\n   * URL to the trained LoRA weights.\n   */\n  lora_file: File;\n  /**\n   * Configuration used for setting up the inference endpoints.\n   */\n  config_file: File;\n};\nexport type WanTrainerT2vInput = {\n  /**\n   * URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better.\n   *\n   * In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.\n   */\n  training_data_url: string | Blob | File;\n  /**\n   * The number of steps to train for. Default value: `400`\n   */\n  number_of_steps?: number;\n  /**\n   * The rate at which the model learns. Higher values can lead to faster training, but over-fitting. Default value: `0.0002`\n   */\n  learning_rate?: number;\n  /**\n   * The phrase that will trigger the model to generate an image. Default value: `\"\"`\n   */\n  trigger_phrase?: string;\n  /**\n   * If true, the input will be automatically scale the video to 81 frames at 16fps.\n   */\n  auto_scale_input?: boolean;\n};\nexport type WanTrainerT2vOutput = {\n  /**\n   * URL to the trained LoRA weights.\n   */\n  lora_file: File;\n  /**\n   * Configuration used for setting up the inference endpoints.\n   */\n  config_file: File;\n};\nexport type WanV225bImageToVideoInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`\n   */\n  frames_per_second?: number;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (580p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n};\nexport type WanV225bImageToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV225bTextToImageInput = {\n  /**\n   * The text prompt to guide image generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Shift value for the image. Must be between 1.0 and 10.0. Default value: `2`\n   */\n  shift?: number;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type WanV225bTextToImageOutput = {\n  /**\n   * The generated image file.\n   */\n  image: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV225bTextToVideoDistillInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (580p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `1`\n   */\n  guidance_scale?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n};\nexport type WanV225bTextToVideoDistillOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV225bTextToVideoFastWanInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (580p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n};\nexport type WanV225bTextToVideoFastWanOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV225bTextToVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `24`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (580p or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `40`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n};\nexport type WanV225bTextToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bImageToVideoLoraInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`\n   */\n  guidance_scale_2?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n  /**\n   * LoRA weights to be used in the inference.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * If true, the video will be reversed.\n   */\n  reverse_video?: boolean;\n};\nexport type WanV22A14bImageToVideoLoraOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bImageToVideoTurboInput = {\n  /**\n   * URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n};\nexport type WanV22A14bImageToVideoTurboOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bTextToImageInput = {\n  /**\n   * The text prompt to guide image generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`\n   */\n  guidance_scale_2?: number;\n  /**\n   * Shift value for the image. Must be between 1.0 and 10.0. Default value: `2`\n   */\n  shift?: number;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type WanV22A14bTextToImageLoraInput = {\n  /**\n   * The text prompt to guide image generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`\n   */\n  guidance_scale_2?: number;\n  /**\n   * Shift value for the image. Must be between 1.0 and 10.0. Default value: `2`\n   */\n  shift?: number;\n  /**\n   * LoRA weights to be used in the inference.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * If true, the video will be reversed.\n   */\n  reverse_video?: boolean;\n  /**\n   * The size of the generated image. Default value: `square_hd`\n   */\n  image_size?:\n    | ImageSize\n    | \"square_hd\"\n    | \"square\"\n    | \"portrait_4_3\"\n    | \"portrait_16_9\"\n    | \"landscape_4_3\"\n    | \"landscape_16_9\";\n};\nexport type WanV22A14bTextToImageLoraOutput = {\n  /**\n   * The generated image file.\n   */\n  image: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bTextToImageOutput = {\n  /**\n   * The generated image file.\n   */\n  image: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bTextToVideoInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`\n   */\n  guidance_scale_2?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n};\nexport type WanV22A14bTextToVideoLoraInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`\n   */\n  guidance_scale_2?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n  /**\n   * LoRA weights to be used in the inference.\n   */\n  loras?: Array<LoRAWeight>;\n  /**\n   * If true, the video will be reversed.\n   */\n  reverse_video?: boolean;\n};\nexport type WanV22A14bTextToVideoLoraOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bTextToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bTextToVideoTurboInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n};\nexport type WanV22A14bTextToVideoTurboOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanV22A14bVideoToVideoInput = {\n  /**\n   * URL of the input video.\n   */\n  video_url: string | Blob | File;\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video. Default value: `0.9`\n   */\n  strength?: number;\n  /**\n   * Number of frames to generate. Must be between 81 to 121 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Negative prompt for video generation. Default value: `\"\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p, 580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video. Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"9:16\" | \"1:1\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `27`\n   */\n  num_inference_steps?: number;\n  /**\n   * If set to true, input data will be checked for safety before processing.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'none'. Default value: `\"none\"`\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model. Default value: `4`\n   */\n  guidance_scale_2?: number;\n  /**\n   * Shift value for the video. Must be between 1.0 and 10.0. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * The model to use for frame interpolation. If None, no interpolation is applied. Default value: `\"film\"`\n   */\n  interpolator_model?: \"none\" | \"film\" | \"rife\";\n  /**\n   * Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4. Default value: `1`\n   */\n  num_interpolated_frames?: number;\n  /**\n   * If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is. Default value: `true`\n   */\n  adjust_fps_for_interpolation?: boolean;\n  /**\n   * If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.\n   */\n  resample_fps?: boolean;\n};\nexport type WanV22A14bVideoToVideoOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The text prompt used for video generation. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace13bInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Task type for the model. Default value: `\"depth\"`\n   */\n  task?: \"depth\" | \"inpainting\" | \"pose\";\n  /**\n   * Shift parameter for video generation. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"auto\" | \"9:16\" | \"16:9\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * URL to the source video file. If provided, the model will use this video as a reference.\n   */\n  video_url?: string | Blob | File;\n  /**\n   * URL to the source mask file. If provided, the model will use this mask as a reference.\n   */\n  mask_video_url?: string | Blob | File;\n  /**\n   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Urls to source reference image. If provided, the model will use this image as reference.\n   */\n  ref_image_urls?: Array<string>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Whether to preprocess the input video.\n   */\n  preprocess?: boolean;\n};\nexport type WanVace13bOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace14bDepthInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.\n   */\n  match_input_num_frames?: boolean;\n  /**\n   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.\n   */\n  match_input_frames_per_second?: boolean;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * URL to the source video file. Required for depth task.\n   */\n  video_url: string | Blob | File;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Whether to preprocess the input video.\n   */\n  preprocess?: boolean;\n  /**\n   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.\n   */\n  acceleration?: \"none\" | \"regular\";\n};\nexport type WanVace14bDepthOutput = {\n  /**\n   * The generated depth video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace14bInpaintingInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.\n   */\n  match_input_num_frames?: boolean;\n  /**\n   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.\n   */\n  match_input_frames_per_second?: boolean;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * URL to the source video file. Required for inpainting.\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL to the source mask file. Required for inpainting.\n   */\n  mask_video_url: string | Blob | File;\n  /**\n   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Urls to source reference image. If provided, the model will use this image as reference.\n   */\n  ref_image_urls?: Array<string>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Whether to preprocess the input video.\n   */\n  preprocess?: boolean;\n  /**\n   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.\n   */\n  acceleration?: \"none\" | \"regular\";\n};\nexport type WanVace14bInpaintingOutput = {\n  /**\n   * The generated inpainting video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace14bInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.\n   */\n  match_input_num_frames?: boolean;\n  /**\n   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.\n   */\n  match_input_frames_per_second?: boolean;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Task type for the model. Default value: `\"depth\"`\n   */\n  task?: \"depth\" | \"pose\" | \"inpainting\" | \"outpainting\" | \"reframe\";\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * URL to the source video file. If provided, the model will use this video as a reference.\n   */\n  video_url: string | Blob | File;\n  /**\n   * URL to the source mask file. If provided, the model will use this mask as a reference.\n   */\n  mask_video_url?: string | Blob | File;\n  /**\n   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Urls to source reference image. If provided, the model will use this image as reference.\n   */\n  ref_image_urls?: Array<string>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Whether to preprocess the input video.\n   */\n  preprocess?: boolean;\n  /**\n   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.\n   */\n  acceleration?: \"none\" | \"regular\";\n};\nexport type WanVace14bOutpaintingInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.\n   */\n  match_input_num_frames?: boolean;\n  /**\n   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.\n   */\n  match_input_frames_per_second?: boolean;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * URL to the source video file. Required for outpainting.\n   */\n  video_url: string | Blob | File;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Whether to expand the video to the left.\n   */\n  expand_left?: boolean;\n  /**\n   * Whether to expand the video to the right.\n   */\n  expand_right?: boolean;\n  /**\n   * Whether to expand the video to the top.\n   */\n  expand_top?: boolean;\n  /**\n   * Whether to expand the video to the bottom.\n   */\n  expand_bottom?: boolean;\n  /**\n   * Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides. Default value: `0.25`\n   */\n  expand_ratio?: number;\n};\nexport type WanVace14bOutpaintingOutput = {\n  /**\n   * The generated outpainting video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace14bOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace14bPoseInput = {\n  /**\n   * The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.\n   */\n  match_input_num_frames?: boolean;\n  /**\n   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.\n   */\n  match_input_frames_per_second?: boolean;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * URL to the source video file. Required for pose task.\n   */\n  video_url: string | Blob | File;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Whether to preprocess the input video.\n   */\n  preprocess?: boolean;\n  /**\n   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.\n   */\n  acceleration?: \"none\" | \"regular\";\n};\nexport type WanVace14bPoseOutput = {\n  /**\n   * The generated pose video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVace14bReframeInput = {\n  /**\n   * The text prompt to guide video generation. Optional for reframing. Default value: `\"\"`\n   */\n  prompt?: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter. Default value: `true`\n   */\n  match_input_num_frames?: boolean;\n  /**\n   * Number of frames to generate. Must be between 81 to 241 (inclusive). Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter. Default value: `true`\n   */\n  match_input_frames_per_second?: boolean;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"auto\"`\n   */\n  aspect_ratio?: \"auto\" | \"16:9\" | \"1:1\" | \"9:16\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt. Default value: `5`\n   */\n  guidance_scale?: number;\n  /**\n   * URL to the source video file. This video will be used as a reference for the reframe task.\n   */\n  video_url: string | Blob | File;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.\n   */\n  acceleration?: \"none\" | \"regular\";\n  /**\n   * Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.\n   */\n  zoom_factor?: number;\n  /**\n   * Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.\n   */\n  temporal_downsample_factor?: number;\n  /**\n   * The model to use for frame interpolation. Options are 'rife' or 'film'. Default value: `\"film\"`\n   */\n  interpolator_model?: \"rife\" | \"film\";\n};\nexport type WanVace14bReframeOutput = {\n  /**\n   * The generated reframe video file.\n   */\n  video: File;\n  /**\n   * The prompt used for generation.\n   */\n  prompt: string;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WanVaceInput = {\n  /**\n   * The text prompt to guide video generation.\n   */\n  prompt: string;\n  /**\n   * Negative prompt for video generation. Default value: `\"bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards\"`\n   */\n  negative_prompt?: string;\n  /**\n   * Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames Default value: `81`\n   */\n  num_frames?: number;\n  /**\n   * Frames per second of the generated video. Must be between 5 to 24. Default value: `16`\n   */\n  frames_per_second?: number;\n  /**\n   * Task type for the model. Default value: `\"depth\"`\n   */\n  task?: \"depth\" | \"inpainting\";\n  /**\n   * Shift parameter for video generation. Default value: `5`\n   */\n  shift?: number;\n  /**\n   * Random seed for reproducibility. If None, a random seed is chosen.\n   */\n  seed?: number;\n  /**\n   * Resolution of the generated video (480p,580p, or 720p). Default value: `\"720p\"`\n   */\n  resolution?: \"480p\" | \"580p\" | \"720p\";\n  /**\n   * Aspect ratio of the generated video (16:9 or 9:16). Default value: `\"16:9\"`\n   */\n  aspect_ratio?: \"auto\" | \"9:16\" | \"16:9\";\n  /**\n   * Number of inference steps for sampling. Higher values give better quality but take longer. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * URL to the source video file. If provided, the model will use this video as a reference.\n   */\n  video_url?: string | Blob | File;\n  /**\n   * URL to the source mask file. If provided, the model will use this mask as a reference.\n   */\n  mask_video_url?: string | Blob | File;\n  /**\n   * URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.\n   */\n  mask_image_url?: string | Blob | File;\n  /**\n   * Urls to source reference image. If provided, the model will use this image as reference.\n   */\n  ref_image_urls?: Array<string>;\n  /**\n   * If set to true, the safety checker will be enabled.\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * Whether to enable prompt expansion.\n   */\n  enable_prompt_expansion?: boolean;\n  /**\n   * Whether to preprocess the input video.\n   */\n  preprocess?: boolean;\n};\nexport type WanVaceOutput = {\n  /**\n   * The generated video file.\n   */\n  video: File;\n  /**\n   * The seed used for generation.\n   */\n  seed: number;\n};\nexport type WaveformInput = {\n  /**\n   * URL of the audio file to analyze\n   */\n  media_url: string | Blob | File;\n  /**\n   * Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one. Default value: `4`\n   */\n  points_per_second?: number;\n  /**\n   * Number of decimal places for the waveform values. Higher values provide more precision but increase payload size. Default value: `2`\n   */\n  precision?: number;\n  /**\n   * Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number. Default value: `3`\n   */\n  smoothing_window?: number;\n};\nexport type WaveformOutput = {\n  /**\n   * Normalized waveform data as an array of values between -1 and 1. The number of points is determined by audio duration × points_per_second.\n   */\n  waveform: Array<number>;\n  /**\n   * Duration of the audio in seconds\n   */\n  duration: number;\n  /**\n   * Number of points in the waveform data\n   */\n  points: number;\n  /**\n   * Number of decimal places used in the waveform values\n   */\n  precision: number;\n};\nexport type WeatherEffectInput = {\n  /**\n   * Image prompt for the omni model.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive. Default value: `\"2\"`\n   */\n  safety_tolerance?: \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\";\n  /**\n   * The format of the generated image. Default value: `\"jpeg\"`\n   */\n  output_format?: \"jpeg\" | \"png\";\n  /**\n   * The aspect ratio of the generated image.\n   */\n  aspect_ratio?:\n    | \"21:9\"\n    | \"16:9\"\n    | \"4:3\"\n    | \"3:2\"\n    | \"1:1\"\n    | \"2:3\"\n    | \"3:4\"\n    | \"9:16\"\n    | \"9:21\";\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The weather effect to apply. Default value: `\"heavy snowfall\"`\n   */\n  prompt?: string;\n};\nexport type WeatherEffectOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type WojakStyleInput = {\n  /**\n   * URL of the image to convert to wojak style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `1`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n};\nexport type WojakStyleOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type XAilabNsfwInput = {\n  /**\n   * List of image URLs to check. If more than 10 images are provided, only the first 10 will be checked.\n   */\n  image_urls: Array<string>;\n};\nexport type XAilabNsfwOutput = {\n  /**\n   * List of booleans indicating if the image has an NSFW concept\n   */\n  has_nsfw_concepts: Array<boolean>;\n};\nexport type YouTubeThumbnailsInput = {\n  /**\n   * URL of the image to convert to YouTube thumbnail style.\n   */\n  image_url: string | Blob | File;\n  /**\n   * The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you. Default value: `3.5`\n   */\n  guidance_scale?: number;\n  /**\n   * Number of inference steps for sampling. Default value: `30`\n   */\n  num_inference_steps?: number;\n  /**\n   * Whether to enable the safety checker for the generated image. Default value: `true`\n   */\n  enable_safety_checker?: boolean;\n  /**\n   * The scale factor for the LoRA model. Controls the strength of the LoRA effect. Default value: `0.5`\n   */\n  lora_scale?: number;\n  /**\n   * The same seed and the same prompt given to the same version of the model will output the same image every time.\n   */\n  seed?: number;\n  /**\n   * If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.\n   */\n  sync_mode?: boolean;\n  /**\n   * The text to include in the YouTube thumbnail. Default value: `\"Generate youtube thumbnails\"`\n   */\n  prompt?: string;\n};\nexport type YouTubeThumbnailsOutput = {\n  /**\n   *\n   */\n  images: Array<Image>;\n  /**\n   *\n   */\n  seed: number;\n};\nexport type YueInput = {\n  /**\n   * The prompt to generate an image from. Must have two sections. Sections start with either [chorus] or a [verse].\n   */\n  lyrics: string;\n  /**\n   * The genres (separated by a space ' ') to guide the music generation.\n   */\n  genres: string;\n};\nexport type YueOutput = {\n  /**\n   * Generated music file.\n   */\n  audio: File;\n};\nexport type ZonosInput = {\n  /**\n   * The reference audio.\n   */\n  reference_audio_url: string | Blob | File;\n  /**\n   * The content generated using cloned voice.\n   */\n  prompt: string;\n};\nexport type ZonosOutput = {\n  /**\n   * The generated audio\n   */\n  audio: File;\n};\nexport type EndpointTypeMap = {\n  \"fal-ai/kling-video/v2.1/master/image-to-video\": {\n    input: KlingVideoV21MasterImageToVideoInput;\n    output: KlingVideoV21MasterImageToVideoOutput;\n  };\n  \"fal-ai/flux-pro/kontext\": {\n    input: FluxProKontextInput;\n    output: FluxProKontextOutput;\n  };\n  \"fal-ai/imagen4/preview\": {\n    input: Imagen4PreviewInput;\n    output: Imagen4PreviewOutput;\n  };\n  \"fal-ai/kling-video/v2/master/image-to-video\": {\n    input: KlingVideoV2MasterImageToVideoInput;\n    output: KlingVideoV2MasterImageToVideoOutput;\n  };\n  \"fal-ai/wan-effects\": {\n    input: WanEffectsInput;\n    output: WanEffectsOutput;\n  };\n  \"fal-ai/wan-pro/image-to-video\": {\n    input: WanProImageToVideoInput;\n    output: WanProImageToVideoOutput;\n  };\n  \"fal-ai/veo2/image-to-video\": {\n    input: Veo2ImageToVideoInput;\n    output: Veo2ImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/image-to-video\": {\n    input: KlingVideoV16ProImageToVideoInput;\n    output: KlingVideoV16ProImageToVideoOutput;\n  };\n  \"fal-ai/flux-lora-fast-training\": {\n    input: FluxLoraFastTrainingInput;\n    output: FluxLoraFastTrainingOutput;\n  };\n  \"fal-ai/playai/tts/dialog\": {\n    input: PlayaiTtsDialogInput;\n    output: PlayaiTtsDialogOutput;\n  };\n  \"fal-ai/flux-pro/v1.1-ultra\": {\n    input: FluxProV11UltraInput;\n    output: FluxProV11UltraOutput;\n  };\n  \"fal-ai/recraft/v3/text-to-image\": {\n    input: RecraftV3TextToImageInput;\n    output: RecraftV3TextToImageOutput;\n  };\n  \"fal-ai/minimax/video-01/image-to-video\": {\n    input: MinimaxVideo01ImageToVideoInput;\n    output: MinimaxVideo01ImageToVideoOutput;\n  };\n  \"fal-ai/flux-krea-trainer\": {\n    input: FluxKreaTrainerInput;\n    output: FluxKreaTrainerOutput;\n  };\n  \"fal-ai/veo3/fast\": {\n    input: Veo3FastInput;\n    output: Veo3FastOutput;\n  };\n  \"bria/video/background-removal\": {\n    input: VideoBackgroundRemovalInput;\n    output: VideoBackgroundRemovalOutput;\n  };\n  \"fal-ai/flux-kontext-trainer\": {\n    input: FluxKontextTrainerInput;\n    output: FluxKontextTrainerOutput;\n  };\n  \"fal-ai/minimax/hailuo-02/standard/image-to-video\": {\n    input: MinimaxHailuo02StandardImageToVideoInput;\n    output: MinimaxHailuo02StandardImageToVideoOutput;\n  };\n  \"fal-ai/minimax/hailuo-02/standard/text-to-video\": {\n    input: MinimaxHailuo02StandardTextToVideoInput;\n    output: MinimaxHailuo02StandardTextToVideoOutput;\n  };\n  \"bria/text-to-image/3.2\": {\n    input: TextToImage32Input;\n    output: TextToImage32Output;\n  };\n  \"fal-ai/bytedance/seedance/v1/pro/image-to-video\": {\n    input: BytedanceSeedanceV1ProImageToVideoInput;\n    output: BytedanceSeedanceV1ProImageToVideoOutput;\n  };\n  \"fal-ai/veo3\": {\n    input: Veo3Input;\n    output: Veo3Output;\n  };\n  \"fal-ai/kling-video/v2.1/standard/image-to-video\": {\n    input: KlingVideoV21StandardImageToVideoInput;\n    output: KlingVideoV21StandardImageToVideoOutput;\n  };\n  \"fal-ai/pixverse/v4.5/image-to-video\": {\n    input: PixverseV45ImageToVideoInput;\n    output: PixverseV45ImageToVideoOutput;\n  };\n  \"fal-ai/tavus/hummingbird-lipsync/v0\": {\n    input: TavusHummingbirdLipsyncV0Input;\n    output: TavusHummingbirdLipsyncV0Output;\n  };\n  \"fal-ai/kling-video/v2/master/text-to-video\": {\n    input: KlingVideoV2MasterTextToVideoInput;\n    output: KlingVideoV2MasterTextToVideoOutput;\n  };\n  \"fal-ai/hidream-i1-full\": {\n    input: HidreamI1FullInput;\n    output: any;\n  };\n  \"fal-ai/hidream-i1-dev\": {\n    input: HidreamI1DevInput;\n    output: HidreamI1DevOutput;\n  };\n  \"fal-ai/hidream-i1-fast\": {\n    input: HidreamI1FastInput;\n    output: HidreamI1FastOutput;\n  };\n  \"fal-ai/flux/dev\": {\n    input: FluxDevInput;\n    output: FluxDevOutput;\n  };\n  \"fal-ai/wan-i2v\": {\n    input: WanI2vInput;\n    output: WanI2vOutput;\n  };\n  \"fal-ai/mmaudio-v2\": {\n    input: MmaudioV2Input;\n    output: MmaudioV2Output;\n  };\n  \"fal-ai/ideogram/v2\": {\n    input: IdeogramV2Input;\n    output: IdeogramV2Output;\n  };\n  \"fal-ai/flux-lora-portrait-trainer\": {\n    input: FluxLoraPortraitTrainerInput;\n    output: FluxLoraPortraitTrainerOutput;\n  };\n  \"fal-ai/stable-diffusion-v35-large\": {\n    input: StableDiffusionV35LargeInput;\n    output: StableDiffusionV35LargeOutput;\n  };\n  \"fal-ai/flux-general\": {\n    input: FluxGeneralInput;\n    output: FluxGeneralOutput;\n  };\n  \"fal-ai/flux-lora\": {\n    input: FluxLoraInput;\n    output: FluxLoraOutput;\n  };\n  \"fal-ai/flux/dev/image-to-image\": {\n    input: FluxDevImageToImageInput;\n    output: FluxDevImageToImageOutput;\n  };\n  \"fal-ai/aura-sr\": {\n    input: AuraSrInput;\n    output: AuraSrOutput;\n  };\n  \"fal-ai/clarity-upscaler\": {\n    input: ClarityUpscalerInput;\n    output: ClarityUpscalerOutput;\n  };\n  \"fal-ai/ideogram/character/edit\": {\n    input: IdeogramCharacterEditInput;\n    output: IdeogramCharacterEditOutput;\n  };\n  \"fal-ai/ideogram/character\": {\n    input: IdeogramCharacterInput;\n    output: IdeogramCharacterOutput;\n  };\n  \"fal-ai/ideogram/character/remix\": {\n    input: IdeogramCharacterRemixInput;\n    output: IdeogramCharacterRemixOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/text-to-video/lora\": {\n    input: WanV22A14bTextToVideoLoraInput;\n    output: WanV22A14bTextToVideoLoraOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/image-to-video/lora\": {\n    input: WanV22A14bImageToVideoLoraInput;\n    output: WanV22A14bImageToVideoLoraOutput;\n  };\n  \"fal-ai/wan/v2.2-5b/text-to-video/distill\": {\n    input: WanV225bTextToVideoDistillInput;\n    output: WanV225bTextToVideoDistillOutput;\n  };\n  \"fal-ai/minimax/hailuo-02-fast/image-to-video\": {\n    input: MinimaxHailuo02FastImageToVideoInput;\n    output: MinimaxHailuo02FastImageToVideoOutput;\n  };\n  \"fal-ai/bytedance/dreamina/v3.1/text-to-image\": {\n    input: BytedanceDreaminaV31TextToImageInput;\n    output: BytedanceDreaminaV31TextToImageOutput;\n  };\n  \"fal-ai/wan/v2.2-5b/text-to-video/fast-wan\": {\n    input: WanV225bTextToVideoFastWanInput;\n    output: WanV225bTextToVideoFastWanOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/text-to-image/lora\": {\n    input: WanV22A14bTextToImageLoraInput;\n    output: WanV22A14bTextToImageLoraOutput;\n  };\n  \"fal-ai/wan/v2.2-5b/text-to-image\": {\n    input: WanV225bTextToImageInput;\n    output: WanV225bTextToImageOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/text-to-image\": {\n    input: WanV22A14bTextToImageInput;\n    output: WanV22A14bTextToImageOutput;\n  };\n  \"fal-ai/qwen-image\": {\n    input: QwenImageInput;\n    output: QwenImageOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/video-to-video\": {\n    input: WanV22A14bVideoToVideoInput;\n    output: WanV22A14bVideoToVideoOutput;\n  };\n  \"fal-ai/flux-krea-lora/stream\": {\n    input: FluxKreaLoraStreamInput;\n    output: FluxKreaLoraStreamOutput;\n  };\n  \"fal-ai/flux-krea-lora/inpainting\": {\n    input: FluxKreaLoraInpaintingInput;\n    output: FluxKreaLoraInpaintingOutput;\n  };\n  \"fal-ai/flux-krea-lora\": {\n    input: FluxKreaLoraInput;\n    output: FluxKreaLoraOutput;\n  };\n  \"fal-ai/flux-krea-lora/image-to-image\": {\n    input: FluxKreaLoraImageToImageInput;\n    output: FluxKreaLoraImageToImageOutput;\n  };\n  \"easel-ai/fashion-tryon\": {\n    input: FashionTryonInput;\n    output: FashionTryonOutput;\n  };\n  \"fal-ai/veo3/image-to-video\": {\n    input: Veo3ImageToVideoInput;\n    output: Veo3ImageToVideoOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/image-to-video/turbo\": {\n    input: WanV22A14bImageToVideoTurboInput;\n    output: WanV22A14bImageToVideoTurboOutput;\n  };\n  \"fal-ai/wan/v2.2-a14b/text-to-video/turbo\": {\n    input: WanV22A14bTextToVideoTurboInput;\n    output: WanV22A14bTextToVideoTurboOutput;\n  };\n  \"fal-ai/flux/krea/image-to-image\": {\n    input: FluxKreaImageToImageInput;\n    output: FluxKreaImageToImageOutput;\n  };\n  \"fal-ai/flux/krea/redux\": {\n    input: FluxKreaReduxInput;\n    output: FluxKreaReduxOutput;\n  };\n  \"fal-ai/flux/krea\": {\n    input: FluxKreaInput;\n    output: FluxKreaOutput;\n  };\n  \"fal-ai/flux-1/krea/image-to-image\": {\n    input: Flux1KreaImageToImageInput;\n    output: Flux1KreaImageToImageOutput;\n  };\n  \"fal-ai/flux-1/krea/redux\": {\n    input: Flux1KreaReduxInput;\n    output: Flux1KreaReduxOutput;\n  };\n  \"fal-ai/flux-1/krea\": {\n    input: Flux1KreaInput;\n    output: Flux1KreaOutput;\n  };\n  \"fal-ai/wan/v2.2-5b/image-to-video\": {\n    input: WanV225bImageToVideoInput;\n    output: WanV225bImageToVideoOutput;\n  };\n  \"fal-ai/flux-kontext-lora/inpaint\": {\n    input: FluxKontextLoraInpaintInput;\n    output: FluxKontextLoraInpaintOutput;\n  };\n  \"fal-ai/wan/v2.2-5b/text-to-video\": {\n    input: WanV225bTextToVideoInput;\n    output: WanV225bTextToVideoOutput;\n  };\n  \"fal-ai/wan/v2.2-a14B/image-to-video\": {\n    input: any;\n    output: any;\n  };\n  \"fal-ai/wan/v2.2-a14b/text-to-video\": {\n    input: WanV22A14bTextToVideoInput;\n    output: WanV22A14bTextToVideoOutput;\n  };\n  \"fal-ai/hunyuan_world\": {\n    input: Hunyuan_worldInput;\n    output: Hunyuan_worldOutput;\n  };\n  \"fal-ai/hunyuan_world/image-to-world\": {\n    input: Hunyuan_worldImageToWorldInput;\n    output: Hunyuan_worldImageToWorldOutput;\n  };\n  \"fal-ai/x-ailab/nsfw\": {\n    input: XAilabNsfwInput;\n    output: XAilabNsfwOutput;\n  };\n  \"fal-ai/bytedance/omnihuman\": {\n    input: BytedanceOmnihumanInput;\n    output: BytedanceOmnihumanOutput;\n  };\n  \"fal-ai/sky-raccoon\": {\n    input: SkyRaccoonInput;\n    output: SkyRaccoonOutput;\n  };\n  \"fal-ai/image-editing/retouch\": {\n    input: ImageEditingRetouchInput;\n    output: ImageEditingRetouchOutput;\n  };\n  \"fal-ai/hidream-e1-1\": {\n    input: HidreamE11Input;\n    output: HidreamE11Output;\n  };\n  \"fal-ai/ltxv-13b-098-distilled/extend\": {\n    input: Ltxv13b098DistilledExtendInput;\n    output: Ltxv13b098DistilledExtendOutput;\n  };\n  \"fal-ai/rife/video\": {\n    input: RifeVideoInput;\n    output: RifeVideoOutput;\n  };\n  \"fal-ai/rife\": {\n    input: RifeInput;\n    output: RifeOutput;\n  };\n  \"fal-ai/film/video\": {\n    input: FilmVideoInput;\n    output: FilmVideoOutput;\n  };\n  \"fal-ai/film\": {\n    input: FilmInput;\n    output: FilmOutput;\n  };\n  \"fal-ai/minimax/voice-design\": {\n    input: MinimaxVoiceDesignInput;\n    output: MinimaxVoiceDesignOutput;\n  };\n  \"creatify/lipsync\": {\n    input: LipsyncInput;\n    output: LipsyncOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2-flash/modify\": {\n    input: LumaDreamMachineRay2FlashModifyInput;\n    output: LumaDreamMachineRay2FlashModifyOutput;\n  };\n  \"fal-ai/ltxv-13b-098-distilled/image-to-video\": {\n    input: Ltxv13b098DistilledImageToVideoInput;\n    output: Ltxv13b098DistilledImageToVideoOutput;\n  };\n  \"fal-ai/ltxv-13b-098-distilled\": {\n    input: Ltxv13b098DistilledInput;\n    output: Ltxv13b098DistilledOutput;\n  };\n  \"fal-ai/ltxv-13b-098-distilled/multiconditioning\": {\n    input: Ltxv13b098DistilledMulticonditioningInput;\n    output: Ltxv13b098DistilledMulticonditioningOutput;\n  };\n  \"fal-ai/any-llm/enterprise\": {\n    input: AnyLlmEnterpriseInput;\n    output: AnyLlmEnterpriseOutput;\n  };\n  \"easel-ai/fashion-photoshoot\": {\n    input: FashionPhotoshootInput;\n    output: FashionPhotoshootOutput;\n  };\n  \"fal-ai/calligrapher\": {\n    input: CalligrapherInput;\n    output: CalligrapherOutput;\n  };\n  \"fal-ai/veo3/fast/image-to-video\": {\n    input: Veo3FastImageToVideoInput;\n    output: Veo3FastImageToVideoOutput;\n  };\n  \"fal-ai/ffmpeg-api/loudnorm\": {\n    input: FfmpegApiLoudnormInput;\n    output: FfmpegApiLoudnormOutput;\n  };\n  \"fal-ai/vidu/q1/reference-to-video\": {\n    input: ViduQ1ReferenceToVideoInput;\n    output: ViduQ1ReferenceToVideoOutput;\n  };\n  \"fal-ai/bria/reimagine\": {\n    input: BriaReimagineInput;\n    output: BriaReimagineOutput;\n  };\n  \"fal-ai/pixverse/sound-effects\": {\n    input: PixverseSoundEffectsInput;\n    output: PixverseSoundEffectsOutput;\n  };\n  \"fal-ai/image-editing/realism\": {\n    input: ImageEditingRealismInput;\n    output: ImageEditingRealismOutput;\n  };\n  \"fal-ai/thinksound/audio\": {\n    input: ThinksoundAudioInput;\n    output: ThinksoundAudioOutput;\n  };\n  \"fal-ai/thinksound\": {\n    input: ThinksoundInput;\n    output: ThinksoundOutput;\n  };\n  \"fal-ai/post-processing/vignette\": {\n    input: PostProcessingVignetteInput;\n    output: PostProcessingVignetteOutput;\n  };\n  \"fal-ai/post-processing/solarize\": {\n    input: PostProcessingSolarizeInput;\n    output: PostProcessingSolarizeOutput;\n  };\n  \"fal-ai/post-processing/sharpen\": {\n    input: PostProcessingSharpenInput;\n    output: PostProcessingSharpenOutput;\n  };\n  \"fal-ai/post-processing/parabolize\": {\n    input: PostProcessingParabolizeInput;\n    output: PostProcessingParabolizeOutput;\n  };\n  \"fal-ai/post-processing/grain\": {\n    input: PostProcessingGrainInput;\n    output: PostProcessingGrainOutput;\n  };\n  \"fal-ai/post-processing/dodge-burn\": {\n    input: PostProcessingDodgeBurnInput;\n    output: PostProcessingDodgeBurnOutput;\n  };\n  \"fal-ai/post-processing/dissolve\": {\n    input: PostProcessingDissolveInput;\n    output: PostProcessingDissolveOutput;\n  };\n  \"fal-ai/post-processing/desaturate\": {\n    input: PostProcessingDesaturateInput;\n    output: PostProcessingDesaturateOutput;\n  };\n  \"fal-ai/post-processing/color-tint\": {\n    input: PostProcessingColorTintInput;\n    output: PostProcessingColorTintOutput;\n  };\n  \"fal-ai/post-processing/color-correction\": {\n    input: PostProcessingColorCorrectionInput;\n    output: PostProcessingColorCorrectionOutput;\n  };\n  \"fal-ai/post-processing/chromatic-aberration\": {\n    input: PostProcessingChromaticAberrationInput;\n    output: PostProcessingChromaticAberrationOutput;\n  };\n  \"fal-ai/post-processing/blur\": {\n    input: PostProcessingBlurInput;\n    output: PostProcessingBlurOutput;\n  };\n  \"fal-ai/pixverse/extend/fast\": {\n    input: PixverseExtendFastInput;\n    output: PixverseExtendFastOutput;\n  };\n  \"fal-ai/pixverse/extend\": {\n    input: PixverseExtendInput;\n    output: PixverseExtendOutput;\n  };\n  \"fal-ai/pixverse/lipsync\": {\n    input: PixverseLipsyncInput;\n    output: PixverseLipsyncOutput;\n  };\n  \"fal-ai/image-editing/youtube-thumbnails\": {\n    input: ImageEditingYoutubeThumbnailsInput;\n    output: ImageEditingYoutubeThumbnailsOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2/modify\": {\n    input: LumaDreamMachineRay2ModifyInput;\n    output: LumaDreamMachineRay2ModifyOutput;\n  };\n  \"fal-ai/bytedance/seededit/v3/edit-image\": {\n    input: BytedanceSeededitV3EditImageInput;\n    output: BytedanceSeededitV3EditImageOutput;\n  };\n  \"fal-ai/image-editing/broccoli-haircut\": {\n    input: ImageEditingBroccoliHaircutInput;\n    output: ImageEditingBroccoliHaircutOutput;\n  };\n  \"fal-ai/image-editing/wojak-style\": {\n    input: ImageEditingWojakStyleInput;\n    output: ImageEditingWojakStyleOutput;\n  };\n  \"fal-ai/image-editing/plushie-style\": {\n    input: ImageEditingPlushieStyleInput;\n    output: ImageEditingPlushieStyleOutput;\n  };\n  \"fal-ai/flux-kontext-lora/text-to-image\": {\n    input: FluxKontextLoraTextToImageInput;\n    output: FluxKontextLoraTextToImageOutput;\n  };\n  \"fal-ai/flux-kontext-lora\": {\n    input: FluxKontextLoraInput;\n    output: FluxKontextLoraOutput;\n  };\n  \"fal-ai/omnigen-v2\": {\n    input: OmnigenV2Input;\n    output: OmnigenV2Output;\n  };\n  \"fal-ai/fashn/tryon/v1.6\": {\n    input: FashnTryonV16Input;\n    output: FashnTryonV16Output;\n  };\n  \"fal-ai/ai-avatar/single-text\": {\n    input: AiAvatarSingleTextInput;\n    output: AiAvatarSingleTextOutput;\n  };\n  \"fal-ai/ai-avatar\": {\n    input: AiAvatarInput;\n    output: AiAvatarOutput;\n  };\n  \"fal-ai/ai-avatar/multi-text\": {\n    input: AiAvatarMultiTextInput;\n    output: AiAvatarMultiTextOutput;\n  };\n  \"fal-ai/ai-avatar/multi\": {\n    input: AiAvatarMultiInput;\n    output: AiAvatarMultiOutput;\n  };\n  \"fal-ai/video-understanding\": {\n    input: VideoUnderstandingInput;\n    output: VideoUnderstandingOutput;\n  };\n  \"fal-ai/wan-vace-14b/reframe\": {\n    input: WanVace14bReframeInput;\n    output: WanVace14bReframeOutput;\n  };\n  \"fal-ai/wan-vace-14b/outpainting\": {\n    input: WanVace14bOutpaintingInput;\n    output: WanVace14bOutpaintingOutput;\n  };\n  \"fal-ai/wan-vace-14b/inpainting\": {\n    input: WanVace14bInpaintingInput;\n    output: WanVace14bInpaintingOutput;\n  };\n  \"fal-ai/wan-vace-14b/pose\": {\n    input: WanVace14bPoseInput;\n    output: WanVace14bPoseOutput;\n  };\n  \"fal-ai/wan-vace-14b/depth\": {\n    input: WanVace14bDepthInput;\n    output: WanVace14bDepthOutput;\n  };\n  \"fal-ai/chain-of-zoom\": {\n    input: ChainOfZoomInput;\n    output: ChainOfZoomOutput;\n  };\n  \"tripo3d/tripo/v2.5/multiview-to-3d\": {\n    input: TripoV25MultiviewTo3dInput;\n    output: TripoV25MultiviewTo3dOutput;\n  };\n  \"fal-ai/minimax/hailuo-02/pro/image-to-video\": {\n    input: MinimaxHailuo02ProImageToVideoInput;\n    output: MinimaxHailuo02ProImageToVideoOutput;\n  };\n  \"fal-ai/minimax/hailuo-02/pro/text-to-video\": {\n    input: MinimaxHailuo02ProTextToVideoInput;\n    output: MinimaxHailuo02ProTextToVideoOutput;\n  };\n  \"fal-ai/pasd\": {\n    input: PasdInput;\n    output: PasdOutput;\n  };\n  \"fal-ai/object-removal/bbox\": {\n    input: ObjectRemovalBboxInput;\n    output: ObjectRemovalBboxOutput;\n  };\n  \"fal-ai/object-removal/mask\": {\n    input: ObjectRemovalMaskInput;\n    output: ObjectRemovalMaskOutput;\n  };\n  \"fal-ai/object-removal\": {\n    input: ObjectRemovalInput;\n    output: ObjectRemovalOutput;\n  };\n  \"fal-ai/bytedance/seedance/v1/pro/text-to-video\": {\n    input: BytedanceSeedanceV1ProTextToVideoInput;\n    output: BytedanceSeedanceV1ProTextToVideoOutput;\n  };\n  \"fal-ai/hunyuan3d-v21\": {\n    input: Hunyuan3dV21Input;\n    output: Hunyuan3dV21Output;\n  };\n  \"fal-ai/bytedance/seedance/v1/lite/image-to-video\": {\n    input: BytedanceSeedanceV1LiteImageToVideoInput;\n    output: BytedanceSeedanceV1LiteImageToVideoOutput;\n  };\n  \"fal-ai/bytedance/seedance/v1/lite/text-to-video\": {\n    input: BytedanceSeedanceV1LiteTextToVideoInput;\n    output: BytedanceSeedanceV1LiteTextToVideoOutput;\n  };\n  \"fal-ai/recraft/vectorize\": {\n    input: RecraftVectorizeInput;\n    output: RecraftVectorizeOutput;\n  };\n  \"fal-ai/wan-trainer/t2v\": {\n    input: WanTrainerT2vInput;\n    output: WanTrainerT2vOutput;\n  };\n  \"fal-ai/wan-trainer/t2v-14b\": {\n    input: WanTrainerT2v14bInput;\n    output: WanTrainerT2v14bOutput;\n  };\n  \"fal-ai/wan-trainer/i2v-720p\": {\n    input: WanTrainerI2v720pInput;\n    output: WanTrainerI2v720pOutput;\n  };\n  \"fal-ai/wan-trainer/flf2v-720p\": {\n    input: WanTrainerFlf2v720pInput;\n    output: WanTrainerFlf2v720pOutput;\n  };\n  \"fal-ai/bytedance/seedream/v3/text-to-image\": {\n    input: BytedanceSeedreamV3TextToImageInput;\n    output: BytedanceSeedreamV3TextToImageOutput;\n  };\n  \"fal-ai/ffmpeg-api/extract-frame\": {\n    input: FfmpegApiExtractFrameInput;\n    output: FfmpegApiExtractFrameOutput;\n  };\n  \"fal-ai/ffmpeg-api/merge-audio-video\": {\n    input: FfmpegApiMergeAudioVideoInput;\n    output: FfmpegApiMergeAudioVideoOutput;\n  };\n  \"fal-ai/luma-photon/flash/modify\": {\n    input: LumaPhotonFlashModifyInput;\n    output: LumaPhotonFlashModifyOutput;\n  };\n  \"fal-ai/luma-photon/modify\": {\n    input: LumaPhotonModifyInput;\n    output: LumaPhotonModifyOutput;\n  };\n  \"fal-ai/image-editing/reframe\": {\n    input: ImageEditingReframeInput;\n    output: ImageEditingReframeOutput;\n  };\n  \"fal-ai/wan-vace-1-3b\": {\n    input: WanVace13bInput;\n    output: WanVace13bOutput;\n  };\n  \"fal-ai/image-editing/baby-version\": {\n    input: ImageEditingBabyVersionInput;\n    output: ImageEditingBabyVersionOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2-flash/reframe\": {\n    input: LumaDreamMachineRay2FlashReframeInput;\n    output: LumaDreamMachineRay2FlashReframeOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2/reframe\": {\n    input: LumaDreamMachineRay2ReframeInput;\n    output: LumaDreamMachineRay2ReframeOutput;\n  };\n  \"fal-ai/luma-photon/flash/reframe\": {\n    input: LumaPhotonFlashReframeInput;\n    output: LumaPhotonFlashReframeOutput;\n  };\n  \"fal-ai/luma-photon/reframe\": {\n    input: LumaPhotonReframeInput;\n    output: LumaPhotonReframeOutput;\n  };\n  \"resemble-ai/chatterboxhd/speech-to-speech\": {\n    input: ChatterboxhdSpeechToSpeechInput;\n    output: ChatterboxhdSpeechToSpeechOutput;\n  };\n  \"resemble-ai/chatterboxhd/text-to-speech\": {\n    input: ChatterboxhdTextToSpeechInput;\n    output: ChatterboxhdTextToSpeechOutput;\n  };\n  \"fal-ai/flux-1/schnell/redux\": {\n    input: Flux1SchnellReduxInput;\n    output: Flux1SchnellReduxOutput;\n  };\n  \"fal-ai/flux-1/dev/redux\": {\n    input: Flux1DevReduxInput;\n    output: Flux1DevReduxOutput;\n  };\n  \"fal-ai/flux-1/dev/image-to-image\": {\n    input: Flux1DevImageToImageInput;\n    output: Flux1DevImageToImageOutput;\n  };\n  \"fal-ai/flux-1/schnell\": {\n    input: Flux1SchnellInput;\n    output: Flux1SchnellOutput;\n  };\n  \"fal-ai/flux-1/dev\": {\n    input: Flux1DevInput;\n    output: Flux1DevOutput;\n  };\n  \"fal-ai/playai/inpaint/diffusion\": {\n    input: PlayaiInpaintDiffusionInput;\n    output: PlayaiInpaintDiffusionOutput;\n  };\n  \"fal-ai/image-editing/text-removal\": {\n    input: ImageEditingTextRemovalInput;\n    output: ImageEditingTextRemovalOutput;\n  };\n  \"fal-ai/image-editing/photo-restoration\": {\n    input: ImageEditingPhotoRestorationInput;\n    output: ImageEditingPhotoRestorationOutput;\n  };\n  \"fal-ai/chatterbox/speech-to-speech\": {\n    input: ChatterboxSpeechToSpeechInput;\n    output: ChatterboxSpeechToSpeechOutput;\n  };\n  \"fal-ai/chatterbox/text-to-speech\": {\n    input: ChatterboxTextToSpeechInput;\n    output: any;\n  };\n  \"fal-ai/image-editing/weather-effect\": {\n    input: ImageEditingWeatherEffectInput;\n    output: ImageEditingWeatherEffectOutput;\n  };\n  \"fal-ai/image-editing/time-of-day\": {\n    input: ImageEditingTimeOfDayInput;\n    output: ImageEditingTimeOfDayOutput;\n  };\n  \"fal-ai/image-editing/style-transfer\": {\n    input: ImageEditingStyleTransferInput;\n    output: ImageEditingStyleTransferOutput;\n  };\n  \"fal-ai/image-editing/scene-composition\": {\n    input: ImageEditingSceneCompositionInput;\n    output: ImageEditingSceneCompositionOutput;\n  };\n  \"fal-ai/image-editing/professional-photo\": {\n    input: ImageEditingProfessionalPhotoInput;\n    output: ImageEditingProfessionalPhotoOutput;\n  };\n  \"fal-ai/image-editing/object-removal\": {\n    input: ImageEditingObjectRemovalInput;\n    output: ImageEditingObjectRemovalOutput;\n  };\n  \"fal-ai/image-editing/hair-change\": {\n    input: ImageEditingHairChangeInput;\n    output: ImageEditingHairChangeOutput;\n  };\n  \"fal-ai/image-editing/face-enhancement\": {\n    input: ImageEditingFaceEnhancementInput;\n    output: ImageEditingFaceEnhancementOutput;\n  };\n  \"fal-ai/image-editing/expression-change\": {\n    input: ImageEditingExpressionChangeInput;\n    output: ImageEditingExpressionChangeOutput;\n  };\n  \"fal-ai/image-editing/color-correction\": {\n    input: ImageEditingColorCorrectionInput;\n    output: ImageEditingColorCorrectionOutput;\n  };\n  \"fal-ai/image-editing/cartoonify\": {\n    input: ImageEditingCartoonifyInput;\n    output: ImageEditingCartoonifyOutput;\n  };\n  \"fal-ai/image-editing/background-change\": {\n    input: ImageEditingBackgroundChangeInput;\n    output: ImageEditingBackgroundChangeOutput;\n  };\n  \"fal-ai/image-editing/age-progression\": {\n    input: ImageEditingAgeProgressionInput;\n    output: ImageEditingAgeProgressionOutput;\n  };\n  \"fal-ai/flux-pro/kontext/max/multi\": {\n    input: FluxProKontextMaxMultiInput;\n    output: FluxProKontextMaxMultiOutput;\n  };\n  \"fal-ai/flux-pro/kontext/multi\": {\n    input: FluxProKontextMultiInput;\n    output: FluxProKontextMultiOutput;\n  };\n  \"fal-ai/hunyuan-avatar\": {\n    input: HunyuanAvatarInput;\n    output: HunyuanAvatarOutput;\n  };\n  \"fal-ai/flux-pro/kontext/max\": {\n    input: FluxProKontextMaxInput;\n    output: FluxProKontextMaxOutput;\n  };\n  \"fal-ai/flux-pro/kontext/max/text-to-image\": {\n    input: FluxProKontextMaxTextToImageInput;\n    output: FluxProKontextMaxTextToImageOutput;\n  };\n  \"fal-ai/kling-video/v2.1/master/text-to-video\": {\n    input: KlingVideoV21MasterTextToVideoInput;\n    output: KlingVideoV21MasterTextToVideoOutput;\n  };\n  \"fal-ai/kling-video/v2.1/pro/image-to-video\": {\n    input: KlingVideoV21ProImageToVideoInput;\n    output: KlingVideoV21ProImageToVideoOutput;\n  };\n  \"fal-ai/flux-pro/kontext/text-to-image\": {\n    input: FluxProKontextTextToImageInput;\n    output: FluxProKontextTextToImageOutput;\n  };\n  \"fal-ai/flux-kontext/dev\": {\n    input: FluxKontextDevInput;\n    output: FluxKontextDevOutput;\n  };\n  \"veed/lipsync\": {\n    input: LipsyncInput;\n    output: LipsyncOutput;\n  };\n  \"veed/avatars/text-to-video\": {\n    input: AvatarsTextToVideoInput;\n    output: AvatarsTextToVideoOutput;\n  };\n  \"veed/avatars/audio-to-video\": {\n    input: AvatarsAudioToVideoInput;\n    output: AvatarsAudioToVideoOutput;\n  };\n  \"fal-ai/hunyuan-portrait\": {\n    input: HunyuanPortraitInput;\n    output: HunyuanPortraitOutput;\n  };\n  \"fal-ai/wan-vace-14b\": {\n    input: WanVace14bInput;\n    output: WanVace14bOutput;\n  };\n  \"fal-ai/bagel/understand\": {\n    input: BagelUnderstandInput;\n    output: BagelUnderstandOutput;\n  };\n  \"fal-ai/bagel/edit\": {\n    input: BagelEditInput;\n    output: BagelEditOutput;\n  };\n  \"fal-ai/bagel\": {\n    input: BagelInput;\n    output: BagelOutput;\n  };\n  \"fal-ai/lyria2\": {\n    input: Lyria2Input;\n    output: Lyria2Output;\n  };\n  \"fal-ai/imagen4/preview/ultra\": {\n    input: Imagen4PreviewUltraInput;\n    output: Imagen4PreviewUltraOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/elements\": {\n    input: KlingVideoV16StandardElementsInput;\n    output: KlingVideoV16StandardElementsOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/elements\": {\n    input: KlingVideoV16ProElementsInput;\n    output: KlingVideoV16ProElementsOutput;\n  };\n  \"fal-ai/dreamo\": {\n    input: DreamoInput;\n    output: DreamoOutput;\n  };\n  \"fal-ai/ltx-video-13b-distilled/extend\": {\n    input: LtxVideo13bDistilledExtendInput;\n    output: LtxVideo13bDistilledExtendOutput;\n  };\n  \"fal-ai/ltx-video-13b-distilled/multiconditioning\": {\n    input: LtxVideo13bDistilledMulticonditioningInput;\n    output: LtxVideo13bDistilledMulticonditioningOutput;\n  };\n  \"fal-ai/ltx-video-13b-distilled/image-to-video\": {\n    input: LtxVideo13bDistilledImageToVideoInput;\n    output: LtxVideo13bDistilledImageToVideoOutput;\n  };\n  \"fal-ai/ltx-video-13b-dev/multiconditioning\": {\n    input: LtxVideo13bDevMulticonditioningInput;\n    output: LtxVideo13bDevMulticonditioningOutput;\n  };\n  \"fal-ai/ltx-video-13b-dev/extend\": {\n    input: LtxVideo13bDevExtendInput;\n    output: LtxVideo13bDevExtendOutput;\n  };\n  \"fal-ai/ltx-video-13b-dev/image-to-video\": {\n    input: LtxVideo13bDevImageToVideoInput;\n    output: LtxVideo13bDevImageToVideoOutput;\n  };\n  \"fal-ai/ltx-video-13b-dev\": {\n    input: LtxVideo13bDevInput;\n    output: LtxVideo13bDevOutput;\n  };\n  \"fal-ai/ltx-video-13b-distilled\": {\n    input: LtxVideo13bDistilledInput;\n    output: LtxVideo13bDistilledOutput;\n  };\n  \"easel-ai/easel-gifswap\": {\n    input: EaselGifswapInput;\n    output: EaselGifswapOutput;\n  };\n  \"fal-ai/flux-lora/stream\": {\n    input: FluxLoraStreamInput;\n    output: FluxLoraStreamOutput;\n  };\n  \"fal-ai/ltx-video-lora/multiconditioning\": {\n    input: LtxVideoLoraMulticonditioningInput;\n    output: LtxVideoLoraMulticonditioningOutput;\n  };\n  \"fal-ai/ltx-video-lora/image-to-video\": {\n    input: LtxVideoLoraImageToVideoInput;\n    output: LtxVideoLoraImageToVideoOutput;\n  };\n  \"fal-ai/ltx-video-lora\": {\n    input: LtxVideoLoraInput;\n    output: LtxVideoLoraOutput;\n  };\n  \"fal-ai/pixverse/v4.5/transition\": {\n    input: PixverseV45TransitionInput;\n    output: PixverseV45TransitionOutput;\n  };\n  \"fal-ai/pixverse/v4.5/image-to-video/fast\": {\n    input: PixverseV45ImageToVideoFastInput;\n    output: PixverseV45ImageToVideoFastOutput;\n  };\n  \"fal-ai/pixverse/v4.5/text-to-video/fast\": {\n    input: PixverseV45TextToVideoFastInput;\n    output: PixverseV45TextToVideoFastOutput;\n  };\n  \"fal-ai/pixverse/v4.5/text-to-video\": {\n    input: PixverseV45TextToVideoInput;\n    output: PixverseV45TextToVideoOutput;\n  };\n  \"fal-ai/pixverse/v4.5/effects\": {\n    input: PixverseV45EffectsInput;\n    output: PixverseV45EffectsOutput;\n  };\n  \"fal-ai/hunyuan-custom\": {\n    input: HunyuanCustomInput;\n    output: HunyuanCustomOutput;\n  };\n  \"fal-ai/framepack/f1\": {\n    input: FramepackF1Input;\n    output: FramepackF1Output;\n  };\n  \"fal-ai/ace-step/audio-outpaint\": {\n    input: AceStepAudioOutpaintInput;\n    output: AceStepAudioOutpaintOutput;\n  };\n  \"fal-ai/ace-step/audio-inpaint\": {\n    input: AceStepAudioInpaintInput;\n    output: AceStepAudioInpaintOutput;\n  };\n  \"fal-ai/ace-step/audio-to-audio\": {\n    input: AceStepAudioToAudioInput;\n    output: AceStepAudioToAudioOutput;\n  };\n  \"fal-ai/ace-step/prompt-to-audio\": {\n    input: AceStepPromptToAudioInput;\n    output: AceStepPromptToAudioOutput;\n  };\n  \"smoretalk-ai/rembg-enhance\": {\n    input: RembgEnhanceInput;\n    output: RembgEnhanceOutput;\n  };\n  \"fal-ai/vidu/q1/start-end-to-video\": {\n    input: ViduQ1StartEndToVideoInput;\n    output: ViduQ1StartEndToVideoOutput;\n  };\n  \"fal-ai/vidu/q1/text-to-video\": {\n    input: ViduQ1TextToVideoInput;\n    output: ViduQ1TextToVideoOutput;\n  };\n  \"fal-ai/vidu/q1/image-to-video\": {\n    input: ViduQ1ImageToVideoInput;\n    output: ViduQ1ImageToVideoOutput;\n  };\n  \"fal-ai/ace-step\": {\n    input: AceStepInput;\n    output: AceStepOutput;\n  };\n  \"fal-ai/ltx-video-trainer\": {\n    input: LtxVideoTrainerInput;\n    output: LtxVideoTrainerOutput;\n  };\n  \"fal-ai/recraft/upscale/creative\": {\n    input: RecraftUpscaleCreativeInput;\n    output: RecraftUpscaleCreativeOutput;\n  };\n  \"fal-ai/recraft/upscale/crisp\": {\n    input: RecraftUpscaleCrispInput;\n    output: RecraftUpscaleCrispOutput;\n  };\n  \"fal-ai/recraft/v3/create-style\": {\n    input: RecraftV3CreateStyleInput;\n    output: RecraftV3CreateStyleOutput;\n  };\n  \"fal-ai/recraft/v3/image-to-image\": {\n    input: RecraftV3ImageToImageInput;\n    output: RecraftV3ImageToImageOutput;\n  };\n  \"fal-ai/ltx-video-v097/extend\": {\n    input: LtxVideoV097ExtendInput;\n    output: LtxVideoV097ExtendOutput;\n  };\n  \"easel-ai/easel-avatar\": {\n    input: EaselAvatarInput;\n    output: EaselAvatarOutput;\n  };\n  \"fal-ai/minimax/voice-clone\": {\n    input: MinimaxVoiceCloneInput;\n    output: MinimaxVoiceCloneOutput;\n  };\n  \"fal-ai/minimax/speech-02-turbo\": {\n    input: MinimaxSpeech02TurboInput;\n    output: MinimaxSpeech02TurboOutput;\n  };\n  \"fal-ai/ltx-video-v097/multiconditioning\": {\n    input: LtxVideoV097MulticonditioningInput;\n    output: LtxVideoV097MulticonditioningOutput;\n  };\n  \"fal-ai/minimax/speech-02-hd\": {\n    input: MinimaxSpeech02HdInput;\n    output: MinimaxSpeech02HdOutput;\n  };\n  \"fal-ai/ltx-video-v097\": {\n    input: LtxVideoV097Input;\n    output: LtxVideoV097Output;\n  };\n  \"fal-ai/ltx-video-v097/image-to-video\": {\n    input: LtxVideoV097ImageToVideoInput;\n    output: LtxVideoV097ImageToVideoOutput;\n  };\n  \"fal-ai/minimax/image-01/subject-reference\": {\n    input: MinimaxImage01SubjectReferenceInput;\n    output: MinimaxImage01SubjectReferenceOutput;\n  };\n  \"fal-ai/minimax/image-01\": {\n    input: MinimaxImage01Input;\n    output: MinimaxImage01Output;\n  };\n  \"fal-ai/hidream-i1-full/image-to-image\": {\n    input: HidreamI1FullImageToImageInput;\n    output: HidreamI1FullImageToImageOutput;\n  };\n  \"fal-ai/trellis/multi\": {\n    input: TrellisMultiInput;\n    output: TrellisMultiOutput;\n  };\n  \"fal-ai/ideogram/v3/reframe\": {\n    input: IdeogramV3ReframeInput;\n    output: IdeogramV3ReframeOutput;\n  };\n  \"fal-ai/ideogram/v3\": {\n    input: IdeogramV3Input;\n    output: IdeogramV3Output;\n  };\n  \"fal-ai/ideogram/v3/replace-background\": {\n    input: IdeogramV3ReplaceBackgroundInput;\n    output: IdeogramV3ReplaceBackgroundOutput;\n  };\n  \"fal-ai/ideogram/v3/remix\": {\n    input: IdeogramV3RemixInput;\n    output: IdeogramV3RemixOutput;\n  };\n  \"fal-ai/ideogram/v3/edit\": {\n    input: IdeogramV3EditInput;\n    output: IdeogramV3EditOutput;\n  };\n  \"fal-ai/hidream-e1-full\": {\n    input: HidreamE1FullInput;\n    output: HidreamE1FullOutput;\n  };\n  \"fal-ai/f-lite/standard\": {\n    input: FLiteStandardInput;\n    output: FLiteStandardOutput;\n  };\n  \"fal-ai/f-lite/texture\": {\n    input: FLiteTextureInput;\n    output: FLiteTextureOutput;\n  };\n  \"fal-ai/moondream2/visual-query\": {\n    input: Moondream2VisualQueryInput;\n    output: Moondream2VisualQueryOutput;\n  };\n  \"fal-ai/moondream2\": {\n    input: Moondream2Input;\n    output: Moondream2Output;\n  };\n  \"fal-ai/moondream2/point-object-detection\": {\n    input: Moondream2PointObjectDetectionInput;\n    output: Moondream2PointObjectDetectionOutput;\n  };\n  \"fal-ai/moondream2/object-detection\": {\n    input: Moondream2ObjectDetectionInput;\n    output: Moondream2ObjectDetectionOutput;\n  };\n  \"fal-ai/step1x-edit\": {\n    input: Step1xEditInput;\n    output: Step1xEditOutput;\n  };\n  \"tripo3d/tripo/v2.5/image-to-3d\": {\n    input: TripoV25ImageTo3dInput;\n    output: TripoV25ImageTo3dOutput;\n  };\n  \"fal-ai/image2svg\": {\n    input: Image2svgInput;\n    output: Image2svgOutput;\n  };\n  \"fal-ai/uno\": {\n    input: UnoInput;\n    output: UnoOutput;\n  };\n  \"fal-ai/gpt-image-1/edit-image/byok\": {\n    input: GptImage1EditImageByokInput;\n    output: GptImage1EditImageByokOutput;\n  };\n  \"fal-ai/gpt-image-1/text-to-image/byok\": {\n    input: GptImage1TextToImageByokInput;\n    output: GptImage1TextToImageByokOutput;\n  };\n  \"fal-ai/magi/extend-video\": {\n    input: MagiExtendVideoInput;\n    output: MagiExtendVideoOutput;\n  };\n  \"fal-ai/magi\": {\n    input: MagiInput;\n    output: MagiOutput;\n  };\n  \"fal-ai/magi/image-to-video\": {\n    input: MagiImageToVideoInput;\n    output: MagiImageToVideoOutput;\n  };\n  \"fal-ai/pixverse/v4/effects\": {\n    input: PixverseV4EffectsInput;\n    output: PixverseV4EffectsOutput;\n  };\n  \"fal-ai/magi-distilled/extend-video\": {\n    input: MagiDistilledExtendVideoInput;\n    output: MagiDistilledExtendVideoOutput;\n  };\n  \"fal-ai/magi-distilled/image-to-video\": {\n    input: MagiDistilledImageToVideoInput;\n    output: MagiDistilledImageToVideoOutput;\n  };\n  \"fal-ai/dia-tts/voice-clone\": {\n    input: DiaTtsVoiceCloneInput;\n    output: DiaTtsVoiceCloneOutput;\n  };\n  \"fal-ai/framepack/flf2v\": {\n    input: FramepackFlf2vInput;\n    output: FramepackFlf2vOutput;\n  };\n  \"fal-ai/dia-tts\": {\n    input: DiaTtsInput;\n    output: DiaTtsOutput;\n  };\n  \"fal-ai/magi-distilled\": {\n    input: MagiDistilledInput;\n    output: MagiDistilledOutput;\n  };\n  \"fal-ai/smart-turn\": {\n    input: SmartTurnInput;\n    output: SmartTurnOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux-lora/inpainting\": {\n    input: JuggernautFluxLoraInpaintingInput;\n    output: JuggernautFluxLoraInpaintingOutput;\n  };\n  \"fal-ai/fashn/tryon/v1.5\": {\n    input: FashnTryonV15Input;\n    output: FashnTryonV15Output;\n  };\n  \"fal-ai/plushify\": {\n    input: PlushifyInput;\n    output: PlushifyOutput;\n  };\n  \"fal-ai/instant-character\": {\n    input: InstantCharacterInput;\n    output: InstantCharacterOutput;\n  };\n  \"fal-ai/wan-flf2v\": {\n    input: WanFlf2vInput;\n    output: WanFlf2vOutput;\n  };\n  \"fal-ai/turbo-flux-trainer\": {\n    input: TurboFluxTrainerInput;\n    output: TurboFluxTrainerOutput;\n  };\n  \"fal-ai/framepack\": {\n    input: FramepackInput;\n    output: FramepackOutput;\n  };\n  \"fal-ai/cartoonify\": {\n    input: CartoonifyInput;\n    output: CartoonifyOutput;\n  };\n  \"fal-ai/wan-vace\": {\n    input: WanVaceInput;\n    output: WanVaceOutput;\n  };\n  \"fal-ai/finegrain-eraser/mask\": {\n    input: FinegrainEraserMaskInput;\n    output: FinegrainEraserMaskOutput;\n  };\n  \"fal-ai/finegrain-eraser/bbox\": {\n    input: FinegrainEraserBboxInput;\n    output: FinegrainEraserBboxOutput;\n  };\n  \"fal-ai/finegrain-eraser\": {\n    input: FinegrainEraserInput;\n    output: FinegrainEraserOutput;\n  };\n  \"cassetteai/video-sound-effects-generator\": {\n    input: VideoSoundEffectsGeneratorInput;\n    output: VideoSoundEffectsGeneratorOutput;\n  };\n  \"fal-ai/speech-to-text/turbo\": {\n    input: SpeechToTextTurboInput;\n    output: SpeechToTextTurboOutput;\n  };\n  \"fal-ai/speech-to-text/turbo/stream\": {\n    input: SpeechToTextTurboStreamInput;\n    output: any;\n  };\n  \"fal-ai/speech-to-text/stream\": {\n    input: SpeechToTextStreamInput;\n    output: any;\n  };\n  \"fal-ai/speech-to-text\": {\n    input: SpeechToTextInput;\n    output: SpeechToTextOutput;\n  };\n  \"cassetteai/sound-effects-generator\": {\n    input: SoundEffectsGeneratorInput;\n    output: SoundEffectsGeneratorOutput;\n  };\n  \"fal-ai/sync-lipsync/v2\": {\n    input: SyncLipsyncV2Input;\n    output: SyncLipsyncV2Output;\n  };\n  \"fal-ai/star-vector\": {\n    input: StarVectorInput;\n    output: StarVectorOutput;\n  };\n  \"fal-ai/pixverse/v4/image-to-video/fast\": {\n    input: PixverseV4ImageToVideoFastInput;\n    output: PixverseV4ImageToVideoFastOutput;\n  };\n  \"fal-ai/pixverse/v4/image-to-video\": {\n    input: PixverseV4ImageToVideoInput;\n    output: PixverseV4ImageToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/effects\": {\n    input: PixverseV35EffectsInput;\n    output: PixverseV35EffectsOutput;\n  };\n  \"fal-ai/pixverse/v4/text-to-video\": {\n    input: PixverseV4TextToVideoInput;\n    output: PixverseV4TextToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/transition\": {\n    input: PixverseV35TransitionInput;\n    output: PixverseV35TransitionOutput;\n  };\n  \"fal-ai/pixverse/v4/text-to-video/fast\": {\n    input: PixverseV4TextToVideoFastInput;\n    output: PixverseV4TextToVideoFastOutput;\n  };\n  \"fal-ai/ghiblify\": {\n    input: GhiblifyInput;\n    output: GhiblifyOutput;\n  };\n  \"fal-ai/orpheus-tts\": {\n    input: OrpheusTtsInput;\n    output: OrpheusTtsOutput;\n  };\n  \"fal-ai/sana/v1.5/1.6b\": {\n    input: SanaV1516bInput;\n    output: SanaV1516bOutput;\n  };\n  \"fal-ai/sana/v1.5/4.8b\": {\n    input: SanaV1548bInput;\n    output: SanaV1548bOutput;\n  };\n  \"fal-ai/sana/sprint\": {\n    input: SanaSprintInput;\n    output: SanaSprintOutput;\n  };\n  \"CassetteAI/music-generator\": {\n    input: MusicGeneratorInput;\n    output: MusicGeneratorOutput;\n  };\n  \"fal-ai/kling-video/lipsync/text-to-video\": {\n    input: KlingVideoLipsyncTextToVideoInput;\n    output: KlingVideoLipsyncTextToVideoOutput;\n  };\n  \"fal-ai/kling-video/lipsync/audio-to-video\": {\n    input: KlingVideoLipsyncAudioToVideoInput;\n    output: KlingVideoLipsyncAudioToVideoOutput;\n  };\n  \"fal-ai/latentsync\": {\n    input: LatentsyncInput;\n    output: LatentsyncOutput;\n  };\n  \"fal-ai/wan-t2v-lora\": {\n    input: WanT2vLoraInput;\n    output: WanT2vLoraOutput;\n  };\n  \"fal-ai/wan-trainer\": {\n    input: WanTrainerInput;\n    output: WanTrainerOutput;\n  };\n  \"fal-ai/thera\": {\n    input: TheraInput;\n    output: TheraOutput;\n  };\n  \"fal-ai/mix-dehaze-net\": {\n    input: MixDehazeNetInput;\n    output: MixDehazeNetOutput;\n  };\n  \"fal-ai/hunyuan3d/v2/multi-view/turbo\": {\n    input: Hunyuan3dV2MultiViewTurboInput;\n    output: Hunyuan3dV2MultiViewTurboOutput;\n  };\n  \"fal-ai/gemini-flash-edit\": {\n    input: GeminiFlashEditInput;\n    output: GeminiFlashEditOutput;\n  };\n  \"fal-ai/hunyuan3d/v2\": {\n    input: Hunyuan3dV2Input;\n    output: Hunyuan3dV2Output;\n  };\n  \"fal-ai/hunyuan3d/v2/multi-view\": {\n    input: Hunyuan3dV2MultiViewInput;\n    output: Hunyuan3dV2MultiViewOutput;\n  };\n  \"fal-ai/hunyuan3d/v2/mini\": {\n    input: Hunyuan3dV2MiniInput;\n    output: Hunyuan3dV2MiniOutput;\n  };\n  \"fal-ai/hunyuan3d/v2/turbo\": {\n    input: Hunyuan3dV2TurboInput;\n    output: Hunyuan3dV2TurboOutput;\n  };\n  \"fal-ai/hunyuan3d/v2/mini/turbo\": {\n    input: Hunyuan3dV2MiniTurboInput;\n    output: Hunyuan3dV2MiniTurboOutput;\n  };\n  \"fal-ai/gemini-flash-edit/multi\": {\n    input: GeminiFlashEditMultiInput;\n    output: GeminiFlashEditMultiOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2-flash/image-to-video\": {\n    input: LumaDreamMachineRay2FlashImageToVideoInput;\n    output: LumaDreamMachineRay2FlashImageToVideoOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2-flash\": {\n    input: LumaDreamMachineRay2FlashInput;\n    output: LumaDreamMachineRay2FlashOutput;\n  };\n  \"fal-ai/pika/v1.5/pikaffects\": {\n    input: PikaV15PikaffectsInput;\n    output: PikaV15PikaffectsOutput;\n  };\n  \"fal-ai/pika/v2/turbo/image-to-video\": {\n    input: PikaV2TurboImageToVideoInput;\n    output: PikaV2TurboImageToVideoOutput;\n  };\n  \"fal-ai/invisible-watermark\": {\n    input: InvisibleWatermarkInput;\n    output: InvisibleWatermarkOutput;\n  };\n  \"fal-ai/pika/v2.1/text-to-video\": {\n    input: PikaV21TextToVideoInput;\n    output: PikaV21TextToVideoOutput;\n  };\n  \"fal-ai/pika/v2/turbo/text-to-video\": {\n    input: PikaV2TurboTextToVideoInput;\n    output: PikaV2TurboTextToVideoOutput;\n  };\n  \"fal-ai/pika/v2.2/image-to-video\": {\n    input: PikaV22ImageToVideoInput;\n    output: PikaV22ImageToVideoOutput;\n  };\n  \"fal-ai/pika/v2.2/text-to-video\": {\n    input: PikaV22TextToVideoInput;\n    output: PikaV22TextToVideoOutput;\n  };\n  \"fal-ai/pika/v2.2/pikascenes\": {\n    input: PikaV22PikascenesInput;\n    output: PikaV22PikascenesOutput;\n  };\n  \"fal-ai/pika/v2.1/image-to-video\": {\n    input: PikaV21ImageToVideoInput;\n    output: PikaV21ImageToVideoOutput;\n  };\n  \"fal-ai/csm-1b\": {\n    input: Csm1bInput;\n    output: Csm1bOutput;\n  };\n  \"fal-ai/vidu/image-to-video\": {\n    input: ViduImageToVideoInput;\n    output: ViduImageToVideoOutput;\n  };\n  \"fal-ai/vidu/start-end-to-video\": {\n    input: ViduStartEndToVideoInput;\n    output: ViduStartEndToVideoOutput;\n  };\n  \"fal-ai/vidu/reference-to-video\": {\n    input: ViduReferenceToVideoInput;\n    output: ViduReferenceToVideoOutput;\n  };\n  \"fal-ai/vidu/template-to-video\": {\n    input: ViduTemplateToVideoInput;\n    output: ViduTemplateToVideoOutput;\n  };\n  \"fal-ai/wan-pro/text-to-video\": {\n    input: WanProTextToVideoInput;\n    output: WanProTextToVideoOutput;\n  };\n  \"easel-ai/advanced-face-swap\": {\n    input: AdvancedFaceSwapInput;\n    output: AdvancedFaceSwapOutput;\n  };\n  \"fal-ai/wan-i2v-lora\": {\n    input: WanI2vLoraInput;\n    output: WanI2vLoraOutput;\n  };\n  \"fal-ai/kling-video/v1/standard/effects\": {\n    input: KlingVideoV1StandardEffectsInput;\n    output: KlingVideoV1StandardEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/effects\": {\n    input: KlingVideoV16ProEffectsInput;\n    output: KlingVideoV16ProEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1.5/pro/effects\": {\n    input: KlingVideoV15ProEffectsInput;\n    output: KlingVideoV15ProEffectsOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/effects\": {\n    input: KlingVideoV16StandardEffectsInput;\n    output: KlingVideoV16StandardEffectsOutput;\n  };\n  \"fal-ai/hunyuan-video-image-to-video\": {\n    input: HunyuanVideoImageToVideoInput;\n    output: HunyuanVideoImageToVideoOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/lightning\": {\n    input: JuggernautFluxLightningInput;\n    output: JuggernautFluxLightningOutput;\n  };\n  \"rundiffusion-fal/rundiffusion-photo-flux\": {\n    input: RundiffusionPhotoFluxInput;\n    output: RundiffusionPhotoFluxOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/pro/image-to-image\": {\n    input: JuggernautFluxProImageToImageInput;\n    output: JuggernautFluxProImageToImageOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux-lora\": {\n    input: JuggernautFluxLoraInput;\n    output: JuggernautFluxLoraOutput;\n  };\n  \"fal-ai/ltx-video-v095/image-to-video\": {\n    input: LtxVideoV095ImageToVideoInput;\n    output: LtxVideoV095ImageToVideoOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/base/image-to-image\": {\n    input: JuggernautFluxBaseImageToImageInput;\n    output: JuggernautFluxBaseImageToImageOutput;\n  };\n  \"fal-ai/ltx-video-v095/extend\": {\n    input: LtxVideoV095ExtendInput;\n    output: LtxVideoV095ExtendOutput;\n  };\n  \"fal-ai/ltx-video-v095/multiconditioning\": {\n    input: LtxVideoV095MulticonditioningInput;\n    output: LtxVideoV095MulticonditioningOutput;\n  };\n  \"fal-ai/ltx-video-v095\": {\n    input: LtxVideoV095Input;\n    output: LtxVideoV095Output;\n  };\n  \"rundiffusion-fal/juggernaut-flux/pro\": {\n    input: JuggernautFluxProInput;\n    output: JuggernautFluxProOutput;\n  };\n  \"rundiffusion-fal/juggernaut-flux/base\": {\n    input: JuggernautFluxBaseInput;\n    output: JuggernautFluxBaseOutput;\n  };\n  \"fal-ai/cogview4\": {\n    input: Cogview4Input;\n    output: Cogview4Output;\n  };\n  \"fal-ai/diffrhythm\": {\n    input: DiffrhythmInput;\n    output: DiffrhythmOutput;\n  };\n  \"fal-ai/topaz/upscale/video\": {\n    input: TopazUpscaleVideoInput;\n    output: TopazUpscaleVideoOutput;\n  };\n  \"fal-ai/docres/dewarp\": {\n    input: DocresDewarpInput;\n    output: DocresDewarpOutput;\n  };\n  \"fal-ai/docres\": {\n    input: DocresInput;\n    output: DocresOutput;\n  };\n  \"fal-ai/swin2sr\": {\n    input: Swin2srInput;\n    output: Swin2srOutput;\n  };\n  \"fal-ai/ideogram/v2a/remix\": {\n    input: IdeogramV2aRemixInput;\n    output: IdeogramV2aRemixOutput;\n  };\n  \"fal-ai/kling-video/v1.6/pro/text-to-video\": {\n    input: KlingVideoV16ProTextToVideoInput;\n    output: KlingVideoV16ProTextToVideoOutput;\n  };\n  \"fal-ai/ideogram/v2a/turbo/remix\": {\n    input: IdeogramV2aTurboRemixInput;\n    output: IdeogramV2aTurboRemixOutput;\n  };\n  \"fal-ai/elevenlabs/tts/multilingual-v2\": {\n    input: ElevenlabsTtsMultilingualV2Input;\n    output: ElevenlabsTtsMultilingualV2Output;\n  };\n  \"fal-ai/ideogram/v2a/turbo\": {\n    input: IdeogramV2aTurboInput;\n    output: IdeogramV2aTurboOutput;\n  };\n  \"fal-ai/elevenlabs/audio-isolation\": {\n    input: ElevenlabsAudioIsolationInput;\n    output: ElevenlabsAudioIsolationOutput;\n  };\n  \"fal-ai/elevenlabs/tts/turbo-v2.5\": {\n    input: ElevenlabsTtsTurboV25Input;\n    output: ElevenlabsTtsTurboV25Output;\n  };\n  \"fal-ai/elevenlabs/speech-to-text\": {\n    input: ElevenlabsSpeechToTextInput;\n    output: ElevenlabsSpeechToTextOutput;\n  };\n  \"fal-ai/elevenlabs/sound-effects\": {\n    input: ElevenlabsSoundEffectsInput;\n    output: ElevenlabsSoundEffectsOutput;\n  };\n  \"fal-ai/ideogram/v2a\": {\n    input: IdeogramV2aInput;\n    output: IdeogramV2aOutput;\n  };\n  \"fal-ai/evf-sam\": {\n    input: EvfSamInput;\n    output: EvfSamOutput;\n  };\n  \"fal-ai/ddcolor\": {\n    input: DdcolorInput;\n    output: DdcolorOutput;\n  };\n  \"fal-ai/wan-t2v\": {\n    input: WanT2vInput;\n    output: WanT2vOutput;\n  };\n  \"fal-ai/video-prompt-generator\": {\n    input: VideoPromptGeneratorInput;\n    output: VideoPromptGeneratorOutput;\n  };\n  \"fal-ai/sam2/auto-segment\": {\n    input: Sam2AutoSegmentInput;\n    output: Sam2AutoSegmentOutput;\n  };\n  \"fal-ai/minimax/video-01-director/image-to-video\": {\n    input: MinimaxVideo01DirectorImageToVideoInput;\n    output: MinimaxVideo01DirectorImageToVideoOutput;\n  };\n  \"fal-ai/drct-super-resolution\": {\n    input: DrctSuperResolutionInput;\n    output: DrctSuperResolutionOutput;\n  };\n  \"fal-ai/veo2\": {\n    input: Veo2Input;\n    output: Veo2Output;\n  };\n  \"fal-ai/nafnet/deblur\": {\n    input: NafnetDeblurInput;\n    output: NafnetDeblurOutput;\n  };\n  \"fal-ai/nafnet/denoise\": {\n    input: NafnetDenoiseInput;\n    output: NafnetDenoiseOutput;\n  };\n  \"fal-ai/post-processing\": {\n    input: PostProcessingInput;\n    output: PostProcessingOutput;\n  };\n  \"fal-ai/skyreels-i2v\": {\n    input: SkyreelsI2vInput;\n    output: SkyreelsI2vOutput;\n  };\n  \"fal-ai/flowedit\": {\n    input: FloweditInput;\n    output: FloweditOutput;\n  };\n  \"fal-ai/kokoro/hindi\": {\n    input: KokoroHindiInput;\n    output: KokoroHindiOutput;\n  };\n  \"fal-ai/kokoro/mandarin-chinese\": {\n    input: KokoroMandarinChineseInput;\n    output: KokoroMandarinChineseOutput;\n  };\n  \"fal-ai/kokoro/brazilian-portuguese\": {\n    input: KokoroBrazilianPortugueseInput;\n    output: KokoroBrazilianPortugueseOutput;\n  };\n  \"fal-ai/kokoro/spanish\": {\n    input: KokoroSpanishInput;\n    output: KokoroSpanishOutput;\n  };\n  \"fal-ai/kokoro/french\": {\n    input: KokoroFrenchInput;\n    output: KokoroFrenchOutput;\n  };\n  \"fal-ai/kokoro/japanese\": {\n    input: KokoroJapaneseInput;\n    output: KokoroJapaneseOutput;\n  };\n  \"fal-ai/kokoro/british-english\": {\n    input: KokoroBritishEnglishInput;\n    output: KokoroBritishEnglishOutput;\n  };\n  \"fal-ai/kokoro/american-english\": {\n    input: KokoroAmericanEnglishInput;\n    output: KokoroAmericanEnglishOutput;\n  };\n  \"fal-ai/zonos\": {\n    input: ZonosInput;\n    output: ZonosOutput;\n  };\n  \"fal-ai/kokoro/italian\": {\n    input: KokoroItalianInput;\n    output: KokoroItalianOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2/image-to-video\": {\n    input: LumaDreamMachineRay2ImageToVideoInput;\n    output: LumaDreamMachineRay2ImageToVideoOutput;\n  };\n  \"fal-ai/got-ocr/v2\": {\n    input: GotOcrV2Input;\n    output: GotOcrV2Output;\n  };\n  \"fal-ai/flux-control-lora-canny/image-to-image\": {\n    input: FluxControlLoraCannyImageToImageInput;\n    output: FluxControlLoraCannyImageToImageOutput;\n  };\n  \"fal-ai/flux-control-lora-depth/image-to-image\": {\n    input: FluxControlLoraDepthImageToImageInput;\n    output: FluxControlLoraDepthImageToImageOutput;\n  };\n  \"fal-ai/ben/v2/image\": {\n    input: BenV2ImageInput;\n    output: BenV2ImageOutput;\n  };\n  \"fal-ai/flux-control-lora-canny\": {\n    input: FluxControlLoraCannyInput;\n    output: FluxControlLoraCannyOutput;\n  };\n  \"fal-ai/flux-control-lora-depth\": {\n    input: FluxControlLoraDepthInput;\n    output: FluxControlLoraDepthOutput;\n  };\n  \"fal-ai/ben/v2/video\": {\n    input: BenV2VideoInput;\n    output: BenV2VideoOutput;\n  };\n  \"fal-ai/minimax/video-01-director\": {\n    input: MinimaxVideo01DirectorInput;\n    output: MinimaxVideo01DirectorOutput;\n  };\n  \"fal-ai/imagen3\": {\n    input: Imagen3Input;\n    output: Imagen3Output;\n  };\n  \"fal-ai/imagen3/fast\": {\n    input: Imagen3FastInput;\n    output: Imagen3FastOutput;\n  };\n  \"fal-ai/ideogram/upscale\": {\n    input: IdeogramUpscaleInput;\n    output: IdeogramUpscaleOutput;\n  };\n  \"fal-ai/hunyuan-video-img2vid-lora\": {\n    input: HunyuanVideoImg2vidLoraInput;\n    output: HunyuanVideoImg2vidLoraOutput;\n  };\n  \"fal-ai/codeformer\": {\n    input: CodeformerInput;\n    output: CodeformerOutput;\n  };\n  \"fal-ai/lumina-image/v2\": {\n    input: LuminaImageV2Input;\n    output: LuminaImageV2Output;\n  };\n  \"fal-ai/hunyuan-video/video-to-video\": {\n    input: HunyuanVideoVideoToVideoInput;\n    output: HunyuanVideoVideoToVideoOutput;\n  };\n  \"fal-ai/hunyuan-video-lora/video-to-video\": {\n    input: HunyuanVideoLoraVideoToVideoInput;\n    output: HunyuanVideoLoraVideoToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/text-to-video\": {\n    input: PixverseV35TextToVideoInput;\n    output: PixverseV35TextToVideoOutput;\n  };\n  \"fal-ai/pixverse/v3.5/image-to-video/fast\": {\n    input: PixverseV35ImageToVideoFastInput;\n    output: PixverseV35ImageToVideoFastOutput;\n  };\n  \"fal-ai/pixverse/v3.5/text-to-video/fast\": {\n    input: PixverseV35TextToVideoFastInput;\n    output: PixverseV35TextToVideoFastOutput;\n  };\n  \"fal-ai/pixverse/v3.5/image-to-video\": {\n    input: PixverseV35ImageToVideoInput;\n    output: PixverseV35ImageToVideoOutput;\n  };\n  \"fal-ai/janus\": {\n    input: JanusInput;\n    output: JanusOutput;\n  };\n  \"fal-ai/yue\": {\n    input: YueInput;\n    output: YueOutput;\n  };\n  \"fal-ai/luma-dream-machine/ray-2\": {\n    input: LumaDreamMachineRay2Input;\n    output: LumaDreamMachineRay2Output;\n  };\n  \"fal-ai/kling/v1-5/kolors-virtual-try-on\": {\n    input: KlingV15KolorsVirtualTryOnInput;\n    output: KlingV15KolorsVirtualTryOnOutput;\n  };\n  \"fal-ai/ffmpeg-api/metadata\": {\n    input: FfmpegApiMetadataInput;\n    output: FfmpegApiMetadataOutput;\n  };\n  \"fal-ai/ffmpeg-api/waveform\": {\n    input: FfmpegApiWaveformInput;\n    output: FfmpegApiWaveformOutput;\n  };\n  \"fal-ai/ffmpeg-api/compose\": {\n    input: FfmpegApiComposeInput;\n    output: FfmpegApiComposeOutput;\n  };\n  \"fal-ai/minimax/video-01-subject-reference\": {\n    input: MinimaxVideo01SubjectReferenceInput;\n    output: MinimaxVideo01SubjectReferenceOutput;\n  };\n  \"fal-ai/moondream-next/batch\": {\n    input: MoondreamNextBatchInput;\n    output: MoondreamNextBatchOutput;\n  };\n  \"fal-ai/flux-pro/v1/canny\": {\n    input: FluxProV1CannyInput;\n    output: FluxProV1CannyOutput;\n  };\n  \"fal-ai/flux-pro/v1/depth\": {\n    input: FluxProV1DepthInput;\n    output: FluxProV1DepthOutput;\n  };\n  \"fal-ai/flux-pro/v1/canny-finetuned\": {\n    input: FluxProV1CannyFinetunedInput;\n    output: FluxProV1CannyFinetunedOutput;\n  };\n  \"fal-ai/flux-lora-canny\": {\n    input: FluxLoraCannyInput;\n    output: FluxLoraCannyOutput;\n  };\n  \"fal-ai/flux-pro-trainer\": {\n    input: FluxProTrainerInput;\n    output: FluxProTrainerOutput;\n  };\n  \"fal-ai/flux-pro/v1.1\": {\n    input: FluxProV11Input;\n    output: FluxProV11Output;\n  };\n  \"fal-ai/flux-pro/v1.1-ultra-finetuned\": {\n    input: FluxProV11UltraFinetunedInput;\n    output: FluxProV11UltraFinetunedOutput;\n  };\n  \"fal-ai/flux-pro/v1/fill-finetuned\": {\n    input: FluxProV1FillFinetunedInput;\n    output: FluxProV1FillFinetunedOutput;\n  };\n  \"fal-ai/flux-pro/v1/depth-finetuned\": {\n    input: FluxProV1DepthFinetunedInput;\n    output: FluxProV1DepthFinetunedOutput;\n  };\n  \"fal-ai/hunyuan-video-lora\": {\n    input: HunyuanVideoLoraInput;\n    output: HunyuanVideoLoraOutput;\n  };\n  \"fal-ai/cogvideox-5b\": {\n    input: Cogvideox5bInput;\n    output: Cogvideox5bOutput;\n  };\n  \"fal-ai/transpixar\": {\n    input: TranspixarInput;\n    output: TranspixarOutput;\n  };\n  \"fal-ai/hunyuan-video-lora-training\": {\n    input: HunyuanVideoLoraTrainingInput;\n    output: HunyuanVideoLoraTrainingOutput;\n  };\n  \"fal-ai/sync-lipsync\": {\n    input: SyncLipsyncInput;\n    output: SyncLipsyncOutput;\n  };\n  \"fal-ai/sa2va/8b/video\": {\n    input: Sa2va8bVideoInput;\n    output: Sa2va8bVideoOutput;\n  };\n  \"fal-ai/sa2va/4b/video\": {\n    input: Sa2va4bVideoInput;\n    output: Sa2va4bVideoOutput;\n  };\n  \"fal-ai/sa2va/4b/image\": {\n    input: Sa2va4bImageInput;\n    output: Sa2va4bImageOutput;\n  };\n  \"fal-ai/sa2va/8b/image\": {\n    input: Sa2va8bImageInput;\n    output: Sa2va8bImageOutput;\n  };\n  \"fal-ai/moondream-next\": {\n    input: MoondreamNextInput;\n    output: MoondreamNextOutput;\n  };\n  \"fal-ai/moondream-next/detection\": {\n    input: MoondreamNextDetectionInput;\n    output: MoondreamNextDetectionOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/image-to-video\": {\n    input: KlingVideoV16StandardImageToVideoInput;\n    output: KlingVideoV16StandardImageToVideoOutput;\n  };\n  \"fal-ai/kling-video/v1.6/standard/text-to-video\": {\n    input: KlingVideoV16StandardTextToVideoInput;\n    output: KlingVideoV16StandardTextToVideoOutput;\n  };\n  \"fal-ai/auto-caption\": {\n    input: AutoCaptionInput;\n    output: AutoCaptionOutput;\n  };\n  \"fal-ai/switti\": {\n    input: SwittiInput;\n    output: SwittiOutput;\n  };\n  \"fal-ai/switti/512\": {\n    input: Switti512Input;\n    output: Switti512Output;\n  };\n  \"fal-ai/mmaudio-v2/text-to-audio\": {\n    input: MmaudioV2TextToAudioInput;\n    output: MmaudioV2TextToAudioOutput;\n  };\n  \"fal-ai/sadtalker/reference\": {\n    input: SadtalkerReferenceInput;\n    output: SadtalkerReferenceOutput;\n  };\n  \"fal-ai/dubbing\": {\n    input: DubbingInput;\n    output: DubbingOutput;\n  };\n  \"fal-ai/bria/text-to-image/fast\": {\n    input: BriaTextToImageFastInput;\n    output: BriaTextToImageFastOutput;\n  };\n  \"fal-ai/bria/expand\": {\n    input: BriaExpandInput;\n    output: BriaExpandOutput;\n  };\n  \"fal-ai/bria/text-to-image/base\": {\n    input: BriaTextToImageBaseInput;\n    output: BriaTextToImageBaseOutput;\n  };\n  \"fal-ai/bria/genfill\": {\n    input: BriaGenfillInput;\n    output: BriaGenfillOutput;\n  };\n  \"fal-ai/playai/tts/v3\": {\n    input: PlayaiTtsV3Input;\n    output: PlayaiTtsV3Output;\n  };\n  \"fal-ai/flux-lora-fill\": {\n    input: FluxLoraFillInput;\n    output: FluxLoraFillOutput;\n  };\n  \"fal-ai/bria/background/replace\": {\n    input: BriaBackgroundReplaceInput;\n    output: BriaBackgroundReplaceOutput;\n  };\n  \"fal-ai/bria/eraser\": {\n    input: BriaEraserInput;\n    output: BriaEraserOutput;\n  };\n  \"fal-ai/bria/text-to-image/hd\": {\n    input: BriaTextToImageHdInput;\n    output: BriaTextToImageHdOutput;\n  };\n  \"fal-ai/bria/product-shot\": {\n    input: BriaProductShotInput;\n    output: BriaProductShotOutput;\n  };\n  \"fal-ai/bria/background/remove\": {\n    input: BriaBackgroundRemoveInput;\n    output: BriaBackgroundRemoveOutput;\n  };\n  \"fal-ai/cat-vton\": {\n    input: CatVtonInput;\n    output: CatVtonOutput;\n  };\n  \"fal-ai/leffa/pose-transfer\": {\n    input: LeffaPoseTransferInput;\n    output: LeffaPoseTransferOutput;\n  };\n  \"fal-ai/minimax-music\": {\n    input: MinimaxMusicInput;\n    output: MinimaxMusicOutput;\n  };\n  \"fal-ai/leffa/virtual-tryon\": {\n    input: LeffaVirtualTryonInput;\n    output: LeffaVirtualTryonOutput;\n  };\n  \"fal-ai/recraft-20b\": {\n    input: Recraft20bInput;\n    output: Recraft20bOutput;\n  };\n  \"fal-ai/minimax/video-01-live/image-to-video\": {\n    input: MinimaxVideo01LiveImageToVideoInput;\n    output: MinimaxVideo01LiveImageToVideoOutput;\n  };\n  \"fal-ai/hyper3d/rodin\": {\n    input: Hyper3dRodinInput;\n    output: Hyper3dRodinOutput;\n  };\n  \"fal-ai/minimax/video-01-live\": {\n    input: MinimaxVideo01LiveInput;\n    output: MinimaxVideo01LiveOutput;\n  };\n  \"fal-ai/ideogram/v2/edit\": {\n    input: IdeogramV2EditInput;\n    output: IdeogramV2EditOutput;\n  };\n  \"fal-ai/trellis\": {\n    input: TrellisInput;\n    output: TrellisOutput;\n  };\n  \"fal-ai/luma-dream-machine\": {\n    input: LumaDreamMachineInput;\n    output: LumaDreamMachineOutput;\n  };\n  \"fal-ai/ideogram/v2/turbo/edit\": {\n    input: IdeogramV2TurboEditInput;\n    output: IdeogramV2TurboEditOutput;\n  };\n  \"fal-ai/ideogram/v2/turbo/remix\": {\n    input: IdeogramV2TurboRemixInput;\n    output: IdeogramV2TurboRemixOutput;\n  };\n  \"fal-ai/ideogram/v2/turbo\": {\n    input: IdeogramV2TurboInput;\n    output: IdeogramV2TurboOutput;\n  };\n  \"fal-ai/video-upscaler\": {\n    input: VideoUpscalerInput;\n    output: VideoUpscalerOutput;\n  };\n  \"fal-ai/ideogram/v2/remix\": {\n    input: IdeogramV2RemixInput;\n    output: IdeogramV2RemixOutput;\n  };\n  \"fal-ai/kling-video/v1/standard/text-to-video\": {\n    input: KlingVideoV1StandardTextToVideoInput;\n    output: KlingVideoV1StandardTextToVideoOutput;\n  };\n  \"fal-ai/luma-photon/flash\": {\n    input: LumaPhotonFlashInput;\n    output: LumaPhotonFlashOutput;\n  };\n  \"fal-ai/aura-flow\": {\n    input: AuraFlowInput;\n    output: AuraFlowOutput;\n  };\n  \"fal-ai/omnigen-v1\": {\n    input: OmnigenV1Input;\n    output: OmnigenV1Output;\n  };\n  \"fal-ai/flux/schnell/redux\": {\n    input: FluxSchnellReduxInput;\n    output: FluxSchnellReduxOutput;\n  };\n};\n"]}